{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting output of NER\n",
    "\n",
    "We wish to convert the output of NER to a format that can be fed to the RE module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import pprint\n",
    "import bisect\n",
    "import re\n",
    "from collections import defaultdict \n",
    "import csv\n",
    "\n",
    "import scispacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to generate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is just one file. Can be easily generalized\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    " \n",
    "def generate_output(text_path,ann_path,output_path):\n",
    "    \n",
    "    with open(text_path, 'r') as f:\n",
    "        input_lines = f.readlines()\n",
    "        \n",
    "    if len(input_lines)==1:\n",
    "        doc = nlp(input_lines[0])\n",
    "        processed_lines = list(doc.sents)\n",
    "        length_arr_cum = [sent.end_char for sent in processed_lines]\n",
    "    \n",
    "    else:\n",
    "        length_arr = [len(l) for l in input_lines] \n",
    "        length_arr_cum = [0] * len(length_arr)\n",
    "        temp = 0\n",
    "        for i in range(len(length_arr)):\n",
    "            temp += length_arr[i]\n",
    "            length_arr_cum[i] = temp\n",
    "\n",
    "    \n",
    "    \n",
    "#     print(length_arr_cum)             \n",
    "\n",
    "#     idx = bisect.bisect_right(length_arr_cum,436)\n",
    "#     print(idx)\n",
    "    # print(lines[idx][1125-lines_len_cum[idx-1]:1135-lines_len_cum[idx-1]])\n",
    "\n",
    "    named_entities = [defaultdict(list) for i in range(len(length_arr_cum))]\n",
    "\n",
    "    with open(ann_path,'r') as f:\n",
    "        lines = csv.reader(f,delimiter='\\t');\n",
    "        for line in lines:\n",
    "            info = line[1].split(' ')\n",
    "            label = info[0]\n",
    "            start = int(info[1])\n",
    "            end = int(info[2])\n",
    "#             print(doc.text[start:end],line[2])\n",
    "            idx = bisect.bisect_right(length_arr_cum,start)\n",
    "            baseline = processed_lines[idx].start_char\n",
    "            named_entities[idx][label].append((start-baseline,end-baseline,line[0],line[2]))\n",
    "            print(processed_lines[idx].text[start-baseline:end-baseline],line[2])\n",
    "    \n",
    "    ## Creating anonymous sentences\n",
    "    \n",
    "    output_sent = []\n",
    "    count = 0\n",
    "    id_list = []\n",
    "    for i in range(len(length_arr_cum)):\n",
    "\n",
    "        for t1 in named_entities[i]['Chemical']:\n",
    "            for t2 in named_entities[i]['Gene']:\n",
    "                (start1,end1,id1,w1) = t1\n",
    "                (start2,end2,id2,w2) = t2\n",
    "                str_temp = processed_lines[i].text\n",
    "                if(start1 < start2):\n",
    "                    output_str = str_temp[:start1] + \"@CHEMICAL$\" + str_temp[end1:start2] + \"@GENE$\" + str_temp[end2:]\n",
    "                    token_tup = (id1,id2)\n",
    "                else:\n",
    "                    output_str = str_temp[:start2] + \"@GENE$\" + str_temp[end2:start1] + \"@CHEMICAL$\" + str_temp[end1:]\n",
    "                    token_tup = (id2,id1)\n",
    "#                 print(output_str)\n",
    "                output_sent.append(output_str)\n",
    "                id_list.append(token_tup)\n",
    "    \n",
    "    ## Generating input for RE\n",
    "    \n",
    "    name = text_path.stem\n",
    "    new_dir = output_path / str(name)\n",
    "    new_dir.mkdir(exist_ok= True)\n",
    "    \n",
    "    shutil.copy(text_path, new_dir / text_path.name)\n",
    "    shutil.copy(ann_path, new_dir / (\"old_\" + str(ann_path.name)))\n",
    "    \n",
    "    with open(new_dir / 'test.tsv','w') as f:\n",
    "        f.write(\"dummy\\tdummy\\tsentence\\n\")\n",
    "        for sent in output_sent:\n",
    "            if '\\n' not in sent:\n",
    "                sent += '\\n'\n",
    "            f.write(\"a\\ta\\t \"+ sent)\n",
    "\n",
    "    with open(new_dir / 'pairs.tsv','w') as f:\n",
    "        for (t1,t2) in id_list:\n",
    "            f.write(t1+\"\\t\"+t2 + \"\\n\")\n",
    "# # pprint.pprint(named_entities)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This block is just a test. Comment this out \n",
    "\n",
    "text_path = Path(\"../data/original/sb3000673.txt\")\n",
    "ann_path = Path(\"../data/original/sb3000673.ann\")\n",
    "output_path = Path(\"../data/processed/\")\n",
    "generate_output(text_path,ann_path,output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating through brat files\n",
    "\n",
    "Given the root, we will iterate through all the .txt and .ann files that exist inside given path (also iterates through nested subdirectories, thus is recusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"../data/original/\")\n",
    "output_dir = Path(\"../data/processed/\")\n",
    "\n",
    "def walk_dir(root):\n",
    "    if(root.is_dir()):\n",
    "        for child in root.iterdir():\n",
    "            walk_dir(child)\n",
    "    else:\n",
    "        if(root.suffix == '.txt'):\n",
    "            ann_path = root.with_suffix(\".ann\")\n",
    "            if(ann_path.exists()):\n",
    "                generate_output(root,ann_path,output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_dir(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

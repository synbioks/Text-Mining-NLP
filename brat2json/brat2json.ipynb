{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code assumes that the text in the input directory ann files and the json files is the exact same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "\n",
    "# in_dir = \"refined_annotations\"\n",
    "# in_dir should consist of ann files\n",
    "in_dir = \"ann\"\n",
    "# ACS data in json format is stored in json_dir\n",
    "json_dir = \"json\"\n",
    "# ACS data in json format with NER annotations would be stored in out_dir\n",
    "out_dir = \"json_out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json(para_data, sent_span):\n",
    "    json_ner_data = []\n",
    "\n",
    "    # add keys to entity mention's features\n",
    "    for mention, mention_data in para_data.items():\n",
    "        ner_instance_array = []\n",
    "        for apprnc in mention_data[1]:\n",
    "            ner_instance_array.append({\"instance_id\": apprnc[0], \"entity_type\": apprnc[1], \n",
    "                                       \"global_span_start\": apprnc[2], \"global_span_end\": apprnc[3],\n",
    "                                      \"local_span_start\": apprnc[2]-sent_span, \"local_span_end\": apprnc[3]-sent_span})\n",
    "\n",
    "        \n",
    "        json_ner_data.append({\"ner_mention\": mention,\"appearance_count\": mention_data[0], \"ner_instance_array\": ner_instance_array})\n",
    "\n",
    "\n",
    "    return json_ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_text(json_file):\n",
    "#     new_text = \"\"\n",
    "    \n",
    "#     data = copy.deepcopy(json_file)\n",
    "    \n",
    "#     para_list = data[\"body\"]\n",
    "#     para_list.insert(0, {\"text\": data[\"abstract\"][0]})\n",
    "    \n",
    "#     sent_span = 0\n",
    "#     # data[\"body\"] is a list of dicts\n",
    "#     for i, para in enumerate(para_list):\n",
    "#         # Empty sections issue\n",
    "#         if \"section_header\" in para and para[\"section_header\"] == para[\"text\"]:\n",
    "# #             print(\"Hello\")\n",
    "#             continue\n",
    "    \n",
    "#         print(i, sent_span, sent_span + len(para[\"text\"]))\n",
    "        \n",
    "#         sent_span += len(para[\"text\"]) + 1\n",
    "        \n",
    "#         new_text += para[\"text\"] + \"\\n\"\n",
    "#     with open(\"test.txt\", \"w\") as fp:\n",
    "#         fp.write(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(json_dir, \"sb7b00428\" + \".json\"), \"r\") as fp:\n",
    "#     json_file = json.load(fp)\n",
    "# get_text(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_annotations(ann_file, txt_file, json_file):\n",
    "    data = copy.deepcopy(json_file)\n",
    "    \n",
    "    para_list = copy.deepcopy(data[\"body\"])\n",
    "    para_list.insert(0, {\"text\": data[\"abstract\"][0]})\n",
    "    \n",
    "    num_covered = 0\n",
    "    \n",
    "    status = True\n",
    "    \n",
    "    sent_span = 0\n",
    "    # data[\"body\"] is a list of dicts\n",
    "    for i, para in enumerate(para_list): \n",
    "        \n",
    "        # para is the actual test of the paragraph\n",
    "        # Empty sections issue\n",
    "        if \"section_header\" in para and para[\"section_header\"] == para[\"text\"]:\n",
    "            # the section name would be a part of the text of the next para, so update sent_span\n",
    "            sent_span += len(para[\"section_header\"]) + 1\n",
    "            continue\n",
    "        \n",
    "        para_data = {}\n",
    "        \n",
    "        # abstracts are not annotated in 20210111\n",
    "        if i==0:\n",
    "            data[\"abstract\"] = [{\"text\": para, \"ner_annotations\": {}}]\n",
    "            continue\n",
    "        else:\n",
    "            data[\"body\"][i-1][\"ner_annotations\"] = {}\n",
    "        \n",
    "        \n",
    "#         # Missing sections issue\n",
    "#         flag = False\n",
    "#         # make sure all mentions in the ann file are in this para span, if any one is missing, skip this para\n",
    "#         for line in ann_file:\n",
    "#             split_line = line.strip().split(\"\\t\")\n",
    "\n",
    "#             instance_id = split_line[0]\n",
    "#             ent_info = split_line[1].strip().split(\" \")\n",
    "#             entity = ent_info[0]\n",
    "#             span_start = int(ent_info[1])\n",
    "#             span_end = int(ent_info[2])\n",
    "#             mention = split_line[-1]\n",
    "            \n",
    "            \n",
    "#             # check if the mention should be in this para\n",
    "#             if span_start < sent_span + len(para[\"text\"]) and span_start >= sent_span:\n",
    "#                 # check whether mention is actually present in para or not\n",
    "#                 if para[\"text\"].find(mention) < 0:\n",
    "#                     flag = True\n",
    "#                     break\n",
    "        \n",
    "#         if(flag):\n",
    "#             continue\n",
    "        \n",
    "        \n",
    "#         if para[\"text\"] != txt_file[sent_span:sent_span + len(para[\"text\"])]:\n",
    "#             status = False\n",
    "        \n",
    "        # find all mentions in the ann file which are present in this span\n",
    "        for line in ann_file:\n",
    "            split_line = line.strip().split(\"\\t\")\n",
    "\n",
    "            instance_id = split_line[0]\n",
    "            ent_info = split_line[1].strip().split(\" \")\n",
    "            entity = ent_info[0]\n",
    "            span_start = int(ent_info[1])\n",
    "            span_end = int(ent_info[2])\n",
    "            mention = split_line[-1]\n",
    "            \n",
    "            # check if the mention is in this para\n",
    "            if span_start < sent_span + len(para[\"text\"]) and span_start >= sent_span:\n",
    "                num_covered += 1\n",
    "                \n",
    "                # just making sure that the mention occurs in this para\n",
    "                # if this assertion fails, the text in the json files is NOT the same as the\n",
    "                # input txt files on which the ann is based\n",
    "#                 assert para[\"text\"].find(mention) >= 0\n",
    "                \n",
    "                if mention in para_data:\n",
    "                    para_data[mention][0] += 1\n",
    "                    para_data[mention][1].append((instance_id, entity, span_start, span_end))\n",
    "                else:\n",
    "                    para_data[mention] = [1, [(instance_id, entity, span_start, span_end)]]\n",
    "    \n",
    "    \n",
    "        jsonify = make_json(para_data, sent_span)\n",
    "    \n",
    "        if i==0:\n",
    "            data[\"abstract\"][0][\"ner_annotations\"] = jsonify\n",
    "        else:\n",
    "            # convert the para_data dict into the json object required by the schema\n",
    "            data[\"body\"][i-1][\"ner_annotations\"] = jsonify\n",
    "    \n",
    "        sent_span += len(para[\"text\"]) + 1 # +1 for newline or a single whitespace\n",
    "    \n",
    "    data[\"abstract\"][0][\"text\"] = data[\"abstract\"][0][\"text\"][\"text\"]\n",
    "    return data, ((num_covered == len(ann_file)) and status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ner_files = []\n",
    "    for f_name in os.listdir(in_dir):\n",
    "        if \".ann\" in f_name:\n",
    "            ner_files.append(f_name[:-4])\n",
    "\n",
    "\n",
    "    for f_name in ner_files:\n",
    "        with open(os.path.join(in_dir, f_name + \".ann\"), \"r\") as fp:\n",
    "            ann_file = fp.readlines()\n",
    "        with open(os.path.join(\"txt\", f_name + \".txt\"), \"r\") as fp:\n",
    "            txt_file = fp.read()\n",
    "        with open(os.path.join(json_dir, f_name + \".json\"), \"r\") as fp:\n",
    "            json_file = json.load(fp)\n",
    "\n",
    "        \n",
    "        json_ann_data, status = add_annotations(ann_file, txt_file, json_file)\n",
    "        \n",
    "        if status:\n",
    "            fp = open(os.path.join(out_dir, f_name+\".json\"), \"w\")\n",
    "            json.dump(json_ann_data, fp, indent=4)\n",
    "            print(f\"Successfully processed {f_name}\" )\n",
    "        else:\n",
    "            print(f\"Could not process {f_name}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed sb6b00010\n",
      "Successfully processed sb5b00012\n",
      "Successfully processed sb5b00007\n",
      "Could not process sb5b00002\n",
      "Successfully processed sb6b00009\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following code transfers few select files to another directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer of files to output directory\n",
    "from shutil import copyfile\n",
    "final_out_dir = \"output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = [\"sb5b00012\", \"sb6b00027\", \"sb7b00086\", \"sb8b00534\", \"sb9b00103\", \n",
    "         \"sb400051t\", \"sb3001084\", \"sb2000275\", \"sb6b00135\", \"sb8b00242\" ]\n",
    "\n",
    "for f_name in files_list:\n",
    "#     try:\n",
    "        src1 = os.path.join(out_dir, f_name + \".json\")\n",
    "        dst1 = os.path.join(final_out_dir, f_name + \".json\")\n",
    "        copyfile(src1, dst1)\n",
    "        src2 = os.path.join(\"txt\", f_name + \".txt\")\n",
    "        dst2 = os.path.join(final_out_dir, f_name + \".txt\")\n",
    "        copyfile(src2, dst2)\n",
    "#     except:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"vcu/sb5b00012.txt\", \"r\") as fp:\n",
    "#     text_vcu = fp.read()\n",
    "# with open(\"acs_research/sb5b00012.txt\", \"r\") as fp:\n",
    "#     text_acs = fp.read()\n",
    "    \n",
    "# for i in range(len(text_acs)):\n",
    "#     if text_acs[i] != text_vcu[i]:\n",
    "#         print(i, text_acs[i], text_vcu[i])\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

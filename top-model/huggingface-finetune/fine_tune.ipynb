{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed value\n",
    "seed_value=42\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "# 4. Set `pytorch` pseudo-random generator at a fixed value\n",
    "import torch\n",
    "torch.manual_seed(seed_value)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"/sbksvol/xiang/sbks_gitlab/top-model/BIOBERT/NER/data/raw/Cellline\"\n",
    "data_path = \"/sbksvol/gaurav/NER_data/\"\n",
    "# saved_model_path = \"/sbksvol/gaurav/biobert_v1.0_pubmed_pmc/\"\n",
    "# saved_model_path = \"/sbksvol/gaurav/biobert_v1.1_pubmed/\"\n",
    "# pretrained_model_path = \"/sbksvol/gaurav/biobert_v1.0_pubmed_pmc/biobert_model.ckpt\"\n",
    "# config_path = \"/sbksvol/gaurav/biobert_v1.0_pubmed_pmc/config.json\"\n",
    "# config_path = \"/sbksvol/gaurav/biobert_v1.1_pubmed/config.json\"\n",
    "# vocab_path = \"/sbksvol/gaurav/biobert_v1.1_pubmed/vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from convert_tf_checkpoint_to_pytorch import convert_tf_checkpoint_to_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENT = \"Cellline\"\n",
    "DATASET = \"jnlpba\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = os.path.join(data_path, ENT, DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install transformers\n",
    "# !pip install seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "files = [\"train\", \"dev\", \"test\"]\n",
    "\n",
    "\n",
    "def convert(lines, f):\n",
    "    tokens_ = []\n",
    "    tags_ = []\n",
    "\n",
    "    data = {\"words\": [], \"ner\": []}\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            data[\"words\"].append(tokens_)\n",
    "            data[\"ner\"].append(tags_)\n",
    "            tokens_ = []\n",
    "            tags_ = []\n",
    "        else:\n",
    "            token, tag = line.split(\"\\t\")\n",
    "            if len(tag) > 1:\n",
    "                tag = tag.split(\"-\")[0]\n",
    "            tokens_.append(token.strip())\n",
    "            tags_.append(tag.strip())\n",
    "            \n",
    "    if len(tokens_) > 0:\n",
    "        data[\"words\"].append(tokens_)\n",
    "        data[\"ner\"].append(tags_)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writer(data, fp, add_str=\"\"):\n",
    "\n",
    "    for (tokens, tags) in zip(data[\"words\"], data[\"ner\"]):\n",
    "        for (token, tag) in zip(tokens, tags):\n",
    "            if tag == \"B\" or tag == \"I\":\n",
    "                tag += add_str\n",
    "            fp.write(\"{}\\t{}\\n\".format(token, tag))\n",
    "        fp.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert all tsv files to txt\n",
    "\n",
    "all_data = {}\n",
    "for f in files:\n",
    "    with open(os.path.join(data_dir, f + \".tsv\"), \"r\") as fp:\n",
    "        lines = fp.readlines()\n",
    "        all_data[f] = convert(lines, fp)\n",
    "    fp = open(os.path.join(data_dir, f + \".txt\"), \"w\")\n",
    "    writer(all_data[f], fp)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11167 1834 5548\n"
     ]
    }
   ],
   "source": [
    "num_train_sents = len(all_data[\"train\"][\"words\"])\n",
    "num_dev_sents = len(all_data[\"dev\"][\"words\"])\n",
    "num_test_sents = len(all_data[\"test\"][\"words\"])\n",
    "print(num_train_sents, num_dev_sents, num_test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# add the index to keep track of sentences\n",
    "train_tuples = []\n",
    "for i,(tokens,tags) in enumerate(zip(all_data[\"train\"][\"words\"],all_data[\"train\"][\"ner\"])):\n",
    "    for token,tag in zip(tokens,tags):\n",
    "        train_tuples.append([i,token,tag])\n",
    "\n",
    "test_tuples = []\n",
    "for i,(tokens,tags) in enumerate(zip(all_data[\"test\"][\"words\"],all_data[\"test\"][\"ner\"])):\n",
    "    for token,tag in zip(tokens,tags):\n",
    "        test_tuples.append([i,token,tag])\n",
    "    \n",
    "train_df = pd.DataFrame(train_tuples, columns=['sentence_id', 'words', 'labels'])\n",
    "test_df = pd.DataFrame(test_tuples, columns=['sentence_id', 'words', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentence_id              words labels\n",
      "0            0         Reactivity      O\n",
      "1            0                 of      O\n",
      "2            0        lymphocytes      B\n",
      "3            0                 to      O\n",
      "4            0                  a      O\n",
      "5            0       progesterone      O\n",
      "6            0  receptor-specific      O\n",
      "7            0         monoclonal      O\n",
      "8            0           antibody      O\n",
      "9            0                  .      O\n"
     ]
    }
   ],
   "source": [
    "print(test_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B', 'I', 'O']\n"
     ]
    }
   ],
   "source": [
    "# a list that has all possible labels \n",
    "labels = np.sort(train_df['labels'].unique()).tolist()\n",
    "label_map =  {i: label for i, label in enumerate(labels)}\n",
    "num_labels = len(labels)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_checkpoint_path = \"/sbksvol/gaurav/biobert_v1.0_pubmed_pmc/model.ckpt.index\"\n",
    "# bert_config_file = config_path\n",
    "# pytorch_dump_path = \"/sbksvol/gaurav/biobert_v1.0_pubmed_pmc_pytorch/pytorch_model.bin\"\n",
    "\n",
    "# convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, bert_config_file, pytorch_dump_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_model_path = \"/sbksvol/gaurav/biobert_v1.0_pubmed_pmc_pytorch/\"\n",
    "# config_path = \"/sbksvol/gaurav/biobert_v1.0_pubmed_pmc_pytorch/config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = dict()\n",
    "\n",
    "# Path to pretrained model or model identifier from huggingface.co/models\n",
    "model_args['model_name_or_path'] = 'dmis-lab/biobert-base-cased-v1.1'\n",
    "# saved_model_path\n",
    "# saved_model_path\n",
    "# pytorch_dump_path\n",
    "# 'dmis-lab/biobert-base-cased-v1.1'\n",
    "\n",
    "# Where do you want to store the pretrained models downloaded from s3\n",
    "model_args['cache_dir'] = \"/sbksvol/gaurav/NER_out/\"\n",
    "\n",
    "# we skip basic white-space tokenization by passing do_basic_tokenize = False to the tokenizer\n",
    "model_args['do_basic_tokenize'] = False\n",
    "\n",
    "\n",
    "data_args = dict()\n",
    "\n",
    "data_args['data_dir'] = data_dir\n",
    "\n",
    "# \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "# \"than this will be truncated, sequences shorter will be padded.\"\n",
    "data_args['max_seq_length'] = 256\n",
    "\n",
    "# Overwrite the cached training and evaluation sets\n",
    "# this means the model does not have to tokenize/preprocess and cache the data each time it's called\n",
    "# this can be made different for each NerDataset (training NerDataset, testing NerDataset)\n",
    "data_args['overwrite_cache'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.4.2'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "# code has been tested with transformers 4.4.2\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForTokenClassification,\n",
    "    BertTokenizer\n",
    ")\n",
    "\n",
    "config = BertConfig.from_pretrained(\n",
    "    model_args['model_name_or_path'],\n",
    "    num_labels=num_labels,\n",
    "    id2label=label_map,\n",
    "    label2id={label: i for i, label in enumerate(labels)},\n",
    "    cache_dir=model_args['cache_dir']\n",
    ")\n",
    "\n",
    "# we skip basic white-space tokenization by passing do_basic_tokenize = False to the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    model_args['model_name_or_path'],\n",
    "    cache_dir=model_args['cache_dir']\n",
    "#     ,do_basic_tokenize = model_args['do_basic_tokenize']\n",
    ")\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    model_args['model_name_or_path'],\n",
    "    config=config\n",
    "    ,cache_dir=model_args['cache_dir']\n",
    "#     ,from_tf=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_utils_path = \"/sbksvol/gaurav/transformers/examples/token-classification/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if data_utils_path not in sys.path:\n",
    "    sys.path.append(data_utils_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_ner import NerDataset, Split\n",
    "# %reset_selective -f \"utils_ner\"\n",
    "# NerDataset.__init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NerDataset(\n",
    "  data_dir=data_args['data_dir'],\n",
    "  tokenizer=tokenizer,\n",
    "  labels=labels,\n",
    "  model_type=config.model_type,\n",
    "  max_seq_length=data_args['max_seq_length'],\n",
    "  overwrite_cache=data_args['overwrite_cache'], # True\n",
    "  mode=Split.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = NerDataset(\n",
    "  data_dir=data_args['data_dir'],\n",
    "  tokenizer=tokenizer,\n",
    "  labels=labels,\n",
    "  model_type=config.model_type,\n",
    "  max_seq_length=data_args['max_seq_length'],\n",
    "  overwrite_cache=data_args['overwrite_cache'],\n",
    "  mode=Split.dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11166 1833\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.__len__(), eval_dataset.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.hf_argparser import HfArgumentParser\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "training_args_dict = {\n",
    "    'output_dir' : \"model_output1/\",\n",
    "    'num_train_epochs' : 10,\n",
    "    'train_batch_size': 32,\n",
    "    'seed':seed_value,\n",
    "#     'save_strategy': \"epoch\",\n",
    "    \"evaluation_strategy\": \"epoch\"\n",
    "    ,\"load_best_model_at_end\": True\n",
    "}\n",
    "\n",
    "with open('training_args.json', 'w') as fp:\n",
    "    json.dump(training_args_dict, fp)\n",
    "    \n",
    "parser = HfArgumentParser(TrainingArguments)\n",
    "# this function returns a tuple so we get the first item in the tuple since we only passed one arguement type \"TrainingArguments\"\n",
    "training_args = parser.parse_json_file(json_file=\"training_args.json\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_list_orig = []\n",
    "# for param in model.parameters():\n",
    "#     param_list_orig.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(param_list_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open(os.path.join(\"model_output1\", \"param_list_orig.pickle\"), \"wb\") as fp:\n",
    "#     pickle.dump(param_list_orig, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='13960' max='13960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13960/13960 45:18, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>0.035633</td>\n",
       "      <td>10.764500</td>\n",
       "      <td>170.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.026418</td>\n",
       "      <td>11.056900</td>\n",
       "      <td>165.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.042056</td>\n",
       "      <td>11.057800</td>\n",
       "      <td>165.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.044100</td>\n",
       "      <td>11.069500</td>\n",
       "      <td>165.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.046253</td>\n",
       "      <td>10.986400</td>\n",
       "      <td>166.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.059593</td>\n",
       "      <td>11.054600</td>\n",
       "      <td>165.814000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.064529</td>\n",
       "      <td>10.982800</td>\n",
       "      <td>166.897000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.071791</td>\n",
       "      <td>11.016700</td>\n",
       "      <td>166.383000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.071113</td>\n",
       "      <td>10.927100</td>\n",
       "      <td>167.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.073964</td>\n",
       "      <td>10.992400</td>\n",
       "      <td>166.752000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=train_dataset,\n",
    "  eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "trainOutput = trainer.train()\n",
    "\n",
    "\n",
    "# torch.save(model, os.path.join(training_args_dict[\"output_dir\"], \"pytorch_model.bin\"))\n",
    "# trainer.save_model()\n",
    "# model.save_pretrained(training_args_dict[\"output_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13960"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainOutput.global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0027, -0.0008,  0.0016], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[-1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Training args ####\n",
    "# training_args_dict = {\n",
    "#     'output_dir' : \"model_output1\",\n",
    "#     'num_train_epochs' : 10,\n",
    "#     'train_batch_size': 32,\n",
    "#     'seed':seed_value\n",
    "# }\n",
    "\n",
    "# with open('training_args.json', 'w') as fp:\n",
    "#     json.dump(training_args_dict, fp)\n",
    "    \n",
    "# parser = HfArgumentParser(TrainingArguments)\n",
    "# # this function returns a tuple so we get the first item in the tuple since we only passed one arguement type \"TrainingArguments\"\n",
    "# training_args = parser.parse_json_file(json_file=\"training_args.json\")[0]\n",
    "\n",
    "\n",
    "# saved_path = \"model_output1/checkpoint-150\"\n",
    "# #### Config ####\n",
    "# config = BertConfig.from_pretrained(\n",
    "#     saved_path,\n",
    "#     num_labels=num_labels,\n",
    "#     id2label=label_map,\n",
    "#     label2id={label: i for i, label in enumerate(labels)},\n",
    "#     cache_dir=model_args['cache_dir']\n",
    "# )\n",
    "\n",
    "# #### Model ####\n",
    "\n",
    "# reloaded_model = BertForTokenClassification.from_pretrained(\n",
    "#     saved_path,\n",
    "#     config=config,\n",
    "#     cache_dir=model_args['cache_dir']\n",
    "# )\n",
    "\n",
    "# # model = torch.load(os.path.join(training_args_dict[\"output_dir\"], \"pytorch_model.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloaded_model.to('cuda')\n",
    "# reloaded_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(reloaded_model.parameters())[-1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#   model=reloaded_model,\n",
    "#   args=training_args,\n",
    "#   train_dataset=train_dataset,\n",
    "#   eval_dataset=eval_dataset\n",
    "# )\n",
    "\n",
    "# trainer.train(\"checkpoint-150\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(reloaded_model.parameters())[-1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_list_new = []\n",
    "# for param in model.parameters():\n",
    "#     param_list_new.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(\"model_output1\", \"param_list_orig.pickle\"), \"rb\") as fp:\n",
    "#     param_list_orig = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param_orig, param_new in zip(param_list_orig, param_list_new):\n",
    "#     assert param_orig.shape == param_new.shape\n",
    "#     print(torch.norm(param_orig.data - param_new.to('cpu').data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = reloaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "import gc\n",
    "# del variables\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='694' max='694' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [694/694 00:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "# we can pass overwrite_cache as True since we might like to make new predictions by just changing test.txt \n",
    "test_dataset = NerDataset(\n",
    "  data_dir=data_args['data_dir'],\n",
    "  tokenizer=tokenizer,\n",
    "  labels=labels,\n",
    "  model_type=config.model_type,\n",
    "  max_seq_length=data_args['max_seq_length'],\n",
    "  overwrite_cache=True,\n",
    "  mode=Split.test)\n",
    "\n",
    "# last layer output/activation has the shape of (batch_size, seq_len,num_of_labels)\n",
    "output, label_ids, metrics = trainer.predict(test_dataset)\n",
    "preds = np.argmax(output, axis=2)\n",
    "num_test_samples, seq_len = preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5547\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of token-level predictions shape = (num_test_samples, seq_len)\n",
    "preds_list = [[] for _ in range(num_test_samples)]\n",
    "for i in range(num_test_samples):\n",
    "#     if i == 184:\n",
    "#         count = 0\n",
    "    for j in range(seq_len):\n",
    "#         if i == 184 and label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n",
    "#             count += 1\n",
    "#             print(j, end=' ')\n",
    "        # ignore pad_tokens\n",
    "        if label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n",
    "            preds_list[i].append(label_map[preds[i][j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_combiner(df):\n",
    "    # 'words' and 'labels' are the column names in the CSV file\n",
    "    tupple_function = lambda x: [(w, t) for w, t in zip(x[\"words\"].values.tolist(),\n",
    "                                                      x[\"labels\"].values.tolist())]\n",
    "    grouped = df.groupby(\"sentence_id\").apply(tupple_function)\n",
    "    return [s for s in grouped]\n",
    "\n",
    "testing_sentences = sentences_combiner(test_df)\n",
    "test_labels = [[w[1] for w in s] for s in testing_sentences]\n",
    "test_tokens = [[w[0] for w in s] for s in testing_sentences]\n",
    "\n",
    "# reconstruct full sentences from lists of (token,label) tuples\n",
    "# test_reconstructed = [\" \".join([w[0] for w in s] ) for s in testing_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABORT  2406\n"
     ]
    }
   ],
   "source": [
    "# make sure all test and pred sentences have the same length\n",
    "\n",
    "test_labels_new = []\n",
    "preds_list_new = []\n",
    "\n",
    "for i, x in enumerate(test_labels):\n",
    "    if len(x) == len(preds_list[i]):\n",
    "        test_labels_new.append(x)\n",
    "        preds_list_new.append(preds_list[i])\n",
    "    else:\n",
    "        print(\"ABORT \", i)\n",
    "#         print(test_df.loc[test_df['sentence_id'] == i])\n",
    "#         assert list(test_df.loc[test_df['sentence_id'] == i]['labels']) == x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(test_df.loc[test_df['sentence_id'] == 184]['words']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_level(tg):\n",
    "    gt = [np.array(x) == tg for x in test_labels]\n",
    "    pred = [np.array(y) == tg for y in preds_list]\n",
    "    \n",
    "    total = sum([np.sum(x) for x in gt])\n",
    "    \n",
    "    correct = sum([np.sum(x&y) for x, y in zip(gt, pred)])\n",
    "    \n",
    "    print(\"correct = {}, total = {}\".format(correct, total))\n",
    "    \n",
    "    acc = correct/total\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_level(\"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get entity level scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 66.3%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.63      0.70      0.66      1117\n",
      "\n",
      "   micro avg       0.63      0.70      0.66      1117\n",
      "   macro avg       0.63      0.70      0.66      1117\n",
      "weighted avg       0.63      0.70      0.66      1117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import f1_score, classification_report\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_labels_new, preds_list_new)))\n",
    "print(classification_report(test_labels_new, preds_list_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Predictions to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write the predictions to a file\n",
    "# fp = open(os.path.join(data_dir, \"pred.tsv\"), \"w\")\n",
    "# writer({\"words\": test_tokens, \"ner\": preds_list}, fp, \"-NP\")\n",
    "# fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # re-write the test file so that B and I tags have \"-NP\" attached\n",
    "# fp = open(os.path.join(data_dir, \"test_withnp.tsv\"), \"w\")\n",
    "# writer({\"words\": test_tokens, \"ner\": test_labels}, fp, \"-NP\")\n",
    "# fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get token-level scores per tag type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_level(\"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_level(\"I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_level(\"O\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy test_withnp.tsv and pred.tsv to local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kubectl cp (pod-name) \\\n",
    "# path_str1 = \"/sbksvol/gaurav/NER_Medium_data/{0}/{1}/pred.tsv /Users/apple/Desktop/test/tmp2/conll2brat/data/huggingface/{0}/pred/{1}.conll.test\".format(ENT, DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(path_str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kubectl cp (pod-name) \\\n",
    "# path_str2 = \"/sbksvol/gaurav/NER_Medium_data/{0}/{1}/test_withnp.tsv /Users/apple/Desktop/test/tmp2/conll2brat/data/huggingface/{0}/true/{1}.conll.test\".format(ENT, DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(path_str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertModel\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.init\n",
    "# from torch.autograd import Variable\n",
    "# from torch.nn.parameter import Parameter\n",
    "# import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # base class for your deep neural networks. It implements the training loop (train_net).\n",
    "# # You will need to implement the \"__init__()\" function to define the networks\n",
    "# # structures and \"forward()\", to propagate your data, in the following problems.\n",
    "# class DNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(DNN, self).__init__()\n",
    "#         pass\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         raise NotImplementedError\n",
    "    \n",
    "#     def train_net(self, epochs=9, batchSize=16):\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer = optim.Adam(self.parameters(), lr = 1e-5)\n",
    "#         data_loader = DataLoader(train_dataset, batch_size=batchSize, shuffle=True, collate_fn=collate_fn_ner)\n",
    "        \n",
    "        \n",
    "#         for epoch in range(epochs):\n",
    "#             self.train()  # set network in training mode\n",
    "#             print(\"Epoch # \", epoch)\n",
    "#             for i, (X_input_ids, X_attention_mask, X_token_type_ids, y_label_ids) in enumerate(data_loader):\n",
    "#                 print(\"Step # \", i)\n",
    "\n",
    "#                 # move tensors to GPU\n",
    "#                 X_input_ids, X_attention_mask, X_token_type_ids, y_label_ids = \\\n",
    "#                 X_input_ids.cuda(), X_attention_mask.cuda(), X_token_type_ids.cuda(), y_label_ids.cuda()\n",
    "\n",
    "#                 # Now train the model using the optimizer and the batch data\n",
    "#                 optimizer.zero_grad()\n",
    "#                 output = self.forward((X_input_ids, X_attention_mask, X_token_type_ids))\n",
    "#                 output = torch.transpose(output, 1, 2)\n",
    "#                 loss = criterion(output.float(), y_label_ids)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "                \n",
    "# #                 del X_input_ids\n",
    "# #                 del X_attention_mask\n",
    "# #                 del X_token_type_ids\n",
    "# #                 del y_label_ids\n",
    "# #                 torch.cuda.empty_cache()\n",
    "#                 # xxxxxxxxxx End of your code, don't change anything else here xxxxxxxxxx\n",
    "\n",
    "#             self.eval()  # set network in evaluation mode\n",
    "#             ckpt_path = os.path.join(model_args['cache_dir'], f\"model-{init_step + train_step_count}\")\n",
    "#             torch.save(self.state_dict(), ckpt_path)\n",
    "# #             print ('Epoch:%d Accuracy: %f'%(epoch+1, test(testData, testLabels, self))) \n",
    "    \n",
    "#     def __call__(self, x):\n",
    "#         inputs = x\n",
    "#         prediction = self.forward(inputs)\n",
    "#         return prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BertNER(DNN):\n",
    "\n",
    "#     def __init__(self, model_path, top_model, is_train=False):\n",
    "#         super(BertNER, self).__init__()\n",
    "#         self.bert = BertModel.from_pretrained(model_path, from_tf=True)\n",
    "#         self.top_model = top_model\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         X_input_ids, X_attention_mask, X_token_type_ids = x\n",
    "#         outputs = self.bert(input_ids=X_input_ids, attention_mask=X_attention_mask, token_type_ids=X_token_type_ids,\n",
    "#                             return_dict=True, output_hidden_states=True)\n",
    "        \n",
    "#         # Experiment with sum of last four hidden states\n",
    "#         last_layer = outputs.hidden_states[-1]\n",
    "#         y = self.top_model(last_layer)\n",
    "#         return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ThreeHiddenLayer(DNN):\n",
    "#     def __init__(self, hidden_units_list=[500, 250, 125], num_classes=3):\n",
    "#         super(ThreeHiddenLayer, self).__init__()\n",
    "#         hid1, hid2, hid3 = hidden_units_list\n",
    "#         self.embedding_size = 768\n",
    "#         self.fc1 = nn.Linear(self.embedding_size, hid1)\n",
    "#         self.fc2 = nn.Linear(hid1, hid2)\n",
    "#         self.fc3 = nn.Linear(hid2, hid3)\n",
    "#         self.fc4 = nn.Linear(hid3, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = F.relu(self.fc3(x))\n",
    "#         x = self.fc4(x)\n",
    "#         return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn_ner(batch):\n",
    "#     X_input_ids = torch.LongTensor([x.input_ids for x in batch])\n",
    "#     X_token_type_ids = torch.LongTensor([x.token_type_ids for x in batch])\n",
    "#     X_attention_mask = torch.FloatTensor([x.attention_mask for x in batch])\n",
    "    \n",
    "#     y_label_ids = [x.label_ids for x in batch]\n",
    "\n",
    "#     return X_input_ids, X_attention_mask, X_token_type_ids, torch.LongTensor(y_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threeHiddenLayerClassifier = ThreeHiddenLayer()\n",
    "# bertNER = BertNER(saved_model_path, threeHiddenLayerClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bertNER = bertNER.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bertNER.train_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "# # import gc\n",
    "# # del variables\n",
    "# # gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train top-model using the Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model.parameters())[-1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gene, bioinfer\n",
    "# seed = 42 -> 0.86      0.87      0.87\n",
    "# seed = 0 -> 0.85      0.89      0.87\n",
    "# seed = 13 -> 0.85      0.86      0.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellline, jnlpba\n",
    "# seed = 42 -> 0.63      0.70      0.66\n",
    "# seed = 0 ->\n",
    "# seed = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.9.0+cpu\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/cpu/torch-1.9.0%2Bcpu-cp36-cp36m-linux_x86_64.whl (175.5MB)\n",
      "\u001b[K     |████████████████████████████████| 175.5MB 101kB/s  eta 0:00:011     |███████████████████████████████▋| 173.5MB 46.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision==0.10.0+cpu\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.10.0%2Bcpu-cp36-cp36m-linux_x86_64.whl (15.7MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7MB 54.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchaudio==0.9.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/75/e432d6c58771668ed917038a6d473edfdd5465640eec169f49a823ecf0cc/torchaudio-0.9.0-cp36-cp36m-manylinux1_x86_64.whl (1.9MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9MB 33.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.9.0+cpu) (3.7.4.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.10.0+cpu) (1.17.4)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.10.0+cpu) (6.2.1)\n",
      "Installing collected packages: dataclasses, torch, torchvision, torchaudio\n",
      "Successfully installed dataclasses-0.8 torch-1.9.0+cpu torchaudio-0.9.0 torchvision-0.10.0+cpu\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch==1.9.0+cpu torchvision==0.10.0+cpu torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/9e/5b80becd952d5f7250eaf8fc64b957077b12ccfe73e9c03d37146ab29712/transformers-4.6.0-py3-none-any.whl (2.3MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3MB 37.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytorch-crf\n",
      "  Downloading https://files.pythonhosted.org/packages/96/7d/4c4688e26ea015fc118a0327e5726e6596836abce9182d3738be8ec2e32a/pytorch_crf-0.7.2-py3-none-any.whl\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.6.0) (1.3.0)\n",
      "Collecting tqdm>=4.27\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/ec/f8ff3ccfc4e59ce619a66a0bf29dc3b49c2e8c07de29d572e191c006eaa2/tqdm-4.61.2-py2.py3-none-any.whl (76kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 4.0MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.6.0) (1.17.4)\n",
      "Collecting huggingface-hub==0.0.8\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.6.0) (19.2)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/20/3605db440db4f96d5ffd66b231a043ae451ec7e5e4d1a2fb6f20608006c4/tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 47.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading https://files.pythonhosted.org/packages/93/83/71a2ee6158bb9f39a90c0dea1637f81d5eef866e188e1971a1b1ab01a35a/filelock-3.0.12-py3-none-any.whl\n",
      "Collecting regex!=2019.12.17\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/e5/c19b88a1f08d988f31f07318ff3e794a0a8db55a4b55bf6e81a6a5ec5506/regex-2021.7.6-cp36-cp36m-manylinux2014_x86_64.whl (722kB)\n",
      "\u001b[K     |████████████████████████████████| 727kB 64.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.6.0) (0.8)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001b[K     |████████████████████████████████| 901kB 62.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.6.0) (2.22.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0) (0.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.6.0) (2.4.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.6.0) (1.12.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.6.0) (0.13.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.6.0) (7.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.6.0) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.6.0) (1.25.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.6.0) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.6.0) (2.6)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->transformers==4.6.0) (8.0.2)\n",
      "Installing collected packages: tqdm, filelock, huggingface-hub, tokenizers, regex, sacremoses, transformers, pytorch-crf\n",
      "Successfully installed filelock-3.0.12 huggingface-hub-0.0.8 pytorch-crf-0.7.2 regex-2021.7.6 sacremoses-0.0.45 tokenizers-0.10.3 tqdm-4.61.2 transformers-4.6.0\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers==4.6.0 pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed value\n",
    "seed_value= 42\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "# 4. Set `pytorch` pseudo-random generator at a fixed value\n",
    "import torch\n",
    "torch.manual_seed(seed_value)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/sbksvol/nikhil/NER_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENT = \"Gene\"\n",
    "DATASET = \"cellfinder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = os.path.join(data_path, ENT, DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 1.3MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.17.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.21.3)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (0.13.2)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-cp36-none-any.whl size=16172 sha256=b9408e8b5eeb10b55ff2f8da07ef087c0bf0bed9f7ba2a1b7b5de6f6397563e7\n",
      "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install pytorch-crf\n",
    "!pip3 install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcrf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0715 01:58:11.937323 140714055468864 font_manager.py:1349] generated new fontManager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_sents, num_dev_sents, num_test_sents =  1253 422 504\n",
      "First 10 words in test data:\n",
      "   sentence_id            words labels\n",
      "0            0  BackgroundUsing      O\n",
      "1            0       antibodies      O\n",
      "2            0               to      O\n",
      "3            0         specific      O\n",
      "4            0          protein      O\n",
      "5            0         antigens      O\n",
      "6            0               is      O\n",
      "7            0              the      O\n",
      "8            0           method      O\n",
      "9            0               of      O\n",
      "unique labels: ['B', 'I', 'O']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0715 01:58:23.330041 140714055468864 filelock.py:274] Lock 140710815215968 acquired on /sbksvol/nikhil/NER_data/Gene/cellfinder/cached_train_BertTokenizer_256.lock\n",
      "I0715 01:58:23.397220 140714055468864 utils_ner.py:111] Creating features from dataset file at /sbksvol/nikhil/NER_data/Gene/cellfinder\n",
      "I0715 01:58:23.435099 140714055468864 utils_ner.py:299] Writing example 0 of 1252\n",
      "I0715 01:58:23.437452 140714055468864 utils_ner.py:378] *** Example ***\n",
      "I0715 01:58:23.437975 140714055468864 utils_ner.py:379] guid: train-1\n",
      "I0715 01:58:23.439040 140714055468864 utils_ner.py:380] tokens: [CLS] Background ##H ##uman em ##b ##ryo ##nic stem cells provide access to the earliest stages of human development and may serve as a source of specialized cells for re ##gene ##rative medicine . [SEP]\n",
      "I0715 01:58:23.439544 140714055468864 utils_ner.py:381] input_ids: 101 24570 3048 19147 9712 1830 26503 7770 8175 3652 2194 2469 1106 1103 5041 5251 1104 1769 1718 1105 1336 2867 1112 170 2674 1104 7623 3652 1111 1231 27054 15306 5182 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0715 01:58:23.440011 140714055468864 utils_ner.py:382] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0715 01:58:23.440501 140714055468864 utils_ner.py:383] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0715 01:58:23.441066 140714055468864 utils_ner.py:384] label_ids: -100 2 -100 -100 2 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0715 01:58:23.447794 140714055468864 utils_ner.py:378] *** Example ***\n",
      "I0715 01:58:23.448256 140714055468864 utils_ner.py:379] guid: train-2\n",
      "I0715 01:58:23.448780 140714055468864 utils_ner.py:380] tokens: [CLS] Thus , it becomes crucial to develop protocols for the directed differentiation of em ##b ##ryo ##nic stem cells into tissue - restricted precursor ##s . Method ##s and Finding ##s Here , we present culture conditions for the der ##ivation of unlimited numbers of pure me ##sen ##chy ##mal precursor ##s from human em ##b ##ryo ##nic stem cells and demonstrate multi ##line ##age differentiation into fat , cart ##ila ##ge , bone , and skeletal muscle cells . Con ##c ##lusion ##O ##ur findings will help to el ##uc ##ida ##te the mechanism of me ##so ##der ##m specification during em ##b ##ryo ##nic stem cell differentiation and provide a platform to efficiently generate specialized human me ##sen ##chy ##mal cell types for future clinical applications . Lo ##ren ##z St ##ude ##r and colleagues describe the use of em ##b ##ryo ##nic stem cells to derive me ##sen ##chy ##mal precursor ##s and then fat , cart ##ila ##ge , bone , and skeletal muscle cells . [SEP]\n",
      "I0715 01:58:23.449327 140714055468864 utils_ner.py:381] input_ids: 101 4516 117 1122 3316 10268 1106 3689 19755 1111 1103 2002 23510 1104 9712 1830 26503 7770 8175 3652 1154 7918 118 7458 15985 1116 119 20569 1116 1105 18036 1116 3446 117 1195 1675 2754 2975 1111 1103 4167 16617 1104 22921 2849 1104 5805 1143 3792 8992 7435 15985 1116 1121 1769 9712 1830 26503 7770 8175 3652 1105 10541 4321 2568 2553 23510 1154 7930 117 12411 8009 2176 117 6028 117 1105 23400 6484 3652 119 16752 1665 17855 2346 2149 9505 1209 1494 1106 8468 21977 6859 1566 1103 6978 1104 1143 7301 2692 1306 14911 1219 9712 1830 26503 7770 8175 2765 23510 1105 2194 170 3482 1106 19723 9509 7623 1769 1143 3792 8992 7435 2765 3322 1111 2174 7300 4683 119 10605 5123 1584 1457 10308 1197 1105 8304 5594 1103 1329 1104 9712 1830 26503 7770 8175 3652 1106 20292 1143 3792 8992 7435 15985 1116 1105 1173 7930 117 12411 8009 2176 117 6028 117 1105 23400 6484 3652 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0715 01:58:23.449922 140714055468864 utils_ner.py:382] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0715 01:58:23.450434 140714055468864 utils_ner.py:383] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0715 01:58:23.451029 140714055468864 utils_ner.py:384] label_ids: -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 2 2 2 2 -100 -100 2 -100 2 -100 -100 2 2 -100 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 -100 -100 -100 2 -100 2 2 2 -100 -100 -100 2 2 2 2 2 -100 -100 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 2 2 2 2 2 -100 -100 -100 2 2 2 2 -100 -100 -100 2 2 2 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 2 2 2 2 2 2 -100 -100 -100 -100 2 -100 -100 2 2 2 2 2 2 2 -100 -100 -100 2 2 2 2 2 -100 -100 -100 2 -100 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0715 01:58:23.453650 140714055468864 utils_ner.py:378] *** Example ***\n",
      "I0715 01:58:23.454123 140714055468864 utils_ner.py:379] guid: train-3\n",
      "I0715 01:58:23.454557 140714055468864 utils_ner.py:380] tokens: [CLS] Em ##b ##ryo ##nic stem ( E ##S ) cells are p ##lu ##rip ##ote ##nt cells derived from the inner cell mass of the blast ##oc ##ys ##t that can be maintained in culture for an extended period of time without losing differentiation potential . [SEP]\n",
      "I0715 01:58:23.455043 140714055468864 utils_ner.py:381] input_ids: 101 18653 1830 26503 7770 8175 113 142 1708 114 3652 1132 185 7535 16669 11860 2227 3652 4408 1121 1103 5047 2765 3367 1104 1103 9232 13335 6834 1204 1115 1169 1129 4441 1107 2754 1111 1126 2925 1669 1104 1159 1443 3196 23510 3209 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0715 01:58:23.455637 140714055468864 utils_ner.py:382] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0715 01:58:23.456238 140714055468864 utils_ner.py:383] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0715 01:58:23.456842 140714055468864 utils_ner.py:384] label_ids: -100 2 -100 -100 -100 2 2 2 -100 2 2 2 2 -100 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0715 01:58:23.459489 140714055468864 utils_ner.py:378] *** Example ***\n",
      "I0715 01:58:23.460021 140714055468864 utils_ner.py:379] guid: train-4\n",
      "I0715 01:58:23.460494 140714055468864 utils_ner.py:380] tokens: [CLS] The successful isolation of human E ##S cells ( h ##ES ##Cs ) has raised the hope that these cells may provide a universal tissue source to treat many human diseases . [SEP]\n",
      "I0715 01:58:23.461135 140714055468864 utils_ner.py:381] input_ids: 101 1109 2265 13345 1104 1769 142 1708 3652 113 177 9919 18363 114 1144 2120 1103 2810 1115 1292 3652 1336 2194 170 8462 7918 2674 1106 7299 1242 1769 8131 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0715 01:58:23.461728 140714055468864 utils_ner.py:382] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0715 01:58:23.462310 140714055468864 utils_ner.py:383] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0715 01:58:23.462945 140714055468864 utils_ner.py:384] label_ids: -100 2 2 2 2 2 2 -100 2 2 2 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0715 01:58:23.464735 140714055468864 utils_ner.py:378] *** Example ***\n",
      "I0715 01:58:23.465325 140714055468864 utils_ner.py:379] guid: train-5\n",
      "I0715 01:58:23.465835 140714055468864 utils_ner.py:380] tokens: [CLS] However , directed differentiation of h ##ES ##Cs into specific tissue types poses a formidable challenge . [SEP]\n",
      "I0715 01:58:23.466444 140714055468864 utils_ner.py:381] input_ids: 101 1438 117 2002 23510 1104 177 9919 18363 1154 2747 7918 3322 25366 170 20466 4506 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0715 01:58:23.467069 140714055468864 utils_ner.py:382] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0715 01:58:23.467663 140714055468864 utils_ner.py:383] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0715 01:58:23.468255 140714055468864 utils_ner.py:384] label_ids: -100 2 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0715 01:58:25.316112 140714055468864 utils_ner.py:134] Saving features into cached file /sbksvol/nikhil/NER_data/Gene/cellfinder/cached_train_BertTokenizer_256\n",
      "I0715 01:58:26.189919 140714055468864 filelock.py:318] Lock 140710815215968 released on /sbksvol/nikhil/NER_data/Gene/cellfinder/cached_train_BertTokenizer_256.lock\n"
     ]
    }
   ],
   "source": [
    "from run_v2 import prepare_data, params, prepare_config_and_tokenizer\n",
    "from utils_ner import NerDataset, Split\n",
    "from models_factory import get_model\n",
    "\n",
    "params[\"entity_type\"] = ENT\n",
    "params[\"dataset\"] = DATASET\n",
    "\n",
    "params[\"LOWER_CASE\"] = False\n",
    "params[\"LOAD_BEST_MODEL\"] = True\n",
    "params[\"MAX_LEN\"] = 256\n",
    "params[\"BATCH_SIZE\"] = 64\n",
    "params[\"EPOCH_TOP\"] = 100\n",
    "params[\"EPOCH_END2END\"] = 100\n",
    "\n",
    "params[\"EXP_NAME\"] = \"test\"\n",
    "params[\"WORKING_DIR\"] = \"sbksvol/nikhil/\" + params[\"EXP_NAME\"] + \"/\"\n",
    "params[\"CACHE_DIR\"] = params[\"WORKING_DIR\"] + \"NER_out_test/\"\n",
    "\n",
    "# Where model checkpoints are stored.\n",
    "params[\"OUTPUT_DIR\"] = params[\"WORKING_DIR\"] + \"model_output_test/\"\n",
    "params[\"TRAIN_ARGS_FILE\"] = params[\"WORKING_DIR\"] + \"train_args_test.json\"\n",
    "params[\"DATA_PATH\"] = data_path\n",
    "\n",
    "train_df, test_df, dev_df, labels, num_labels, label_map, data_dir = prepare_data()\n",
    "\n",
    "data_args, model_args, config, tokenizer = prepare_config_and_tokenizer(\n",
    "    data_dir, labels, num_labels, label_map)\n",
    "\n",
    "# ## Create Dataset Objects\n",
    "\n",
    "train_dataset = NerDataset(\n",
    "    data_dir=data_args['data_dir'],\n",
    "    tokenizer=tokenizer,\n",
    "    labels=labels,\n",
    "    model_type=config.model_type,\n",
    "    max_seq_length=data_args['max_seq_length'],\n",
    "    overwrite_cache=data_args['overwrite_cache'],  # True\n",
    "    mode=Split.train, data_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputFeatures(input_ids=[101, 25549, 118, 9618, 2765, 26258, 113, 6820, 12122, 114, 113, 2891, 1559, 1495, 118, 153, 2036, 132, 7642, 1813, 2107, 15016, 117, 1727, 4494, 117, 1756, 117, 1244, 1311, 114, 1108, 1982, 1113, 170, 12556, 2271, 2858, 113, 27688, 18778, 1891, 117, 3144, 6266, 117, 4369, 117, 1244, 1311, 114, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[-100, 2, -100, -100, 2, 2, 2, 2, -100, -100, 2, 0, -100, -100, -100, -100, -100, 2, 2, -100, -100, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, 2, 2, -100, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100])\n",
      "['[CLS]', 'Flow', '-', 'activated', 'cell', 'sorting', '(', 'FA', '##CS', ')', '(', 'CD', '##7', '##3', '-', 'P', '##E', ';', 'Ph', '##ar', '##M', '##ingen', ',', 'San', 'Diego', ',', 'California', ',', 'United', 'States', ')', 'was', 'performed', 'on', 'a', 'Mo', '##F', '##lo', '(', 'Cy', '##tom', '##ation', ',', 'Fort', 'Collins', ',', 'Colorado', ',', 'United', 'States', ')', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.__getitem__(15))\n",
    "print(tokenizer.convert_ids_to_tokens(train_dataset.__getitem__(15).input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertNERCRFFCN(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (top_layers): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): FullyConnectedLayers(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (3): Dropout(p=0.2, inplace=False)\n",
       "        (4): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (7): Dropout(p=0.2, inplace=False)\n",
       "        (8): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (11): Dropout(p=0.2, inplace=False)\n",
       "        (12): Linear(in_features=32, out_features=3, bias=True)\n",
       "        (13): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (crf): CRF(num_tags=3)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.models_enum import ModelsType\n",
    "from models.models import BertNERCRFFCN, BertNERCRF\n",
    "model = get_model(\n",
    "        model_path=\"/sbksvol/nikhil/gene_cell_fcn_crf_lr5_6_wd3/model_output_test/checkpoint-2613/\",\n",
    "        cache_dir=model_args['cache_dir'],\n",
    "        config=None,\n",
    "        model_type=ModelsType.FCN_CRF)\n",
    "# model = BertNERCRFFCN.from_pretrained(\n",
    "#             \"/sbksvol/nikhil/gene_cell_fcn_crf_lr5_6_wd3/model_output_test/checkpoint-2613/\",\n",
    "#             xargs = {}\n",
    "#         )\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.bert.encoder.layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29363\n",
      "28996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['HP', '##WR', '##12', '##P', '##14', '##2']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer.add_tokens(list(train_df[train_df['labels']=='B']['words'].unique()))\n",
    "print(len(tokenizer))\n",
    "print(len(tokenizer.vocab))\n",
    "tokenizer.tokenize(\"HPWR12P142\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(29363, 768)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_id         15\n",
       "words          CD73-PE\n",
       "labels               B\n",
       "Name: 548, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.loc[548]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sentence_id         words labels\n",
      "548             15       CD73-PE      B\n",
      "720             21        TGF-β3      B\n",
      "936             34          VCAM      B\n",
      "938             34        STRO-1      B\n",
      "940             34  ICAM-1(CD54)      B\n",
      "...            ...           ...    ...\n",
      "35273         1237          Oct4      B\n",
      "35275         1237         Nanog      B\n",
      "35279         1238         FOXa2      B\n",
      "35281         1238        HNF3B)      B\n",
      "35283         1238     Brachyury      B\n",
      "\n",
      "[848 rows x 3 columns]\n",
      "0        False\n",
      "1        False\n",
      "2        False\n",
      "3        False\n",
      "4        False\n",
      "         ...  \n",
      "35527    False\n",
      "35528    False\n",
      "35529    False\n",
      "35530    False\n",
      "35531    False\n",
      "Name: labels, Length: 35532, dtype: bool\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['CD73-PE', 'TGF-β3', 'VCAM', 'STRO-1', 'ICAM-1(CD54)', 'CD105',\n",
       "       'CD29', 'MF20', 'CD73', 'CD44', 'ALCAM(CD166', 'MyoD', 'nestin',\n",
       "       'vimentin', 'alpha', 'fast-switch', 'pan-cytokeratin', 'human',\n",
       "       'phosphatase', 'extracellular', 'proteoglycans', 'CD73+',\n",
       "       'CD105(SH2', 'CD106', 'CD29(integrin', 'β1', 'ICAM', 'CD34',\n",
       "       'CD45', 'CD14', 'pancytokeratin', 'desmin', 'Housekeeping',\n",
       "       'DSC54', 'neuropilin', 'hepatocyte', 'forkhead', 'notch', 'PPARγ',\n",
       "       'collagen', 'aggrecan', 'β-glycerolphosphate', 'bone-specific',\n",
       "       'bone', 'MyoD+', 'myosin', 'heavy', 'myogenin', 'Nanog', 'Oct-4',\n",
       "       'CD4', 'CCR5', 'CXCR4', 'HLA-DR', 'B7.1', 'GFP', 'siRNAs', 'bFGF',\n",
       "       'GM-CSF', 'M-CSF', 'derived', 'class', 'EGFP', 'CD34)', 'PECY5',\n",
       "       'MHC', 'LPS', 'IL-1', 'TNF-α', 'LPS.', 'TNFα', 'GFP-ES', 'MHCII',\n",
       "       'B7', 'molecules', 'anti-HIV', 'collagenase', 'CD34+',\n",
       "       'trypsin/EDTA', 'M-CSF.', 'PE-CD14', 'PE-HLA-DR', 'PECY5-CD4',\n",
       "       'PECY5-CCR5', 'PECY5-CXCR4', 'costimulatory', 'GFP-alone',\n",
       "       'anti-B7.1', 'Oct4', 'Myf-5', 'delta-Notch', 'Myf5', 'M-cadherin',\n",
       "       'Paired', 'Pax7', 'embryonic', 'desmin+', 'Notch',\n",
       "       'desmin+/Myf5+/BrdU+', 'Ki67', 'Oct4+', 'M-cadherin−/desmin+',\n",
       "       'M-cadherin−/desmin−', 'M-cadherin+/desmin+', 'desmin+/BrdU+',\n",
       "       'desmin-specific', 'nuclear', 'NuMA', 'eMyHC', 'desmin)',\n",
       "       'eMyHC-specific', 'NuMA+', 'NuMA−/desmin+', 'NuMA+/desmin+',\n",
       "       'NuMA+/Oct4+', 'Numa+/Oct4+', 'Collagenase', 'hbFGF', 'hbFGF.',\n",
       "       'Dispase', 'Trypsin/EDTA', 'Myf5/Pax7', 'BrdU', 'Desmin', 'FK506',\n",
       "       'DNaseI', 'DNaseI-hypersensitive', 'TAL1/SCL', 'DNaseI-digested',\n",
       "       'TAL1', 'acute', 'SCL', 'stem', 'RNaseA', 'proteinaseK', 'DNaseI.',\n",
       "       'T4', 'polymerase', 'Stil', 'DNaseI-treated', 'porphobilinogen',\n",
       "       'Hmbs', 'DNaseI-digestion', 'STIL', 'BglII', 'haematopoietic',\n",
       "       'endothelial-haematopoietic', 'erythroid', '+19', '+50', 'mouse',\n",
       "       'Activin', 'FGF2', 'protease', 'proteinase', 'Vent', 'β-actin',\n",
       "       'MAP', 'MAPK)', 'trypsin-', 'AKT', 'caveolin1', 'ERK1', 'GS15',\n",
       "       'ABP-280', 'B2', 'Karyopherin', 'BiP', 'Inhibitor', 'Caspase',\n",
       "       'OXA1Hs', 'protein', 'phosphatases', 'fibronectin', 'STAT3', 'FGF',\n",
       "       'PI3', 'Src', 'MAPK', 'GSK3', 'p38', 'cell', 'neurotensin',\n",
       "       'endothelin', 'thrombin', 'glial', 'Smad2/3', 'phospho-GSK3',\n",
       "       'Connexin', 'E-Cad', 'GDNFRα', 'HSP70', 'tyrosine', 'GTPases',\n",
       "       'AIM-1', 'NrBMXP51813MEK2P36506CaM', 'Kinase',\n",
       "       'alpha/SAPK2aQ16539Casein', 'CP17612Cdk5Q00535PKC',\n",
       "       'betaP05771Cdk7P50613PKC', 'deltaQ05655DAP', 'KinaseP53355PP2A',\n",
       "       'betaP18266RbP13405I', 'kappa',\n",
       "       'betaO14920Stat1A46159JAK1P23458Stat3P52631JNK1P45983VHRP51452MEK1Q02750',\n",
       "       'IKKgamma', 'E-CAD', 'Hsp70', 'CtBP1', 'CtBP2', 'GS-28', 'HDJ-2',\n",
       "       'Hsp40', 'heat', 'L-Caldesmon', 'Rabaptin', 'phosphorylated-p130',\n",
       "       'Cas', 'Crk-associated', 'Ras-GAP', 'phosphorylated', 'p21ras',\n",
       "       'ShcC', 'hESCs', 'Ras-GAP.', 'phospho-p130', 'TNIK', 'p130',\n",
       "       'Traf2', 'TNIK)', 'F-actin', 'p130Cas', 'TGFβ',\n",
       "       'GSK3β/Wnt/β-catenin', 'Jak/Stat', 'MAPK/ERK', 'Gap', 'Wnt',\n",
       "       'Stat1', 'SMADs', 'GSK3β', 'β-catenin', 'βStat1++',\n",
       "       'Jun+++Smad4/DPC4+++Endoglin+-WntCtBP2++', 'Catalytic',\n",
       "       'D3/CCND3++', 'beta++', 'Jun+++Casein',\n",
       "       'Receptor/PAR1/F2R+++SHPS-1/PTPNS1++', 'Jun+++Bcl-x/BCL2L1++',\n",
       "       'Bradykinin', 'Receptor', '3.4.24.16/NLN++', 'C++-PKA', 'RI',\n",
       "       'alpha++-C-Raf/RAF1++', 'iota++', 'beta/PRKCB1++',\n",
       "       'alpha/Akt+++GSK-3', 'Acid', 'Jun+++RAFT1/FRAP++',\n",
       "       'p170/PIK3C2A++PTP1B/PTPN1+++Dok1/p62dok+++PI3-Kinase', 'p110',\n",
       "       'alpha++-Yes', 'alpha/SAPK2a++-G3BP++', 'Inhibitor2/PPP1R2++',\n",
       "       'epsilon/YWHAE++', 'IGF', 'ERBB2', 'GPCR', 'GDNF', 'ZO1',\n",
       "       'occludin', 'EGF)-receptor', 'EGF-family', 'Heregulin', 'ERBB3',\n",
       "       'ERBB', 'ERBB2/3', 'accutase', 'Occludin', 'IGF1R', 'tight',\n",
       "       'transferrin', 'hergulin1β', 'activinA', 'LR3-IGF1', 'P190',\n",
       "       'Adaptin', 'STAT-3', 'PTP1D', 'Mek-2', 'RACK-1', 'GRB-2', 'Rap2',\n",
       "       'Exportin-1/CRM1', 'MCM', 'Nucleoporin', 'α-tubulin', 'Actin',\n",
       "       'KNP-1/HES1', 'NTF2', 'p190', 'Hip1R', 'Transportin',\n",
       "       'Calreticulin', 'Arp3', 'eIF-6', 'horse', 'peroxidase',\n",
       "       'Rabaptin-5', 'phospho-Ras-GAP', 'Shc-C', 'epidermal', 'EGF)',\n",
       "       'basic', 'leukemia', 'LIF', 'EGF', 'LIF.', 'trypsin-EDTA.', '3CB2',\n",
       "       'brachyury', 'foxa2', 'Vimentin', 'Notch1', 'neural', 'β-tubulin',\n",
       "       'medium-size', 'NF-M)', 'microtubule-associated', 'MAP-2', 'GFAP',\n",
       "       'myelin', 'MBP', 'glutamic', 'GAD', 'TH)', 'nestin+', 'TuJ1',\n",
       "       'galactocerebrocide', 'GC', 'GFAP+', 'nestin.10.1371/journal',\n",
       "       'NCAM', 'MBP)', 'TH.', 'GC-expressing', 'TuJ1+', 'hNuc', 'GluT1',\n",
       "       'doublecortin', 'CNPase-expressing', 'glutamate', 'Ki67+', 'hNuc+',\n",
       "       'hNuc+(', 'CNPase', 'GAD65/67', 'Sox1', 'heparin', 'insulin',\n",
       "       'trypsin', 'trypsin-EDTA', 'Anti-human', 'Anti-TuJ1',\n",
       "       'anti-GAD65/67', 'Anti-glial', 'Anti-galactocerebrocide',\n",
       "       'Anti-CNPase', 'Anti-Glucose', 'Transporter', 'Glut-1',\n",
       "       'Anti-Nestin', 'Anti-vimentin', 'Anti-3CB2', 'Anti-doublecortin',\n",
       "       'Anti-Ki67', 'RNase', 'Transcriptase', 'MAP2', 'N-CAM', 'Nestin',\n",
       "       'NF-M', 'Notch-1', 'FOXa2', 'HNF3B)', 'Brachyury'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = temp_df.copy()\n",
    "print(train_df[train_df['labels']=='B'])\n",
    "print(train_df['labels']=='B')\n",
    "train_df.loc[temp_df['labels']=='B',['words']] = temp_df[temp_df['labels']=='B']['words'].apply(lambda x:x.replace('-','-'))\n",
    "train_df[train_df['labels']=='B']['words'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 11164, 100, 100, 1769, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 23555, 100, 100, 100, 100, 100, 6028, 100, 100, 2302, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 4408, 1705, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 10799, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 4272, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 12104, 100, 8175, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 10322, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 4592, 100, 100, 100, 100, 100, 100, 100, 100, 100, 2765, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 3208, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 16580, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 3600, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 3241, 100, 100, 100, 100, 100, 100, 3501, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 18250, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 26825, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'alpha', '[UNK]', '[UNK]', 'human', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'notch', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'bone', '[UNK]', '[UNK]', 'heavy', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'derived', 'class', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'molecules', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'nuclear', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'acute', '[UNK]', 'stem', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'mouse', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'protein', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'cell', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'heat', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'Gap', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'tight', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'horse', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'basic', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'neural', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'insulin', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]']\n",
      "['CD', '##7', '##3', '-', 'P', '##E']\n",
      "['T', '##G', '##F', '-', 'β', '##3']\n",
      "['VC', '##AM']\n",
      "['ST', '##RO', '-', '1']\n",
      "['I', '##CA', '##M', '-', '1', '(', 'CD', '##5', '##4', ')']\n",
      "['CD', '##10', '##5']\n",
      "['CD', '##29']\n",
      "['M', '##F', '##20']\n",
      "['CD', '##7', '##3']\n",
      "['CD', '##44']\n",
      "['AL', '##CA', '##M', '(', 'CD', '##16', '##6']\n",
      "['My', '##o', '##D']\n",
      "['nest', '##in']\n",
      "['v', '##iment', '##in']\n",
      "['alpha']\n",
      "['fast', '-', 'switch']\n",
      "['pan', '-', 'c', '##yt', '##oker', '##ati', '##n']\n",
      "['human']\n",
      "['p', '##hos', '##pha', '##tase']\n",
      "['extra', '##cellular']\n",
      "['pro', '##te', '##og', '##ly', '##cans']\n",
      "['CD', '##7', '##3', '+']\n",
      "['CD', '##10', '##5', '(', 'SH', '##2']\n",
      "['CD', '##10', '##6']\n",
      "['CD', '##29', '(', 'in', '##te', '##g', '##rin']\n",
      "['β', '##1']\n",
      "['I', '##CA', '##M']\n",
      "['CD', '##34']\n",
      "['CD', '##45']\n",
      "['CD', '##14']\n",
      "['pan', '##cy', '##tok', '##era', '##tin']\n",
      "['des', '##min']\n",
      "['House', '##keeping']\n",
      "['DS', '##C', '##5', '##4']\n",
      "['ne', '##uro', '##pi', '##lin']\n",
      "['he', '##pa', '##to', '##cy', '##te']\n",
      "['fork', '##head']\n",
      "['notch']\n",
      "['PP', '##AR', '##γ']\n",
      "['co', '##lla', '##gen']\n",
      "['a', '##gg', '##re', '##can']\n",
      "['β', '-', 'g', '##ly', '##cer', '##ol', '##ph', '##os', '##phate']\n",
      "['bone', '-', 'specific']\n",
      "['bone']\n",
      "['My', '##o', '##D', '+']\n",
      "['my', '##os', '##in']\n",
      "['heavy']\n",
      "['my', '##ogen', '##in']\n",
      "['Nan', '##og']\n",
      "['Oct', '-', '4']\n",
      "['CD', '##4']\n",
      "['CC', '##R', '##5']\n",
      "['C', '##X', '##CR', '##4']\n",
      "['H', '##LA', '-', 'DR']\n",
      "['B', '##7', '.', '1']\n",
      "['G', '##FP']\n",
      "['si', '##RNA', '##s']\n",
      "['b', '##F', '##G', '##F']\n",
      "['GM', '-', 'CS', '##F']\n",
      "['M', '-', 'CS', '##F']\n",
      "['derived']\n",
      "['class']\n",
      "['E', '##G', '##FP']\n",
      "['CD', '##34', ')']\n",
      "['P', '##EC', '##Y', '##5']\n",
      "['M', '##HC']\n",
      "['LP', '##S']\n",
      "['IL', '-', '1']\n",
      "['T', '##NF', '-', 'α']\n",
      "['LP', '##S', '.']\n",
      "['T', '##NF', '##α']\n",
      "['G', '##FP', '-', 'E', '##S']\n",
      "['M', '##HC', '##II']\n",
      "['B', '##7']\n",
      "['molecules']\n",
      "['anti', '-', 'HIV']\n",
      "['co', '##lla', '##gen', '##ase']\n",
      "['CD', '##34', '+']\n",
      "['try', '##ps', '##in', '/', 'E', '##D', '##TA']\n",
      "['M', '-', 'CS', '##F', '.']\n",
      "['P', '##E', '-', 'CD', '##14']\n",
      "['P', '##E', '-', 'H', '##LA', '-', 'DR']\n",
      "['P', '##EC', '##Y', '##5', '-', 'CD', '##4']\n",
      "['P', '##EC', '##Y', '##5', '-', 'CC', '##R', '##5']\n",
      "['P', '##EC', '##Y', '##5', '-', 'C', '##X', '##CR', '##4']\n",
      "['cost', '##im', '##ulatory']\n",
      "['G', '##FP', '-', 'alone']\n",
      "['anti', '-', 'B', '##7', '.', '1']\n",
      "['Oct', '##4']\n",
      "['My', '##f', '-', '5']\n",
      "['delta', '-', 'Not', '##ch']\n",
      "['My', '##f', '##5']\n",
      "['M', '-', 'ca', '##dh', '##eri', '##n']\n",
      "['Pa', '##ired']\n",
      "['Pa', '##x', '##7']\n",
      "['em', '##b', '##ryo', '##nic']\n",
      "['des', '##min', '+']\n",
      "['Not', '##ch']\n",
      "['des', '##min', '+', '/', 'My', '##f', '##5', '+', '/', 'B', '##rd', '##U', '+']\n",
      "['Ki', '##6', '##7']\n",
      "['Oct', '##4', '+']\n",
      "['M', '-', 'ca', '##dh', '##eri', '##n', '##−', '/', 'des', '##min', '+']\n",
      "['M', '-', 'ca', '##dh', '##eri', '##n', '##−', '/', 'des', '##min', '##−']\n",
      "['M', '-', 'ca', '##dh', '##eri', '##n', '+', '/', 'des', '##min', '+']\n",
      "['des', '##min', '+', '/', 'B', '##rd', '##U', '+']\n",
      "['des', '##min', '-', 'specific']\n",
      "['nuclear']\n",
      "['N', '##u', '##MA']\n",
      "['e', '##M', '##y', '##HC']\n",
      "['des', '##min', ')']\n",
      "['e', '##M', '##y', '##HC', '-', 'specific']\n",
      "['N', '##u', '##MA', '+']\n",
      "['N', '##u', '##MA', '##−', '/', 'des', '##min', '+']\n",
      "['N', '##u', '##MA', '+', '/', 'des', '##min', '+']\n",
      "['N', '##u', '##MA', '+', '/', 'Oct', '##4', '+']\n",
      "['N', '##uma', '+', '/', 'Oct', '##4', '+']\n",
      "['Col', '##lage', '##nas', '##e']\n",
      "['h', '##b', '##F', '##G', '##F']\n",
      "['h', '##b', '##F', '##G', '##F', '.']\n",
      "['Di', '##sp', '##ase']\n",
      "['Try', '##ps', '##in', '/', 'E', '##D', '##TA']\n",
      "['My', '##f', '##5', '/', 'Pa', '##x', '##7']\n",
      "['B', '##rd', '##U']\n",
      "['Des', '##min']\n",
      "['FK', '##50', '##6']\n",
      "['D', '##N', '##ase', '##I']\n",
      "['D', '##N', '##ase', '##I', '-', 'h', '##yper', '##sen', '##sitive']\n",
      "['T', '##AL', '##1', '/', 'SC', '##L']\n",
      "['D', '##N', '##ase', '##I', '-', 'dig', '##ested']\n",
      "['T', '##AL', '##1']\n",
      "['acute']\n",
      "['SC', '##L']\n",
      "['stem']\n",
      "['R', '##N', '##ase', '##A']\n",
      "['protein', '##ase', '##K']\n",
      "['D', '##N', '##ase', '##I', '.']\n",
      "['T', '##4']\n",
      "['polymer', '##ase']\n",
      "['St', '##il']\n",
      "['D', '##N', '##ase', '##I', '-', 'treated']\n",
      "['p', '##or', '##ph', '##ob', '##ili', '##no', '##gen']\n",
      "['H', '##mbs']\n",
      "['D', '##N', '##ase', '##I', '-', 'dig', '##est', '##ion']\n",
      "['ST', '##IL']\n",
      "['B', '##g', '##l', '##II']\n",
      "['ha', '##ema', '##top', '##oi', '##etic']\n",
      "['end', '##oth', '##eli', '##al', '-', 'ha', '##ema', '##top', '##oi', '##etic']\n",
      "['er', '##yt', '##hr', '##oid']\n",
      "['+', '19']\n",
      "['+', '50']\n",
      "['mouse']\n",
      "['Act', '##iv', '##in']\n",
      "['F', '##G', '##F', '##2']\n",
      "['pro', '##te', '##ase']\n",
      "['protein', '##ase']\n",
      "['V', '##ent']\n",
      "['β', '-', 'act', '##in']\n",
      "['MA', '##P']\n",
      "['MA', '##P', '##K', ')']\n",
      "['try', '##ps', '##in', '-']\n",
      "['AK', '##T']\n",
      "['cave', '##olin', '##1']\n",
      "['ER', '##K', '##1']\n",
      "['G', '##S', '##15']\n",
      "['AB', '##P', '-', '280']\n",
      "['B', '##2']\n",
      "['Ka', '##ryo', '##pher', '##in']\n",
      "['B', '##i', '##P']\n",
      "['In', '##hibit', '##or']\n",
      "['C', '##as', '##pas', '##e']\n",
      "['O', '##X', '##A', '##1', '##H', '##s']\n",
      "['protein']\n",
      "['p', '##hos', '##pha', '##tase', '##s']\n",
      "['fi', '##bro', '##nect', '##in']\n",
      "['ST', '##AT', '##3']\n",
      "['F', '##G', '##F']\n",
      "['P', '##I', '##3']\n",
      "['Sr', '##c']\n",
      "['MA', '##P', '##K']\n",
      "['G', '##S', '##K', '##3']\n",
      "['p', '##38']\n",
      "['cell']\n",
      "['ne', '##uro', '##tens', '##in']\n",
      "['end', '##oth', '##elin']\n",
      "['th', '##rom', '##bin']\n",
      "['g', '##lial']\n",
      "['S', '##mad', '##2', '/', '3']\n",
      "['p', '##hos', '##ph', '##o', '-', 'G', '##S', '##K', '##3']\n",
      "['Con', '##nex', '##in']\n",
      "['E', '-', 'C', '##ad']\n",
      "['G', '##D', '##NF', '##R', '##α']\n",
      "['H', '##SP', '##70']\n",
      "['t', '##yr', '##os', '##ine']\n",
      "['GT', '##P', '##ases']\n",
      "['AI', '##M', '-', '1']\n",
      "['Nr', '##BM', '##X', '##P', '##51', '##8', '##13', '##ME', '##K', '##2', '##P', '##36', '##50', '##6', '##C', '##a', '##M']\n",
      "['Ki', '##nas', '##e']\n",
      "['alpha', '/', 'SA', '##P', '##K', '##2', '##a', '##Q', '##16', '##53', '##9', '##C', '##ase', '##in']\n",
      "['CP', '##17', '##6', '##12', '##C', '##d', '##k', '##5', '##Q', '##00', '##53', '##5', '##P', '##K', '##C']\n",
      "['beta', '##P', '##0', '##5', '##7', '##7', '##1', '##C', '##d', '##k', '##7', '##P', '##50', '##6', '##13', '##P', '##K', '##C']\n",
      "['delta', '##Q', '##0', '##5', '##65', '##5', '##DA', '##P']\n",
      "['Ki', '##nas', '##e', '##P', '##53', '##35', '##5', '##PP', '##2', '##A']\n",
      "['beta', '##P', '##18', '##26', '##6', '##R', '##b', '##P', '##13', '##40', '##5', '##I']\n",
      "['ka', '##ppa']\n",
      "['beta', '##O', '##14', '##9', '##20', '##S', '##tat', '##1', '##A', '##46', '##15', '##9', '##J', '##A', '##K', '##1', '##P', '##23', '##45', '##8', '##S', '##tat', '##3', '##P', '##5', '##26', '##31', '##J', '##N', '##K', '##1', '##P', '##45', '##9', '##8', '##3', '##V', '##H', '##RP', '##51', '##45', '##2', '##ME', '##K', '##1', '##Q', '##0', '##27', '##50']\n",
      "['I', '##K', '##K', '##gam', '##ma']\n",
      "['E', '-', 'CA', '##D']\n",
      "['H', '##sp', '##70']\n",
      "['C', '##t', '##B', '##P', '##1']\n",
      "['C', '##t', '##B', '##P', '##2']\n",
      "['G', '##S', '-', '28']\n",
      "['HD', '##J', '-', '2']\n",
      "['H', '##sp', '##40']\n",
      "['heat']\n",
      "['L', '-', 'Cal', '##des', '##mon']\n",
      "['Ra', '##ba', '##pt', '##in']\n",
      "['p', '##hos', '##ph', '##ory', '##lated', '-', 'p', '##13', '##0']\n",
      "['C', '##as']\n",
      "['C', '##rk', '-', 'associated']\n",
      "['Ra', '##s', '-', 'GA', '##P']\n",
      "['p', '##hos', '##ph', '##ory', '##lated']\n",
      "['p', '##21', '##ras']\n",
      "['S', '##h', '##c', '##C']\n",
      "['h', '##ES', '##Cs']\n",
      "['Ra', '##s', '-', 'GA', '##P', '.']\n",
      "['p', '##hos', '##ph', '##o', '-', 'p', '##13', '##0']\n",
      "['T', '##NI', '##K']\n",
      "['p', '##13', '##0']\n",
      "['T', '##ra', '##f', '##2']\n",
      "['T', '##NI', '##K', ')']\n",
      "['F', '-', 'act', '##in']\n",
      "['p', '##13', '##0', '##C', '##as']\n",
      "['T', '##G', '##F', '##β']\n",
      "['G', '##S', '##K', '##3', '##β', '/', 'W', '##nt', '/', 'β', '-', 'cat', '##eni', '##n']\n",
      "['J', '##ak', '/', 'St', '##at']\n",
      "['MA', '##P', '##K', '/', 'ER', '##K']\n",
      "['Gap']\n",
      "['W', '##nt']\n",
      "['St', '##at', '##1']\n",
      "['SM', '##AD', '##s']\n",
      "['G', '##S', '##K', '##3', '##β']\n",
      "['β', '-', 'cat', '##eni', '##n']\n",
      "['β', '##S', '##tat', '##1', '+', '+']\n",
      "['Jun', '+', '+', '+', 'S', '##mad', '##4', '/', 'D', '##PC', '##4', '+', '+', '+', 'End', '##og', '##lin', '+', '-', 'W', '##nt', '##C', '##t', '##B', '##P', '##2', '+', '+']\n",
      "['Cat', '##alytic']\n",
      "['D', '##3', '/', 'CC', '##ND', '##3', '+', '+']\n",
      "['beta', '+', '+']\n",
      "['Jun', '+', '+', '+', 'Case', '##in']\n",
      "['Re', '##ceptor', '/', 'PA', '##R', '##1', '/', 'F', '##2', '##R', '+', '+', '+', 'SH', '##PS', '-', '1', '/', 'PT', '##P', '##NS', '##1', '+', '+']\n",
      "['Jun', '+', '+', '+', 'B', '##c', '##l', '-', 'x', '/', 'BC', '##L', '##2', '##L', '##1', '+', '+']\n",
      "['Brady', '##kini', '##n']\n",
      "['Re', '##ceptor']\n",
      "['3', '.', '4', '.', '24', '.', '16', '/', 'NL', '##N', '+', '+']\n",
      "['C', '+', '+', '-', 'P', '##KA']\n",
      "['R', '##I']\n",
      "['alpha', '+', '+', '-', 'C', '-', 'Ra', '##f', '/', 'RAF', '##1', '+', '+']\n",
      "['i', '##ota', '+', '+']\n",
      "['beta', '/', 'PR', '##K', '##C', '##B', '##1', '+', '+']\n",
      "['alpha', '/', 'A', '##kt', '+', '+', '+', 'G', '##S', '##K', '-', '3']\n",
      "['A', '##cid']\n",
      "['Jun', '+', '+', '+', 'RAF', '##T', '##1', '/', 'F', '##RA', '##P', '+', '+']\n",
      "['p', '##17', '##0', '/', 'P', '##I', '##K', '##3', '##C', '##2', '##A', '+', '+', 'PT', '##P', '##1', '##B', '/', 'PT', '##P', '##N', '##1', '+', '+', '+', 'Do', '##k', '##1', '/', 'p', '##6', '##2', '##do', '##k', '+', '+', '+', 'P', '##I', '##3', '-', 'Ki', '##nas', '##e']\n",
      "['p', '##11', '##0']\n",
      "['alpha', '+', '+', '-', 'Yes']\n",
      "['alpha', '/', 'SA', '##P', '##K', '##2', '##a', '+', '+', '-', 'G', '##3', '##B', '##P', '+', '+']\n",
      "['In', '##hibit', '##or', '##2', '/', 'PP', '##P', '##1', '##R', '##2', '+', '+']\n",
      "['e', '##ps', '##ilon', '/', 'Y', '##W', '##HA', '##E', '+', '+']\n",
      "['I', '##G', '##F']\n",
      "['ER', '##BB', '##2']\n",
      "['GP', '##CR']\n",
      "['G', '##D', '##NF']\n",
      "['Z', '##O', '##1']\n",
      "['o', '##cc', '##lu', '##din']\n",
      "['E', '##G', '##F', ')', '-', 'receptor']\n",
      "['E', '##G', '##F', '-', 'family']\n",
      "['Here', '##gu', '##lin']\n",
      "['ER', '##BB', '##3']\n",
      "['ER', '##BB']\n",
      "['ER', '##BB', '##2', '/', '3']\n",
      "['a', '##cc', '##uta', '##se']\n",
      "['O', '##cc', '##lu', '##din']\n",
      "['I', '##G', '##F', '##1', '##R']\n",
      "['tight']\n",
      "['transfer', '##rin']\n",
      "['her', '##gu', '##lin', '##1', '##β']\n",
      "['act', '##iv', '##in', '##A']\n",
      "['L', '##R', '##3', '-', 'I', '##G', '##F', '##1']\n",
      "['P', '##19', '##0']\n",
      "['Ada', '##pt', '##in']\n",
      "['ST', '##AT', '-', '3']\n",
      "['PT', '##P', '##1', '##D']\n",
      "['Me', '##k', '-', '2']\n",
      "['RA', '##C', '##K', '-', '1']\n",
      "['G', '##RB', '-', '2']\n",
      "['Rap', '##2']\n",
      "['Expo', '##rt', '##in', '-', '1', '/', 'CR', '##M', '##1']\n",
      "['MC', '##M']\n",
      "['N', '##uc', '##leo', '##por', '##in']\n",
      "['α', '-', 'tub', '##ulin']\n",
      "['Act', '##in']\n",
      "['K', '##NP', '-', '1', '/', 'H', '##ES', '##1']\n",
      "['N', '##TF', '##2']\n",
      "['p', '##19', '##0']\n",
      "['Hip', '##1', '##R']\n",
      "['Transport', '##in']\n",
      "['Cal', '##ret', '##ic', '##ulin']\n",
      "['A', '##rp', '##3']\n",
      "['e', '##IF', '-', '6']\n",
      "['horse']\n",
      "['per', '##ox', '##idas', '##e']\n",
      "['Ra', '##ba', '##pt', '##in', '-', '5']\n",
      "['p', '##hos', '##ph', '##o', '-', 'Ra', '##s', '-', 'GA', '##P']\n",
      "['S', '##h', '##c', '-', 'C']\n",
      "['e', '##pid', '##er', '##mal']\n",
      "['E', '##G', '##F', ')']\n",
      "['basic']\n",
      "['le', '##uke', '##mia']\n",
      "['L', '##IF']\n",
      "['E', '##G', '##F']\n",
      "['L', '##IF', '.']\n",
      "['try', '##ps', '##in', '-', 'E', '##D', '##TA', '.']\n",
      "['3', '##C', '##B', '##2']\n",
      "['bra', '##chy', '##ury']\n",
      "['fox', '##a', '##2']\n",
      "['V', '##iment', '##in']\n",
      "['Not', '##ch', '##1']\n",
      "['neural']\n",
      "['β', '-', 'tub', '##ulin']\n",
      "['medium', '-', 'size']\n",
      "['N', '##F', '-', 'M', ')']\n",
      "['micro', '##tub', '##ule', '-', 'associated']\n",
      "['MA', '##P', '-', '2']\n",
      "['G', '##FA', '##P']\n",
      "['my', '##elin']\n",
      "['MB', '##P']\n",
      "['g', '##lut', '##ami', '##c']\n",
      "['GA', '##D']\n",
      "['T', '##H', ')']\n",
      "['nest', '##in', '+']\n",
      "['Tu', '##J', '##1']\n",
      "['gal', '##act', '##oc', '##ere', '##bro', '##cid', '##e']\n",
      "['G', '##C']\n",
      "['G', '##FA', '##P', '+']\n",
      "['nest', '##in', '.', '10', '.', '137', '##1', '/', 'journal']\n",
      "['NC', '##AM']\n",
      "['MB', '##P', ')']\n",
      "['T', '##H', '.']\n",
      "['G', '##C', '-', 'expressing']\n",
      "['Tu', '##J', '##1', '+']\n",
      "['h', '##N', '##uc']\n",
      "['G', '##lu', '##T', '##1']\n",
      "['double', '##cor', '##tin']\n",
      "['C', '##NP', '##ase', '-', 'expressing']\n",
      "['g', '##lut', '##ama', '##te']\n",
      "['Ki', '##6', '##7', '+']\n",
      "['h', '##N', '##uc', '+']\n",
      "['h', '##N', '##uc', '+', '(']\n",
      "['C', '##NP', '##ase']\n",
      "['GA', '##D', '##65', '/', '67']\n",
      "['Sox', '##1']\n",
      "['he', '##par', '##in']\n",
      "['insulin']\n",
      "['try', '##ps', '##in']\n",
      "['try', '##ps', '##in', '-', 'E', '##D', '##TA']\n",
      "['Anti', '-', 'human']\n",
      "['Anti', '-', 'Tu', '##J', '##1']\n",
      "['anti', '-', 'GA', '##D', '##65', '/', '67']\n",
      "['Anti', '-', 'g', '##lial']\n",
      "['Anti', '-', 'gal', '##act', '##oc', '##ere', '##bro', '##cid', '##e']\n",
      "['Anti', '-', 'C', '##NP', '##ase']\n",
      "['Anti', '-', 'G', '##lu', '##cos', '##e']\n",
      "['Transport', '##er']\n",
      "['G', '##lut', '-', '1']\n",
      "['Anti', '-', 'N', '##est', '##in']\n",
      "['Anti', '-', 'v', '##iment', '##in']\n",
      "['Anti', '-', '3', '##C', '##B', '##2']\n",
      "['Anti', '-', 'double', '##cor', '##tin']\n",
      "['Anti', '-', 'Ki', '##6', '##7']\n",
      "['R', '##N', '##ase']\n",
      "['Trans', '##cript', '##ase']\n",
      "['MA', '##P', '##2']\n",
      "['N', '-', 'CA', '##M']\n",
      "['N', '##est', '##in']\n",
      "['N', '##F', '-', 'M']\n",
      "['Not', '##ch', '-', '1']\n",
      "['F', '##OX', '##a', '##2']\n",
      "['H', '##NF', '##3', '##B', ')']\n",
      "['B', '##rac', '##hy', '##ury']\n",
      "[100]\n",
      "[28996]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_ids(train_df[train_df['labels']=='B']['words'].unique()))\n",
    "print(tokenizer.convert_ids_to_tokens(tokenizer.convert_tokens_to_ids(train_df[train_df['labels']=='B']['words'].unique())))\n",
    "for sent in train_df[train_df['labels']=='B']['words'].unique():\n",
    "    print(tokenizer.tokenize(sent))\n",
    "train_df[train_df['labels']=='B']['words'].unique()\n",
    "print(tokenizer.convert_tokens_to_ids([\"cellsf\"]))\n",
    "tokenizer.add_tokens(\"cellsf\")\n",
    "\n",
    "print(tokenizer.convert_tokens_to_ids([\"cellsf\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_dir': 'sbksvol/nikhil/test/model_output_test/', 'num_train_epochs': 100, 'train_batch_size': 64, 'save_strategy': 'epoch', 'evaluation_strategy': 'steps', 'eval_steps': 22, 'logging_steps': 22, 'do_train': True, 'load_best_model_at_end': True, 'learning_rate': 5e-05, 'save_total_limit': 2}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer, EarlyStoppingCallback\n",
    "from utils import LogCallback, plot_loss_log\n",
    "import json\n",
    "from transformers.hf_argparser import HfArgumentParser\n",
    "training_args_dict = {\n",
    "        'output_dir': params[\"OUTPUT_DIR\"],\n",
    "        'num_train_epochs': params[\"EPOCH_TOP\"],\n",
    "        'train_batch_size': params[\"BATCH_SIZE\"],\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"eval_steps\": max(10,train_dataset.__len__()//params[\"BATCH_SIZE\"]),\n",
    "        \"logging_steps\":max(10,train_dataset.__len__()//params[\"BATCH_SIZE\"]),\n",
    "        \"do_train\": True,\n",
    "        \"load_best_model_at_end\": params[\"LOAD_BEST_MODEL\"],\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"save_total_limit\": 2,\n",
    "#         \"resume_from_checkpoint\"\n",
    "    }\n",
    "print(training_args_dict)\n",
    "with open(params[\"TRAIN_ARGS_FILE\"], 'w') as fp:\n",
    "    json.dump(training_args_dict, fp)\n",
    "parser = HfArgumentParser(TrainingArguments)\n",
    "training_args = parser.parse_json_file(\n",
    "    json_file=params[\"TRAIN_ARGS_FILE\"])[0]\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset\n",
    "    )\n",
    "print(trainer.optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, -100, 2, 0, -100]\n",
      "['H', '##1', '(', 'WA', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['WA', '-', '01', ',', 'X']\n",
      "\n",
      "\n",
      "[0, -100, 2, 0, -100]\n",
      "['H', '##9', '(', 'WA', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['WA', '-', '09', ',', 'X']\n",
      "\n",
      "\n",
      "[0, -100, -100, 0, 2]\n",
      "['O', '##P', '##9', 'cells', 'were']\n",
      "\n",
      "\n",
      "[0, 2, 2, 2, 2]\n",
      "['cells', 'were', 'maintained', 'in', 'alpha']\n",
      "\n",
      "\n",
      "[0, -100, -100, 1, 2]\n",
      "['O', '##P', '##9', 'cells', 'in']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['C', '##2', '##C', '##12', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['C', '##2', '##C', '##12', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, 2, 2, 2]\n",
      "['H', '##1', ';', 'passages', '42']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, 2, -100, -100]\n",
      "['H', '##1', '[', 'WA', '-']\n",
      "\n",
      "\n",
      "[0, -100, 2, -100, -100]\n",
      "['H', '##9', '[', 'WA', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['O', '##P', '##9', 's', '##trom']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['O', '##P', '##9', 'cells', 'have']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['O', '##P', '##9', 'and', 'differentiated']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 0]\n",
      "['H', '##1', '-', 'and', 'H']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['H', '##9', '-', 'derived', 'CD']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, 0, 2]\n",
      "['O', '##P', '##9', 'cells', '(']\n",
      "\n",
      "\n",
      "[0, 2, 2, 2, -100]\n",
      "['cells', '(', 'Figure', 'S', '##1']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['C', '##2', '##C', '##12', 'previously']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 1]\n",
      "['C', '##2', '##C', '##12', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 1]\n",
      "['C', '##2', '##C', '##12', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '##9']\n",
      "\n",
      "\n",
      "[0, -100, 2, 2, 2]\n",
      "['S', '##17', 'mouse', 'bone', 'ma']\n",
      "\n",
      "\n",
      "[0, -100, 2, 2, 2]\n",
      "['S', '##17', 'mouse', 'bone', 'ma']\n",
      "\n",
      "\n",
      "[0, -100, 2, -100, 2]\n",
      "['H', '##1', 'h', '##ES', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, 2, 2, -100]\n",
      "['S', '##17', 'bone', 'ma', '##rrow']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Ma', '##gi', '-', 'C', '##X']\n",
      "\n",
      "\n",
      "[0, -100, 2, 2, 2]\n",
      "['H', '##1', 'cell', 'line', 'was']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['29', '##3', '##T', 'cells', 'and']\n",
      "\n",
      "\n",
      "[0, -100, 2, 2, 2]\n",
      "['S', '##17', 'mouse', 'bone', 'ma']\n",
      "\n",
      "\n",
      "[0, -100, 0, 2, 2]\n",
      "['S', '##17', 'cell', 'layers', 'and']\n",
      "\n",
      "\n",
      "[0, 2, 2, 2, -100]\n",
      "['cell', 'layers', 'and', 'culture', '##d']\n",
      "\n",
      "\n",
      "[0, -100, 0, 2, -100]\n",
      "['S', '##17', 'cells', '[', '20']\n",
      "\n",
      "\n",
      "[0, 2, -100, -100, -100]\n",
      "['cells', '[', '20', ']', '.']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Ma', '##gi', '-', 'C', '##X']\n",
      "\n",
      "\n",
      "[0, -100, -100, 0, 2]\n",
      "['He', '##L', '##a', 'cell', 'derivative']\n",
      "\n",
      "\n",
      "[0, 2, 2, 2, 2]\n",
      "['cell', 'derivative', 'with', 'no', 'p']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', '.']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['B', '##W', '##51', '##47', '.']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['B', '##W', '##51', '##47', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'material']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'biological']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 0]\n",
      "['Cy', '##ther', '##a', '(', 'Cy']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['H', '##UE', '##S', '##6', '(']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##SC']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##SC']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##6', '-', 'E']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'h', '##ES']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['Cy', '##20', '##3', '(', 'Cy']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['H', '##UE', '##S', '##6', 'was']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['F', '##BR', '##16', '##64', '(']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['B', '##G', '##01', 'cells', 'for']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['B', '##G', '##01', 'h', '##ES']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['B', '##G', '##01', 'h', '##ES']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['B', '##G', '##01', 'h', '##ES']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['B', '##G', '##0', '##3', 'h']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['B', '##G', '##0', '##3', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['B', '##G', '##0', '##3', 'l']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['B', '##G', '##01', 'cultures', '(']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['B', '##G', '##01', 'cells', 'grown']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['B', '##G', '##01', 'cultures', 'were']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['B', '##G', '##01', 'cultures', 'were']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['B', '##G', '##01', 'h', '##ES']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['B', '##G', '##01', 'cells', ',']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['B', '##G', '##01', 'h', '##ES']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['B', '##G', '##0', '##3', 'h']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['B', '##G', '##01', 'cells', 'were']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['B', '##G', '##0', '##3', 'cell']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 0]\n",
      "['B', '##G', '##01', ',', 'B']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['B', '##G', '##0', '##2', 'and']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['B', '##G', '##0', '##3', 'cell']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['SD', '##5', '##6', ')', 'from']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['SD', '##5', '##6', 'cell', 'line']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['SD', '##5', '##6', 'human', 'neural']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['SD', '##5', '##6', '(', 'intermittent']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['SD', '##5', '##6', ',', 'was']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['SD', '##5', '##6', 'cells', 'did']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, 2, 2, -100]\n",
      "['H', '##9', '(', 'W', '##i']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "['B', 'I', 'O']\n"
     ]
    }
   ],
   "source": [
    "for x in train_dataset:\n",
    "    if 0 in x.label_ids:\n",
    "        for i in range(len(x.label_ids)):\n",
    "            if x.label_ids[i] == 0:# and '[UNK]' == tokenizer.convert_ids_to_tokens(x.input_ids)[i]:\n",
    "                print(x.label_ids[i:i+5])\n",
    "                print(tokenizer.convert_ids_to_tokens(x.input_ids)[i:i+5])\n",
    "                print(\"\\n\")\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "files = [\"train\", \"dev\", \"test\"]\n",
    "\n",
    "\n",
    "def convert(lines, f):\n",
    "    tokens_ = []\n",
    "    tags_ = []\n",
    "\n",
    "    data = {\"words\": [], \"ner\": []}\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            data[\"words\"].append(tokens_)\n",
    "            data[\"ner\"].append(tags_)\n",
    "            tokens_ = []\n",
    "            tags_ = []\n",
    "        else:\n",
    "            token, tag = line.split(\"\\t\")\n",
    "            if len(tag) > 1:\n",
    "                tag = tag.split(\"-\")[0]\n",
    "            tokens_.append(token.strip())\n",
    "            tags_.append(tag.strip())\n",
    "            \n",
    "    if len(tokens_) > 0:\n",
    "        data[\"words\"].append(tokens_)\n",
    "        data[\"ner\"].append(tags_)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writer(data, fp, add_str=\"\"):\n",
    "\n",
    "    for (tokens, tags) in zip(data[\"words\"], data[\"ner\"]):\n",
    "        for (token, tag) in zip(tokens, tags):\n",
    "            if tag == \"B\" or tag == \"I\":\n",
    "                tag += add_str\n",
    "            fp.write(\"{}\\t{}\\n\".format(token, tag))\n",
    "        fp.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert all tsv files to txt\n",
    "\n",
    "all_data = {}\n",
    "for f in files:\n",
    "    with open(os.path.join(data_dir, f + \".tsv\"), \"r\") as fp:\n",
    "        lines = fp.readlines()\n",
    "        all_data[f] = convert(lines, fp)\n",
    "    fp = open(os.path.join(data_dir, f + \".txt\"), \"w\")\n",
    "    writer(all_data[f], fp)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121 22 61\n"
     ]
    }
   ],
   "source": [
    "num_train_sents = len(all_data[\"train\"][\"words\"])\n",
    "num_dev_sents = len(all_data[\"dev\"][\"words\"])\n",
    "num_test_sents = len(all_data[\"test\"][\"words\"])\n",
    "print(num_train_sents, num_dev_sents, num_test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# add the index to keep track of sentences\n",
    "train_tuples = []\n",
    "for i,(tokens,tags) in enumerate(zip(all_data[\"train\"][\"words\"],all_data[\"train\"][\"ner\"])):\n",
    "    for token,tag in zip(tokens,tags):\n",
    "        train_tuples.append([i,token,tag])\n",
    "\n",
    "test_tuples = []\n",
    "for i,(tokens,tags) in enumerate(zip(all_data[\"test\"][\"words\"],all_data[\"test\"][\"ner\"])):\n",
    "    for token,tag in zip(tokens,tags):\n",
    "        test_tuples.append([i,token,tag])\n",
    "    \n",
    "train_df = pd.DataFrame(train_tuples, columns=['sentence_id', 'words', 'labels'])\n",
    "test_df = pd.DataFrame(test_tuples, columns=['sentence_id', 'words', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentence_id       words labels\n",
      "0            0          By      O\n",
      "1            0    Northern      O\n",
      "2            0        blot      O\n",
      "3            0    analysis      O\n",
      "4            0           ,      O\n",
      "5            0         the      O\n",
      "6            0  expression      O\n",
      "7            0          of      O\n",
      "8            0          IL      O\n",
      "9            0           -      O\n"
     ]
    }
   ],
   "source": [
    "print(test_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B', 'I', 'O']\n"
     ]
    }
   ],
   "source": [
    "# a list that has all possible labels \n",
    "labels = np.sort(train_df['labels'].unique()).tolist()\n",
    "label_map =  {i: label for i, label in enumerate(labels)}\n",
    "num_labels = len(labels)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = dict()\n",
    "\n",
    "# Path to pretrained model or model identifier from huggingface.co/models\n",
    "model_args['model_name_or_path'] = 'dmis-lab/biobert-base-cased-v1.1'\n",
    "# saved_model_path\n",
    "# saved_model_path\n",
    "# pytorch_dump_path\n",
    "# 'dmis-lab/biobert-base-cased-v1.1'\n",
    "\n",
    "# Where do you want to store the pretrained models downloaded from s3\n",
    "model_args['cache_dir'] = \"/sbksvol/gaurav/NER_out/\"\n",
    "\n",
    "# we skip basic white-space tokenization by passing do_basic_tokenize = False to the tokenizer\n",
    "model_args['do_basic_tokenize'] = False\n",
    "\n",
    "\n",
    "data_args = dict()\n",
    "\n",
    "data_args['data_dir'] = data_dir\n",
    "\n",
    "# \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "# \"than this will be truncated, sequences shorter will be padded.\"\n",
    "data_args['max_seq_length'] = 256\n",
    "\n",
    "# Overwrite the cached training and evaluation sets\n",
    "# this means the model does not have to tokenize/preprocess and cache the data each time it's called\n",
    "# this can be made different for each NerDataset (training NerDataset, testing NerDataset)\n",
    "data_args['overwrite_cache'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.4.2'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizer\n",
    ")\n",
    "\n",
    "config = BertConfig.from_pretrained(\n",
    "    model_args['model_name_or_path'],\n",
    "    num_labels=num_labels,\n",
    "    id2label=label_map,\n",
    "    label2id={label: i for i, label in enumerate(labels)},\n",
    "    cache_dir=model_args['cache_dir']\n",
    ")\n",
    "\n",
    "# we skip basic white-space tokenization by passing do_basic_tokenize = False to the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    model_args['model_name_or_path'],\n",
    "    cache_dir=model_args['cache_dir']\n",
    "#     ,do_basic_tokenize = model_args['do_basic_tokenize']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_utils_path = \"/sbksvol/gaurav/transformers/examples/token-classification/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if data_utils_path not in sys.path:\n",
    "    sys.path.append(data_utils_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_ner import NerDataset, Split\n",
    "# %reset_selective -f \"utils_ner\"\n",
    "# NerDataset.__init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NerDataset(\n",
    "  data_dir=data_args['data_dir'],\n",
    "  tokenizer=tokenizer,\n",
    "  labels=labels,\n",
    "  model_type=config.model_type,\n",
    "  max_seq_length=data_args['max_seq_length'],\n",
    "  overwrite_cache=data_args['overwrite_cache'], # True\n",
    "  mode=Split.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = NerDataset(\n",
    "  data_dir=data_args['data_dir'],\n",
    "  tokenizer=tokenizer,\n",
    "  labels=labels,\n",
    "  model_type=config.model_type,\n",
    "  max_seq_length=data_args['max_seq_length'],\n",
    "  overwrite_cache=data_args['overwrite_cache'],\n",
    "  mode=Split.dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 21\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.__len__(), eval_dataset.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train top-model using the Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models import FullyConnectedLayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel, BertModel\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertNERTopModel(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "#         self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        hidden_units_list=[500, 250, 125]\n",
    "#         activations_list = [\"none\", \"none\", \"none\", \"none\"]\n",
    "\n",
    "        hid1, hid2, hid3 = hidden_units_list\n",
    "        self.fc1 = nn.Linear(config.hidden_size, hid1)\n",
    "        self.fc2 = nn.Linear(hid1, hid2)\n",
    "        self.fc3 = nn.Linear(hid2, hid3)\n",
    "        self.fc4 = nn.Linear(hid3, config.num_labels)\n",
    "        \n",
    "        self.crf = CRF(config.num_labels, batch_first=True)\n",
    "\n",
    "#         self.classifier = FullyConnectedLayers(hidden_units_list, activations_list,\n",
    "#                                                config.hidden_size, config.num_labels)\n",
    "\n",
    "\n",
    "        ## 0-hidden layers ##\n",
    "#         self.fc1 = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "\n",
    "        ## 1-hidden layer ##\n",
    "#         self.fc1 = nn.Linear(config.hidden_size, 250)\n",
    "#         self.fc2 = nn.Linear(250, config.num_labels)\n",
    "\n",
    "        print(\"Initializing weights\")\n",
    "        self.init_weights()\n",
    "        \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
    "            1]``.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        \n",
    "#         logits = self.classifier(sequence_output)\n",
    "\n",
    "        x = sequence_output\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        logits = x\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            ## the tokens whose labels == -100 denote padding tokens\n",
    "            ## since they are ignored for loss calculation and because crf cannot accept label values other \n",
    "            ## than 0, 1, ... num_tags-1, we just set all the pad token indices to 2 instead of -100\n",
    "            labels_copy = labels.detach().clone()\n",
    "            labels_copy[labels_copy == -100] = 2\n",
    "            loss = -self.crf.forward(logits, labels_copy, attention_mask.type(torch.uint8), reduction=\"mean\")\n",
    "#             loss_fct = nn.CrossEntropyLoss()\n",
    "#             # Only keep active parts of the loss\n",
    "#             if attention_mask is not None:\n",
    "#                 active_loss = attention_mask.view(-1) == 1\n",
    "#                 active_logits = logits.view(-1, self.num_labels)\n",
    "#                 active_labels = torch.where(\n",
    "#                     active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "#                 )\n",
    "                \n",
    "#                 loss = loss_fct(active_logits, active_labels)\n",
    "#             else:\n",
    "#                 print(\"Labels None\")\n",
    "#                 loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.top_model_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First freeze bert weights and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertNERTopModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertNERTopModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertNERTopModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertNERTopModel were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertNERTopModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc1): Linear(in_features=768, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=250, bias=True)\n",
       "  (fc3): Linear(in_features=250, out_features=125, bias=True)\n",
       "  (fc4): Linear(in_features=125, out_features=3, bias=True)\n",
       "  (crf): CRF(num_tags=3)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = BertNERTopModel.from_pretrained(\n",
    "    model_args['model_name_or_path'],\n",
    "    config=config\n",
    "    ,cache_dir=model_args['cache_dir']\n",
    ")\n",
    "\n",
    "## base_model -> bert (excluding the classification layer)\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.hf_argparser import HfArgumentParser\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "training_args_dict = {\n",
    "    'output_dir' : \"model_output/\",\n",
    "    'num_train_epochs' : 20,\n",
    "    'train_batch_size': 32,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"evaluation_strategy\": \"epoch\"\n",
    "#     ,\n",
    "#     \"load_best_model_at_end\": True\n",
    "}\n",
    "\n",
    "with open('training_args.json', 'w') as fp:\n",
    "    json.dump(training_args_dict, fp)\n",
    "    \n",
    "parser = HfArgumentParser(TrainingArguments)\n",
    "# this function returns a tuple so we get the first item in the tuple since we only passed one arguement type \"TrainingArguments\"\n",
    "training_args = parser.parse_json_file(json_file=\"training_args.json\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 02:06, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>59.282146</td>\n",
       "      <td>0.465500</td>\n",
       "      <td>45.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>48.672626</td>\n",
       "      <td>0.459300</td>\n",
       "      <td>45.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>33.185772</td>\n",
       "      <td>0.460400</td>\n",
       "      <td>45.613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>17.980986</td>\n",
       "      <td>0.457000</td>\n",
       "      <td>45.954000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>9.539531</td>\n",
       "      <td>0.460800</td>\n",
       "      <td>45.572000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.969814</td>\n",
       "      <td>0.458500</td>\n",
       "      <td>45.804000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.941916</td>\n",
       "      <td>0.460600</td>\n",
       "      <td>45.594000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.141191</td>\n",
       "      <td>0.461300</td>\n",
       "      <td>45.522000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.474339</td>\n",
       "      <td>0.462700</td>\n",
       "      <td>45.381000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.931677</td>\n",
       "      <td>0.471900</td>\n",
       "      <td>44.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.518576</td>\n",
       "      <td>0.457200</td>\n",
       "      <td>45.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.200491</td>\n",
       "      <td>0.464300</td>\n",
       "      <td>45.227000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.962125</td>\n",
       "      <td>0.461800</td>\n",
       "      <td>45.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.791451</td>\n",
       "      <td>0.490200</td>\n",
       "      <td>42.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.673448</td>\n",
       "      <td>0.460900</td>\n",
       "      <td>45.563000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.584123</td>\n",
       "      <td>0.459700</td>\n",
       "      <td>45.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.522175</td>\n",
       "      <td>0.458700</td>\n",
       "      <td>45.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.483365</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>45.405000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.459371</td>\n",
       "      <td>0.463000</td>\n",
       "      <td>45.361000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.450893</td>\n",
       "      <td>0.464000</td>\n",
       "      <td>45.261000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=train_dataset,\n",
    "  eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "trainOutput = trainer.train()\n",
    "# trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0533, 0.0881, 0.0246],\n",
       "        [0.0301, 0.0743, 0.0117],\n",
       "        [0.0033, 0.0089, 0.0249]], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[-1].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now reload the model from saved checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing weights\n"
     ]
    }
   ],
   "source": [
    "num_steps = trainOutput.global_step # 17880\n",
    "checkpoint = f\"checkpoint-{num_steps}\"\n",
    "top_model_path = f\"{training_args_dict['output_dir']}/{checkpoint}\" \n",
    "\n",
    "# model_output/checkpoint-17880\n",
    "\n",
    "#### Config ####\n",
    "config = BertConfig.from_pretrained(\n",
    "    top_model_path,\n",
    "    num_labels=num_labels,\n",
    "    id2label=label_map,\n",
    "    label2id={label: i for i, label in enumerate(labels)},\n",
    "    cache_dir=model_args['cache_dir']\n",
    ")\n",
    "\n",
    "#### Model ####\n",
    "\n",
    "reloaded_model = BertNERTopModel.from_pretrained(\n",
    "    top_model_path,\n",
    "    config=config,\n",
    "    cache_dir=model_args['cache_dir']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0533, 0.0881, 0.0246],\n",
       "        [0.0301, 0.0743, 0.0117],\n",
       "        [0.0033, 0.0089, 0.0249]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reloaded_model.parameters())[-1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Training args ####\n",
    "training_args_dict = {\n",
    "    'output_dir' : \"model_output\",\n",
    "    'num_train_epochs' : 5,\n",
    "    'train_batch_size': 32,\n",
    "    'seed':seed_value,\n",
    "    \"evaluation_strategy\": \"epoch\"\n",
    "#     ,\"load_best_model_at_end\": True\n",
    "}\n",
    "\n",
    "with open('training_args.json', 'w') as fp:\n",
    "    json.dump(training_args_dict, fp)\n",
    "    \n",
    "parser = HfArgumentParser(TrainingArguments)\n",
    "# this function returns a tuple so we get the first item in the tuple since we only passed one arguement type \"TrainingArguments\"\n",
    "training_args = parser.parse_json_file(json_file=\"training_args.json\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then unfreeze the bert weights and train end-to-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = reloaded_model\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(name, param.shape, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertNERTopModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc1): Linear(in_features=768, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=250, bias=True)\n",
       "  (fc3): Linear(in_features=250, out_features=125, bias=True)\n",
       "  (fc4): Linear(in_features=125, out_features=3, bias=True)\n",
       "  (crf): CRF(num_tags=3)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(trainer.model_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:39, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.891200</td>\n",
       "      <td>0.461100</td>\n",
       "      <td>45.538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.461137</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>45.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.151145</td>\n",
       "      <td>0.470300</td>\n",
       "      <td>44.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.953274</td>\n",
       "      <td>0.462600</td>\n",
       "      <td>45.397000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.862683</td>\n",
       "      <td>0.463200</td>\n",
       "      <td>45.341000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=3.7158329264322916, metrics={'train_runtime': 40.482, 'train_samples_per_second': 1.853, 'total_flos': 99773520076800.0, 'epoch': 5.0, 'init_mem_cpu_alloc_delta': 48563, 'init_mem_gpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 18306, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 208707, 'train_mem_gpu_alloc_delta': 1305872896, 'train_mem_cpu_peaked_delta': 138334, 'train_mem_gpu_peaked_delta': 2282095616})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=train_dataset,\n",
    "  eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "# Begin training from the latest checkpoint\n",
    "trainer.train(checkpoint)\n",
    "# trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0520, 0.0873, 0.0263],\n",
       "        [0.0290, 0.0743, 0.0098],\n",
       "        [0.0050, 0.0071, 0.0243]], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[-1].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Softmax models ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # we can pass overwrite_cache as True since we might like to make new predictions by just changing test.txt \n",
    "# test_dataset = NerDataset(\n",
    "#   data_dir=data_args['data_dir'],\n",
    "#   tokenizer=tokenizer,\n",
    "#   labels=labels,\n",
    "#   model_type=config.model_type,\n",
    "#   max_seq_length=data_args['max_seq_length'],\n",
    "#   overwrite_cache=True,\n",
    "#   mode=Split.test)\n",
    "\n",
    "# # last layer output/activation has the shape of (batch_size, seq_len,num_of_labels)\n",
    "# output, label_ids, metrics = trainer.predict(test_dataset)\n",
    "# preds = np.argmax(output, axis=2)\n",
    "# batch_size, seq_len = preds.shape\n",
    "\n",
    "# # list of token-level predictions shape = (batch_size, seq_len)\n",
    "# preds_list = [[] for _ in range(batch_size)]\n",
    "# for i in range(batch_size):\n",
    "#     for j in range(seq_len):\n",
    "#         # ignore pad_tokens\n",
    "#         if label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n",
    "#             preds_list[i].append(label_map[preds[i][j]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For CRF models ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# we can pass overwrite_cache as True since we might like to make new predictions by just changing test.txt \n",
    "test_dataset = NerDataset(\n",
    "  data_dir=data_args['data_dir'],\n",
    "  tokenizer=tokenizer,\n",
    "  labels=labels,\n",
    "  model_type=config.model_type,\n",
    "  max_seq_length=data_args['max_seq_length'],\n",
    "  overwrite_cache=True,\n",
    "  mode=Split.test)\n",
    "\n",
    "# last layer output/activation has the shape of (batch_size, seq_len, num_labels)\n",
    "output, label_ids, metrics = trainer.predict(test_dataset)\n",
    "batch_size, seq_len, num_labels = output.shape\n",
    "\n",
    "output = torch.tensor(output).to('cuda')\n",
    "\n",
    "all_attention_masks = []\n",
    "for sample in test_dataset:\n",
    "    all_attention_masks.append(sample.attention_mask)\n",
    "    \n",
    "all_attention_masks = torch.tensor(all_attention_masks).to('cuda')\n",
    "\n",
    "# get the best tag sequences using CRF's viterbi decode algo\n",
    "preds = model.crf.decode(output, all_attention_masks.type(torch.uint8))\n",
    "\n",
    "\n",
    "preds_list = [[] for _ in range(batch_size)]\n",
    "for i in range(batch_size):\n",
    "    for j in range(seq_len):\n",
    "        # ignore pad_tokens\n",
    "        if label_ids[i, j] != -100:\n",
    "            preds_list[i].append(label_map[preds[i][j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_combiner(df):\n",
    "    # 'words' and 'labels' are the column names in the CSV file\n",
    "    tupple_function = lambda x: [(w, t) for w, t in zip(x[\"words\"].values.tolist(),\n",
    "                                                      x[\"labels\"].values.tolist())]\n",
    "    grouped = df.groupby(\"sentence_id\").apply(tupple_function)\n",
    "    return [s for s in grouped]\n",
    "\n",
    "testing_sentences = sentences_combiner(test_df)\n",
    "test_labels = [[w[1] for w in s] for s in testing_sentences]\n",
    "test_tokens = [[w[0] for w in s] for s in testing_sentences]\n",
    "\n",
    "# reconstruct full sentences from lists of (token,label) tuples \n",
    "# test_reconstructed = [\" \".join([w[0] for w in s] ) for s in testing_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all test and pred sentences have the same length\n",
    "\n",
    "test_labels_new = []\n",
    "preds_list_new = []\n",
    "\n",
    "for i, x in enumerate(test_labels):\n",
    "    if len(x) == len(preds_list[i]):\n",
    "        test_labels_new.append(x)\n",
    "        preds_list_new.append(preds_list[i])\n",
    "    else:\n",
    "        print(\"ABORT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get entity level scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 95.5%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.95      0.96      0.95        77\n",
      "\n",
      "   micro avg       0.95      0.96      0.95        77\n",
      "   macro avg       0.95      0.96      0.95        77\n",
      "weighted avg       0.95      0.96      0.95        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import f1_score, classification_report\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_labels_new, preds_list_new)))\n",
    "print(classification_report(test_labels_new, preds_list_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gene, bioinfer (no relu)\n",
    "# seed = 42 -> 0.84      0.87      0.85\n",
    "# seed = 0 -> 0.86      0.89      0.88\n",
    "# seed = 13 -> 0.83      0.88      0.85\n",
    "\n",
    "# Gene, bioinfer (all relu)\n",
    "# seed = 42 -> 0.84      0.90      0.87\n",
    "# seed = 0 -> 0.83      0.87      0.85\n",
    "# seed = 13 -> 0.84      0.88      0.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.78      0.66      0.71 -> gellus, 3-layer +softmax with relu after each fc layer \n",
    "# 0.97 0.94 0.95 -> cll, 3-layer +softmax with relu after each fc layer except the 1st, seed=42\n",
    "# 0.96 0.86, 0.90, 0.94, 0.96, 0.95 -> '' seed=0 \n",
    "# 0.97, 0.91, 0.94 -> '' seed=13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following results are for the cll dataset with different # relu layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu after 1st, 2nd and 3rd layer -> 0, 0, 0\n",
    "# relu after 2nd and 3rd layer -> 0.86, 0.96, 0.91\n",
    "# relu only after 3rd layer -> 0.92, 0.99, 0.95\n",
    "# No relu -> 0.97, 0.94, 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following results are for 3 hidden layers without any relu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cll\n",
    "# seed = 42 -> 0.92      0.99      0.95\n",
    "# seed = 0  -> 0.95      0.97      0.96\n",
    "# seed = 13 -> 0.97      0.96      0.97\n",
    "# seed = 20 -> 0.90      0.95      0.92\n",
    "# seed = 50 -> 0.96      0.96      0.96\n",
    "# seed = 75 -> 0.92      0.95      0.94\n",
    "# seed = 100 -> 0.93      0.92      0.93\n",
    "# -----------------------\n",
    "# average f1 -> 95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following results are for 1 hidden layers without any relu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cll\n",
    "# seed = 42 -> Not obtained yet\n",
    "# seed = 0  -> 0.83      0.99      0.90\n",
    "# seed = 13 -> 0.90      0.99      0.94\n",
    "# seed = 20 -> 0.90      0.97      0.94\n",
    "# seed = 50 -> Not obtained yet\n",
    "# seed = 75 -> Not obtained yet\n",
    "# seed = 100 -> Not obtained yet\n",
    "# -----------------------\n",
    "# average f1 -> Not obtained yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cellfinder\n",
    "# seed = 42 -> 0.86      0.63      0.73\n",
    "# seed = 0  -> 0.84      0.77      0.80\n",
    "# seed = 13 -> 0.83      0.70      0.76\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

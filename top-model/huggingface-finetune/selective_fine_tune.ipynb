{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.9.0+cpu\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/cpu/torch-1.9.0%2Bcpu-cp36-cp36m-linux_x86_64.whl (175.5MB)\n",
      "\u001b[K     |████████████████████████████████| 175.5MB 90kB/s /s eta 0:00:01s eta 0:00:01     |██████████████████████▉         | 125.3MB 118.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision==0.10.0+cpu\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.10.0%2Bcpu-cp36-cp36m-linux_x86_64.whl (15.7MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7MB 71.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchaudio==0.9.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/75/e432d6c58771668ed917038a6d473edfdd5465640eec169f49a823ecf0cc/torchaudio-0.9.0-cp36-cp36m-manylinux1_x86_64.whl (1.9MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9MB 7.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.9.0+cpu) (3.7.4.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.10.0+cpu) (1.17.4)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.10.0+cpu) (6.2.1)\n",
      "Installing collected packages: dataclasses, torch, torchvision, torchaudio\n",
      "Successfully installed dataclasses-0.8 torch-1.9.0+cpu torchaudio-0.9.0 torchvision-0.10.0+cpu\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch==1.9.0+cpu torchvision==0.10.0+cpu torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/f5/b76d021f06e50f770d3f6c1a1b50b62a69e587b1f0db7248269c4be21206/torch-1.10.1-cp36-cp36m-manylinux1_x86_64.whl (881.9MB)\n",
      "\u001b[K     |████████████████████████████████| 881.9MB 11kB/s /s eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/03/c963ecdf98fae15286437ae533750e2c39e988b7d8c86fad4dbc73a3a146/torchvision-0.11.2-cp36-cp36m-manylinux1_x86_64.whl (23.3MB)\n",
      "\u001b[K     |████████████████████████████████| 23.3MB 35.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchaudio\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/4b/f5c4127441dae6fe75f2da89eb203f05c68c30e10ef24a9639d899fbdf66/torchaudio-0.10.1-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 97.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.1)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.4)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (6.2.1)\n",
      "Installing collected packages: dataclasses, torch, torchvision, torchaudio\n",
      "Successfully installed dataclasses-0.8 torch-1.10.1 torchaudio-0.10.1 torchvision-0.11.2\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/9e/5b80becd952d5f7250eaf8fc64b957077b12ccfe73e9c03d37146ab29712/transformers-4.6.0-py3-none-any.whl (2.3MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3MB 6.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytorch-crf\n",
      "  Downloading https://files.pythonhosted.org/packages/96/7d/4c4688e26ea015fc118a0327e5726e6596836abce9182d3738be8ec2e32a/pytorch_crf-0.7.2-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.6.0) (1.17.4)\n",
      "Collecting regex!=2019.12.17\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/99/dad689cc27a041a01376957c4c3b0147bcc537c93dc01e03e89ebc5df807/regex-2021.11.10-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748kB)\n",
      "\u001b[K     |████████████████████████████████| 757kB 68.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/e5/407e634cbd3b96a9ce6960874c5b66829592ead9ac762bd50662244ce20b/sacremoses-0.0.47-py2.py3-none-any.whl (895kB)\n",
      "\u001b[K     |████████████████████████████████| 901kB 83.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.6.0) (19.2)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/20/3605db440db4f96d5ffd66b231a043ae451ec7e5e4d1a2fb6f20608006c4/tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 72.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.6.0) (1.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.6.0) (2.22.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.6.0) (0.8)\n",
      "Collecting tqdm>=4.27\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/f3/b7a1b8e40fd1bd049a34566eb353527bb9b8e9b98f8b6cf803bb64d8ce95/tqdm-4.62.3-py2.py3-none-any.whl (76kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 21.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading https://files.pythonhosted.org/packages/84/ce/8916d10ef537f3f3b046843255f9799504aa41862bfa87844b9bdc5361cd/filelock-3.4.1-py3-none-any.whl\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.6.0) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.6.0) (0.13.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.6.0) (1.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.6.0) (2.4.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0) (0.6.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.6.0) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.6.0) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.6.0) (1.25.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.6.0) (3.0.4)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->transformers==4.6.0) (8.0.2)\n",
      "Installing collected packages: regex, tqdm, sacremoses, filelock, huggingface-hub, tokenizers, transformers, pytorch-crf\n",
      "Successfully installed filelock-3.4.1 huggingface-hub-0.0.8 pytorch-crf-0.7.2 regex-2021.11.10 sacremoses-0.0.47 tokenizers-0.10.3 tqdm-4.62.3 transformers-4.6.0\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers==4.6.0 pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed value\n",
    "seed_value= 42\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "# 4. Set `pytorch` pseudo-random generator at a fixed value\n",
    "import torch\n",
    "torch.manual_seed(seed_value)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/sbksvol/nikhil/NER_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENT = \"Cellline\"\n",
    "DATASET = \"jnlpba\"\n",
    "import os\n",
    "data_dir = os.path.join(data_path, ENT, DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = os.path.join(data_path, ENT, DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/6d/6463d49a933f547439d6b5b98b46af8742cc03ae83543e4d7688c2420f8b/pip-21.3.1-py3-none-any.whl (1.7MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7MB 9.7MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 19.3.1\n",
      "    Uninstalling pip-19.3.1:\n",
      "      Successfully uninstalled pip-19.3.1\n",
      "Successfully installed pip-21.3.1\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "     |████████████████████████████████| 43 kB 1.9 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.17.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.21.3)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (0.13.2)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16171 sha256=fea51c9559dbfc9aea55718f9cecabfe22f071695bda3535629ca0cd6f298c60\n",
      "  Stored in directory: /root/.cache/pip/wheels/39/29/36/1c4f7905c133e11748ca375960154964082d4fb03478323089\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install pytorch-crf\n",
    "!pip3 install --upgrade pip\n",
    "!pip3 install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.50      0.50      0.50         2\n",
      "\n",
      "   micro avg       0.50      0.50      0.50         2\n",
      "   macro avg       0.50      0.50      0.50         2\n",
      "weighted avg       0.50      0.50      0.50         2\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.50      0.50      0.50         2\n",
      "\n",
      "   micro avg       0.50      0.50      0.50         2\n",
      "   macro avg       0.50      0.50      0.50         2\n",
      "weighted avg       0.50      0.50      0.50         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB2, IOB1\n",
    "y_true = [['O','O', 'B', 'O','B']]\n",
    "y_pred = [['O','B', 'I', 'O','B']]\n",
    "print(classification_report(y_true, y_pred,output_dict=False))\n",
    "print(classification_report(y_true, y_pred, mode='strict', scheme=IOB2,output_dict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (21.3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.12.9-py2.py3-none-any.whl (1.7 MB)\n",
      "     |████████████████████████████████| 1.7 MB 9.7 MB/s            \n",
      "\u001b[?25hCollecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
      "     |████████████████████████████████| 170 kB 97.5 MB/s            \n",
      "\u001b[?25hCollecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
      "Collecting configparser>=3.8.1\n",
      "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting protobuf>=3.12.0\n",
      "  Downloading protobuf-3.19.3-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "     |████████████████████████████████| 1.1 MB 93.6 MB/s            \n",
      "\u001b[?25hCollecting subprocess32>=3.5.3\n",
      "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
      "     |████████████████████████████████| 97 kB 336 kB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.0)\n",
      "Collecting yaspin>=1.0.0\n",
      "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from wandb) (5.1.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.22.0)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.5.2-py2.py3-none-any.whl (142 kB)\n",
      "     |████████████████████████████████| 142 kB 105.0 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.6.7)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting six>=1.13.0\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.6/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.1)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "     |████████████████████████████████| 63 kB 140 kB/s             \n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (1.25.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2.6)\n",
      "Requirement already satisfied: dataclasses<0.9,>=0.8 in /usr/local/lib/python3.6/dist-packages (from yaspin>=1.0.0->wandb) (0.8)\n",
      "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: promise, subprocess32, pathtools\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=9245d2c5f2f880bdbab61b03b695d80295dda5a539c27f1ef860eef150d32baf\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/9a/1d/3f1afbbb5122d0410547bf9eb50955f4a7a98e53a6d8b99bd1\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6488 sha256=45b24f719206908d9bd8e2e22f08288118e64fab773355b3576ac170af5fc4f9\n",
      "  Stored in directory: /root/.cache/pip/wheels/44/3a/ab/102386d84fe551b6cedb628ed1e74c5f5be76af8b909aeda09\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8785 sha256=25abde0aab6f2465ecf215e27be8d46d558d027434879e630d631582f0257772\n",
      "  Stored in directory: /root/.cache/pip/wheels/42/ea/90/e37d463fb3b03848bf715080595de62545266f53dd546b2497\n",
      "Successfully built promise subprocess32 pathtools\n",
      "Installing collected packages: smmap, six, gitdb, yaspin, subprocess32, shortuuid, sentry-sdk, protobuf, promise, pathtools, GitPython, docker-pycreds, configparser, wandb\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.12.0\n",
      "    Uninstalling six-1.12.0:\n",
      "      Successfully uninstalled six-1.12.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.9.1\n",
      "    Uninstalling protobuf-3.9.1:\n",
      "      Successfully uninstalled protobuf-3.9.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorboard 2.0.2 requires grpcio>=1.24.3, but you have grpcio 1.22.0 which is incompatible.\u001b[0m\n",
      "Successfully installed GitPython-3.1.18 configparser-5.2.0 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 promise-2.3 protobuf-3.19.3 sentry-sdk-1.5.2 shortuuid-1.0.8 six-1.16.0 smmap-5.0.0 subprocess32-3.5.4 wandb-0.12.9 yaspin-2.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting ray[tune]\n",
      "  Downloading ray-1.9.2-cp36-cp36m-manylinux2014_x86_64.whl (57.6 MB)\n",
      "     |████████████████████████████████| 57.6 MB 8.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (3.2.0)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (0.8)\n",
      "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (3.19.3)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (19.3.0)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (7.0)\n",
      "Collecting redis>=3.5.0\n",
      "  Downloading redis-4.1.0-py3-none-any.whl (171 kB)\n",
      "     |████████████████████████████████| 171 kB 104.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (5.1.2)\n",
      "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (1.17.4)\n",
      "Collecting msgpack<2.0.0,>=1.0.0\n",
      "  Downloading msgpack-1.0.3-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
      "     |████████████████████████████████| 299 kB 115.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (3.4.1)\n",
      "Collecting grpcio>=1.28.1\n",
      "  Downloading grpcio-1.43.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
      "     |████████████████████████████████| 4.1 MB 68.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (2.22.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (0.25.3)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Collecting tensorboardX>=1.9\n",
      "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
      "     |████████████████████████████████| 124 kB 78.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from grpcio>=1.28.1->ray[tune]) (1.16.0)\n",
      "Collecting deprecated>=1.2.3\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.0 in /usr/local/lib/python3.6/dist-packages (from redis>=3.5.0->ray[tune]) (1.3.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting packaging>=21.3\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "     |████████████████████████████████| 40 kB 20.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from jsonschema->ray[tune]) (41.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema->ray[tune]) (0.15.6)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->ray[tune]) (2019.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->ray[tune]) (2.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->ray[tune]) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->ray[tune]) (1.25.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->ray[tune]) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests->ray[tune]) (2.6)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.3->redis>=3.5.0->ray[tune]) (1.11.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=1.0->redis>=3.5.0->ray[tune]) (0.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=21.3->redis>=3.5.0->ray[tune]) (2.4.2)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib-metadata>=1.0->redis>=3.5.0->ray[tune]) (8.0.2)\n",
      "Installing collected packages: packaging, deprecated, redis, msgpack, grpcio, tensorboardX, tabulate, ray\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 19.2\n",
      "    Uninstalling packaging-19.2:\n",
      "      Successfully uninstalled packaging-19.2\n",
      "  Attempting uninstall: msgpack\n",
      "    Found existing installation: msgpack 0.6.2\n",
      "    Uninstalling msgpack-0.6.2:\n",
      "      Successfully uninstalled msgpack-0.6.2\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.22.0\n",
      "    Uninstalling grpcio-1.22.0:\n",
      "      Successfully uninstalled grpcio-1.22.0\n",
      "Successfully installed deprecated-1.2.13 grpcio-1.43.0 msgpack-1.0.3 packaging-21.3 ray-1.9.2 redis-4.1.0 tabulate-0.8.9 tensorboardX-2.4.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting six\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: six\n",
      "Successfully installed six-1.16.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade pip\n",
    "!pip3 install wandb\n",
    "!pip3 install \"ray[tune]\"\n",
    "!pip3 install --ignore-installed six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcrf import CRF\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_sents, num_dev_sents, num_test_sents =  3286 555 1632\n",
      "First 10 words in test data:\n",
      "   sentence_id      words labels\n",
      "0            0  Selection      O\n",
      "1            0         of      O\n",
      "2            0      cDNAs      O\n",
      "3            0   encoding      O\n",
      "4            0   putative      O\n",
      "5            0       type      O\n",
      "6            0         II      O\n",
      "7            0   membrane      O\n",
      "8            0   proteins      O\n",
      "9            0         on      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [ 7.54568574 33.00675407  0.35246302]\n",
      "Split.dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0113 07:49:08.483378 140454686455616 utils_ner.py:113] Creating features from dataset file at /sbksvol/nikhil/NER_data/Gene/deca\n",
      "I0113 07:49:08.547636 140454686455616 utils_ner.py:322] Writing example 0 of 3285\n",
      "I0113 07:49:08.549216 140454686455616 utils_ner.py:410] *** Example ***\n",
      "I0113 07:49:08.549551 140454686455616 utils_ner.py:411] guid: train-1\n",
      "I0113 07:49:08.550644 140454686455616 utils_ner.py:412] tokens: [CLS] T ##IF ##1 ##gam ##ma , a novel member of the transcription ##al inter ##media ##ry factor 1 family . [SEP]\n",
      "I0113 07:49:08.550991 140454686455616 utils_ner.py:413] input_ids: 101 157 15499 1475 17634 1918 117 170 2281 1420 1104 1103 15416 1348 9455 16418 1616 5318 122 1266 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:08.551342 140454686455616 utils_ner.py:414] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:08.551691 140454686455616 utils_ner.py:415] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:08.551968 140454686455616 utils_ner.py:416] label_ids: -100 0 1 1 1 1 2 2 2 2 2 2 0 1 1 1 1 1 1 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0113 07:49:08.553369 140454686455616 utils_ner.py:410] *** Example ***\n",
      "I0113 07:49:08.553675 140454686455616 utils_ner.py:411] guid: train-2\n",
      "I0113 07:49:08.553977 140454686455616 utils_ner.py:412] tokens: [CLS] We report the c ##lon ##ing and characterization of a novel member of the Trans ##cription ##al Inter ##media ##ry Factor 1 ( T ##IF ##1 ) gene family , human T ##IF ##1 ##gam ##ma . [SEP]\n",
      "I0113 07:49:08.554327 140454686455616 utils_ner.py:413] input_ids: 101 1284 2592 1103 172 4934 1158 1105 27419 1104 170 2281 1420 1104 1103 13809 27530 1348 11300 16418 1616 15926 122 113 157 15499 1475 114 5565 1266 117 1769 157 15499 1475 17634 1918 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:08.554622 140454686455616 utils_ner.py:414] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:08.554896 140454686455616 utils_ner.py:415] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:08.555164 140454686455616 utils_ner.py:416] label_ids: -100 2 2 2 2 -100 -100 2 2 2 2 2 2 2 2 0 1 1 1 1 1 1 1 2 0 1 1 2 2 2 2 2 0 1 1 1 1 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0113 07:49:08.556685 140454686455616 utils_ner.py:410] *** Example ***\n",
      "I0113 07:49:08.556991 140454686455616 utils_ner.py:411] guid: train-3\n",
      "I0113 07:49:08.557302 140454686455616 utils_ner.py:412] tokens: [CLS] Similar to T ##IF ##1 ##al ##pha and T ##IF ##1 ##bet ##a , the structure of T ##IF ##1 ##bet ##a is characterized by multiple domains : R ##ING finger , B boxes , Co ##iled coil , P ##HD / T ##TC , and br ##omo ##dom ##ain . [SEP]\n",
      "I0113 07:49:08.557636 140454686455616 utils_ner.py:413] input_ids: 101 12250 1106 157 15499 1475 1348 20695 1105 157 15499 1475 16632 1161 117 1103 2401 1104 157 15499 1475 16632 1161 1110 6858 1118 2967 13770 131 155 15740 3602 117 139 8171 117 3291 11908 20614 117 153 23527 120 157 9481 117 1105 9304 18445 9277 8104 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0113 07:49:08.557914 140454686455616 utils_ner.py:414] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:08.558179 140454686455616 utils_ner.py:415] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:08.558462 140454686455616 utils_ner.py:416] label_ids: -100 2 2 0 1 1 1 1 2 0 1 1 1 1 2 2 2 2 0 1 1 1 1 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 -100 2 2 2 -100 -100 -100 -100 2 2 2 -100 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0113 07:49:08.559430 140454686455616 utils_ner.py:410] *** Example ***\n",
      "I0113 07:49:08.559686 140454686455616 utils_ner.py:411] guid: train-4\n",
      "I0113 07:49:08.559926 140454686455616 utils_ner.py:412] tokens: [CLS] Although structural ##ly related to T ##IF ##1 ##al ##pha and T ##IF ##1 ##bet ##a , T ##IF ##1 ##gam ##ma presents several functional differences . [SEP]\n",
      "I0113 07:49:08.560192 140454686455616 utils_ner.py:413] input_ids: 101 1966 8649 1193 2272 1106 157 15499 1475 1348 20695 1105 157 15499 1475 16632 1161 117 157 15499 1475 17634 1918 8218 1317 8458 5408 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:08.560472 140454686455616 utils_ner.py:414] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:08.560739 140454686455616 utils_ner.py:415] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:08.561069 140454686455616 utils_ner.py:416] label_ids: -100 2 2 -100 2 2 0 1 1 1 1 2 0 1 1 1 1 2 0 1 1 1 1 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0113 07:49:08.562880 140454686455616 utils_ner.py:410] *** Example ***\n",
      "I0113 07:49:08.563179 140454686455616 utils_ner.py:411] guid: train-5\n",
      "I0113 07:49:08.563492 140454686455616 utils_ner.py:412] tokens: [CLS] In contrast to T ##IF ##1 ##al ##pha , but like T ##IF ##1 ##bet ##a , T ##IF ##1 does not interact with nuclear receptors in yeast two - hybrid or G ##ST pull - down ass ##ays and does not interfere with re ##tino ##ic acid response in trans ##fected ma ##mmal ##ian cells . [SEP]\n",
      "I0113 07:49:08.563778 140454686455616 utils_ner.py:413] input_ids: 101 1130 5014 1106 157 15499 1475 1348 20695 117 1133 1176 157 15499 1475 16632 1161 117 157 15499 1475 1674 1136 12254 1114 4272 14392 1107 25693 1160 118 9890 1137 144 9272 3373 118 1205 3919 22979 1105 1674 1136 15891 1114 1231 20064 1596 5190 2593 1107 14715 21601 12477 27568 1811 3652 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:08.564056 140454686455616 utils_ner.py:414] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:08.564395 140454686455616 utils_ner.py:415] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0113 07:49:08.564687 140454686455616 utils_ner.py:416] label_ids: -100 2 2 2 0 1 1 1 1 2 2 2 0 1 1 1 1 2 0 1 1 2 2 2 2 2 2 2 2 2 -100 -100 2 2 -100 2 -100 -100 2 -100 2 2 2 2 2 2 -100 -100 2 2 2 2 -100 2 -100 -100 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0113 07:49:11.990449 140454686455616 utils_ner.py:139] Saving features into cached file /sbksvol/nikhil/NER_data/Gene/deca/cached_train_BertTokenizer_256\n",
      "I0113 07:49:13.565454 140454686455616 utils_ner.py:113] Creating features from dataset file at /sbksvol/nikhil/NER_data/Gene/deca\n",
      "I0113 07:49:13.577726 140454686455616 utils_ner.py:322] Writing example 0 of 554\n",
      "I0113 07:49:13.579148 140454686455616 utils_ner.py:410] *** Example ***\n",
      "I0113 07:49:13.579457 140454686455616 utils_ner.py:411] guid: dev-1\n",
      "I0113 07:49:13.579728 140454686455616 utils_ner.py:412] tokens: [CLS] I ##dent ##ification of a W ##nt - re ##sp ##ons ##ive signal trans ##duction pathway in primary end ##oth ##eli ##al cells . [SEP]\n",
      "I0113 07:49:13.580003 140454686455616 utils_ner.py:413] input_ids: 101 146 11951 5783 1104 170 160 2227 118 1231 20080 4199 2109 4344 14715 11243 13548 1107 2425 1322 12858 21091 1348 3652 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:13.580292 140454686455616 utils_ner.py:414] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:13.580565 140454686455616 utils_ner.py:415] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:13.580839 140454686455616 utils_ner.py:416] label_ids: -100 2 -100 -100 2 2 0 1 1 1 1 1 1 2 2 -100 2 2 2 2 -100 -100 -100 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0113 07:49:13.582644 140454686455616 utils_ner.py:410] *** Example ***\n",
      "I0113 07:49:13.583126 140454686455616 utils_ner.py:411] guid: dev-2\n",
      "I0113 07:49:13.583518 140454686455616 utils_ner.py:412] tokens: [CLS] The beta - cat ##eni ##n signal trans ##duction pathway , which can be activated by secret ##ed W ##nt proteins , plays a key role in normal em ##b ##ryo ##nic development and in ma ##li ##gnant transformation of the ma ##mma ##ry g ##land and co ##lon . [SEP]\n",
      "I0113 07:49:13.583821 140454686455616 utils_ner.py:413] input_ids: 101 1109 11933 118 5855 21462 1179 4344 14715 11243 13548 117 1134 1169 1129 9618 1118 3318 1174 160 2227 7865 117 2399 170 2501 1648 1107 2999 9712 1830 26503 7770 1718 1105 1107 12477 2646 15454 9047 1104 1103 12477 12917 1616 176 1931 1105 1884 4934 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:13.584095 140454686455616 utils_ner.py:414] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:13.584435 140454686455616 utils_ner.py:415] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:13.584769 140454686455616 utils_ner.py:416] label_ids: -100 2 0 1 1 1 1 2 2 -100 2 2 2 2 2 2 2 2 -100 0 1 2 2 2 2 2 2 2 2 2 -100 -100 -100 2 2 2 2 -100 -100 2 2 2 2 -100 -100 2 -100 2 2 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0113 07:49:13.585939 140454686455616 utils_ner.py:410] *** Example ***\n",
      "I0113 07:49:13.586189 140454686455616 utils_ner.py:411] guid: dev-3\n",
      "I0113 07:49:13.586435 140454686455616 utils_ner.py:412] tokens: [CLS] Here we demonstrate , for the first time , that W ##nt and beta - cat ##eni ##n signaling also function in cells of the v ##as ##cula ##ture . [SEP]\n",
      "I0113 07:49:13.586702 140454686455616 utils_ner.py:413] input_ids: 101 3446 1195 10541 117 1111 1103 1148 1159 117 1115 160 2227 1105 11933 118 5855 21462 1179 16085 1145 3053 1107 3652 1104 1103 191 2225 21608 5332 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:13.586971 140454686455616 utils_ner.py:414] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:13.587243 140454686455616 utils_ner.py:415] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:13.587579 140454686455616 utils_ner.py:416] label_ids: -100 2 2 2 2 2 2 2 2 2 2 0 1 2 0 1 1 1 1 2 2 2 2 2 2 2 2 -100 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0113 07:49:13.589197 140454686455616 utils_ner.py:410] *** Example ***\n",
      "I0113 07:49:13.589456 140454686455616 utils_ner.py:411] guid: dev-4\n",
      "I0113 07:49:13.589694 140454686455616 utils_ner.py:412] tokens: [CLS] R ##T - PC ##R analysis showed that primary end ##oth ##eli ##al and smooth muscle cell cultures , of both mouse and human origin , express members of the W ##nt and W ##nt receptor ( Fr ##iz ##zled ) gene families . [SEP]\n",
      "I0113 07:49:13.589967 140454686455616 utils_ner.py:413] input_ids: 101 155 1942 118 7054 2069 3622 2799 1115 2425 1322 12858 21091 1348 1105 5307 6484 2765 8708 117 1104 1241 10322 1105 1769 4247 117 6848 1484 1104 1103 160 2227 1105 160 2227 10814 113 13359 9368 21524 114 5565 2073 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:13.590245 140454686455616 utils_ner.py:414] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:13.590578 140454686455616 utils_ner.py:415] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:13.590915 140454686455616 utils_ner.py:416] label_ids: -100 2 -100 -100 -100 -100 2 2 2 2 2 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 1 2 0 1 2 2 2 -100 -100 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0113 07:49:13.592411 140454686455616 utils_ner.py:410] *** Example ***\n",
      "I0113 07:49:13.592711 140454686455616 utils_ner.py:411] guid: dev-5\n",
      "I0113 07:49:13.593015 140454686455616 utils_ner.py:412] tokens: [CLS] Trans ##fect ##ion of an expression vector for W ##nt - 1 into primary end ##oth ##eli ##al cells increased both the free pool of beta - cat ##eni ##n and the transcription from a Le ##f / t ##c ##f - dependent reporter gene construct . [SEP]\n",
      "I0113 07:49:13.593360 140454686455616 utils_ner.py:413] input_ids: 101 13809 11916 1988 1104 1126 2838 9479 1111 160 2227 118 122 1154 2425 1322 12858 21091 1348 3652 2569 1241 1103 1714 4528 1104 11933 118 5855 21462 1179 1105 1103 15416 1121 170 3180 2087 120 189 1665 2087 118 7449 6672 5565 9417 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:13.593694 140454686455616 utils_ner.py:414] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0113 07:49:13.593971 140454686455616 utils_ner.py:415] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:13.594250 140454686455616 utils_ner.py:416] label_ids: -100 2 -100 -100 2 2 2 2 2 0 1 1 1 2 2 2 -100 -100 -100 2 2 2 2 2 2 2 0 1 1 1 1 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0113 07:49:14.192324 140454686455616 utils_ner.py:139] Saving features into cached file /sbksvol/nikhil/NER_data/Gene/deca/cached_dev_BertTokenizer_256\n",
      "I0113 07:49:15.395507 140454686455616 utils_ner.py:113] Creating features from dataset file at /sbksvol/nikhil/NER_data/Gene/deca\n",
      "I0113 07:49:15.429651 140454686455616 utils_ner.py:322] Writing example 0 of 1631\n",
      "I0113 07:49:15.431270 140454686455616 utils_ner.py:410] *** Example ***\n",
      "I0113 07:49:15.431596 140454686455616 utils_ner.py:411] guid: test-1\n",
      "I0113 07:49:15.431849 140454686455616 utils_ner.py:412] tokens: [CLS] Selection of c ##D ##NA ##s encoding put ##ative type II membrane proteins on the cell surface from a human full - length c ##D ##NA bank . [SEP]\n",
      "I0113 07:49:15.432171 140454686455616 utils_ner.py:413] input_ids: 101 20045 1104 172 2137 11185 1116 18922 1508 5838 2076 1563 10936 7865 1113 1103 2765 2473 1121 170 1769 1554 118 2251 172 2137 11185 3085 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:15.432438 140454686455616 utils_ner.py:414] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:15.432776 140454686455616 utils_ner.py:415] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:15.433105 140454686455616 utils_ner.py:416] label_ids: -100 2 2 2 -100 -100 -100 2 2 -100 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 -100 -100 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0113 07:49:15.434767 140454686455616 utils_ner.py:410] *** Example ***\n",
      "I0113 07:49:15.435070 140454686455616 utils_ner.py:411] guid: test-2\n",
      "I0113 07:49:15.435328 140454686455616 utils_ner.py:412] tokens: [CLS] We have developed a simple method to test whether a h ##ydro ##phobic segment near the N - terminus of a protein functions as a type II signal anchor ( SA ) in which the N - terminus faces the c ##yt ##op ##las ##m . [SEP]\n",
      "I0113 07:49:15.435601 140454686455616 utils_ner.py:413] input_ids: 101 1284 1138 1872 170 3014 3442 1106 2774 2480 170 177 19694 22050 6441 1485 1103 151 118 7132 1104 170 4592 4226 1112 170 2076 1563 4344 8494 113 13411 114 1107 1134 1103 151 118 7132 4876 1103 172 25669 4184 7580 1306 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:15.436037 140454686455616 utils_ner.py:414] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:15.436365 140454686455616 utils_ner.py:415] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:15.436653 140454686455616 utils_ner.py:416] label_ids: -100 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 2 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 -100 -100 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0113 07:49:15.438294 140454686455616 utils_ner.py:410] *** Example ***\n",
      "I0113 07:49:15.438545 140454686455616 utils_ner.py:411] guid: test-3\n",
      "I0113 07:49:15.438783 140454686455616 utils_ner.py:412] tokens: [CLS] A c ##D ##NA fragment containing the put ##ative SA sequence of a target clone was fused in - frame to the 5 ' end of a c ##D ##NA fragment encoding the pro ##te ##ase domain of u ##rok ##inas ##e - type p ##las ##min ##ogen act ##iva ##tor ( u - PA ) . [SEP]\n",
      "I0113 07:49:15.439052 140454686455616 utils_ner.py:413] input_ids: 101 138 172 2137 11185 17906 4051 1103 1508 5838 13411 4954 1104 170 4010 22121 1108 21859 1107 118 4207 1106 1103 126 112 1322 1104 170 172 2137 11185 17906 18922 1103 5250 1566 6530 5777 1104 190 24830 16924 1162 118 2076 185 7580 7937 19790 2496 12416 2772 113 190 118 8544 114 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:15.439332 140454686455616 utils_ner.py:414] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:15.439600 140454686455616 utils_ner.py:415] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:15.439869 140454686455616 utils_ner.py:416] label_ids: -100 2 2 -100 -100 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 -100 2 2 2 2 -100 -100 2 2 2 2 -100 -100 2 2 0 1 1 1 1 1 1 1 1 1 1 1 1 2 -100 -100 -100 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0113 07:49:15.440539 140454686455616 utils_ner.py:410] *** Example ***\n",
      "I0113 07:49:15.440778 140454686455616 utils_ner.py:411] guid: test-4\n",
      "I0113 07:49:15.441009 140454686455616 utils_ner.py:412] tokens: [CLS] The resulting fused gene was expressed in CO ##S ##7 cells . [SEP]\n",
      "I0113 07:49:15.441282 140454686455616 utils_ner.py:413] input_ids: 101 1109 3694 21859 5565 1108 4448 1107 18732 1708 1559 3652 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:15.441556 140454686455616 utils_ner.py:414] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:15.441820 140454686455616 utils_ner.py:415] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:15.442087 140454686455616 utils_ner.py:416] label_ids: -100 2 2 2 2 2 2 2 2 -100 -100 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0113 07:49:15.443373 140454686455616 utils_ner.py:410] *** Example ***\n",
      "I0113 07:49:15.443678 140454686455616 utils_ner.py:411] guid: test-5\n",
      "I0113 07:49:15.443971 140454686455616 utils_ner.py:412] tokens: [CLS] Fi ##bri ##no ##ly ##tic activity on the cell surface was measured by placing a fi ##bri ##n sheet in contact with the trans ##fected CO ##S ##7 cells after removing the medium . [SEP]\n",
      "I0113 07:49:15.444256 140454686455616 utils_ner.py:413] input_ids: 101 17355 27647 2728 1193 2941 3246 1113 1103 2765 2473 1108 7140 1118 6544 170 20497 27647 1179 6837 1107 3232 1114 1103 14715 21601 18732 1708 1559 3652 1170 9305 1103 5143 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:15.444528 140454686455616 utils_ner.py:414] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0113 07:49:15.444792 140454686455616 utils_ner.py:415] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0113 07:49:15.445057 140454686455616 utils_ner.py:416] label_ids: -100 2 -100 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 2 2 2 -100 2 -100 -100 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0113 07:49:17.104630 140454686455616 utils_ner.py:139] Saving features into cached file /sbksvol/nikhil/NER_data/Gene/deca/cached_test_BertTokenizer_256\n"
     ]
    }
   ],
   "source": [
    "from run_v2 import prepare_data, params, prepare_config_and_tokenizer\n",
    "import utils_ner\n",
    "import importlib\n",
    "importlib.reload(utils_ner)\n",
    "from utils_ner import NerDataset, Split\n",
    "from models_factory import get_model\n",
    "ENT = \"Gene\"\n",
    "DATASET = \"deca\"\n",
    "import os\n",
    "data_dir = os.path.join(data_path, ENT, DATASET)\n",
    "params[\"entity_type\"] = ENT\n",
    "params[\"dataset\"] = DATASET\n",
    "\n",
    "params[\"LOWER_CASE\"] = False\n",
    "params[\"LOAD_BEST_MODEL\"] = True\n",
    "params[\"MAX_LEN\"] = 256\n",
    "params[\"BATCH_SIZE\"] = 32\n",
    "params[\"EPOCH_TOP\"] = 100\n",
    "params[\"EPOCH_END2END\"] = 100\n",
    "\n",
    "params[\"EXP_NAME\"] = \"test\"\n",
    "params[\"WORKING_DIR\"] = \"sbksvol/nikhil/\" + params[\"EXP_NAME\"] + \"/\"\n",
    "params[\"CACHE_DIR\"] = params[\"WORKING_DIR\"] + \"NER_out_test/\"\n",
    "\n",
    "# Where model checkpoints are stored.\n",
    "params[\"OUTPUT_DIR\"] = params[\"WORKING_DIR\"] + \"model_output_test/\"\n",
    "params[\"TRAIN_ARGS_FILE\"] = params[\"WORKING_DIR\"] + \"train_args_test.json\"\n",
    "params[\"DATA_PATH\"] = data_path\n",
    "\n",
    "train_df, test_df, dev_df, labels, num_labels, label_map, data_dir, wt = prepare_data()\n",
    "\n",
    "data_args, model_args, config, tokenizer = prepare_config_and_tokenizer(\n",
    "    data_dir, labels, num_labels, label_map)\n",
    "\n",
    "## Create Dataset Objects\n",
    "print(Split.dev)\n",
    "\n",
    "train_dataset = NerDataset(\n",
    "    data_dir=data_args['data_dir'],\n",
    "    tokenizer=tokenizer,\n",
    "    labels=labels,\n",
    "    model_type=config.model_type,\n",
    "    max_seq_length=data_args['max_seq_length'],\n",
    "    overwrite_cache=data_args['overwrite_cache'],  # True\n",
    "    mode=Split.train, data_size=100)\n",
    "\n",
    "val_dataset = NerDataset(\n",
    "    data_dir=data_args['data_dir'],\n",
    "    tokenizer=tokenizer,\n",
    "    labels=labels,\n",
    "    model_type=config.model_type,\n",
    "    max_seq_length=data_args['max_seq_length'],\n",
    "    overwrite_cache=data_args['overwrite_cache'],  # True\n",
    "    mode=Split.dev, data_size=100)\n",
    "\n",
    "test_dataset = NerDataset(\n",
    "    data_dir=data_args['data_dir'],\n",
    "    tokenizer=tokenizer,\n",
    "    labels=labels,\n",
    "    model_type=config.model_type,\n",
    "    max_seq_length=data_args['max_seq_length'],\n",
    "    overwrite_cache=data_args['overwrite_cache'],  # True\n",
    "    mode=Split.test, data_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sentence_id                words labels\n",
      "0                0         Radiolabeled      O\n",
      "1                0                    m      B\n",
      "2                0                    -      I\n",
      "3                0  iodobenzylguanidine      I\n",
      "4                0                    (      O\n",
      "...            ...                  ...    ...\n",
      "18579          590          transfusion      O\n",
      "18580          590                    -      O\n",
      "18581          590            dependent      O\n",
      "18582          590          thalassemia      O\n",
      "18583          590                    .      O\n",
      "\n",
      "[18584 rows x 3 columns]\n",
      "['m']\n",
      "['MI', '##B', '##G']\n",
      "['nor', '##ep', '##ine', '##ph', '##rine']\n",
      "['NE']\n",
      "['(']\n",
      "['ni', '##so', '##xe', '##tine']\n",
      "['1']\n",
      "['derivatives']\n",
      "['r', '##ac']\n",
      "['g', '##uan', '##id', '##ine']\n",
      "['derivative']\n",
      "['2']\n",
      "['th', '##io', '##phe', '##ne']\n",
      "['et', '##hyl']\n",
      "['th', '##ien', '##o', '##2']\n",
      "['th', '##ien', '##o', '##3']\n",
      "['o', '##xa', '##zin']\n",
      "['4', '##H']\n",
      "['ben', '##zen', '##e']\n",
      "['En', '##ala', '##p', '##ril']\n",
      "['en', '##ala', '##p', '##ril']\n",
      "['u', '##rea']\n",
      "['C', '##rea', '##tin', '##ine']\n",
      "['c', '##rea', '##tin', '##ine']\n",
      "['ni', '##trate']\n",
      "['Tam', '##ox', '##ife', '##n']\n",
      "['ta', '##mo', '##xi', '##fen']\n",
      "['tore', '##mi', '##fen', '##e']\n",
      "['ni', '##tric']\n",
      "['ni', '##tri', '##te']\n",
      "['All', '##op', '##uri', '##no', '##l']\n",
      "['p', '##yr', '##im', '##id', '##ine']\n",
      "['all', '##op', '##uri', '##no', '##l']\n",
      "['din', '##it', '##ro', '##f', '##lu', '##oro', '##ben', '##zen', '##e']\n",
      "['D', '##NF', '##B']\n",
      "['u', '##rid', '##ine']\n",
      "['or', '##ot', '##id', '##ine']\n",
      "['O', '##D']\n",
      "['T', '##EI']\n",
      "['p', '##yr', '##id', '##ino']\n",
      "['6', ',', '7']\n",
      "['N']\n",
      "['6']\n",
      "['q', '##uin', '##olin', '##e']\n",
      "['m', '##oi', '##ety']\n",
      "['c', '##is']\n",
      "['Mai', '##to', '##to', '##xin']\n",
      "['yes', '##so', '##to', '##xin']\n",
      "['C', '##a']\n",
      "['ma', '##ito', '##to', '##xin']\n",
      "['M', '##T', '##X']\n",
      "['calcium']\n",
      "['Y', '##T', '##X']\n",
      "['Ni', '##C', '##l']\n",
      "['SK', '##F', '##9', '##6', '##36', '##5']\n",
      "['ni', '##fe', '##di', '##pine']\n",
      "['2', '##HC', '##l']\n",
      "['H']\n",
      "['g', '##eni', '##stein']\n",
      "['bi', '##sin', '##do', '##li', '##lma', '##le', '##im', '##ide']\n",
      "['G', '##F', '##10', '##9', '##20', '##3', '##X']\n",
      "['w', '##ort', '##mann', '##in']\n",
      "['Ni']\n",
      "['c', '##ira', '##zo', '##line']\n",
      "['im', '##ida', '##zo', '##line']\n",
      "['R', '##X', '##8', '##21', '##00', '##2']\n",
      "['a', '##c', '##rol', '##ein']\n",
      "['c', '##y', '##c', '##lop', '##hos', '##pha', '##mi', '##de']\n",
      "['analog']\n",
      "['4', ',', '4', ',', '4']\n",
      "['din', '##it', '##rophe', '##ny', '##l', '##hy', '##dra', '##zone']\n",
      "['must', '##ard']\n",
      "['analog', '##s']\n",
      "['5']\n",
      "['br', '##omi', '##ne']\n",
      "['sub', '##st', '##it', '##uen', '##t']\n",
      "['compounds']\n",
      "['t']\n",
      "['H', '##B', '##r']\n",
      "['di', '##ox', '##ane']\n",
      "['analogue', '##s']\n",
      "['[']\n",
      "['n', '##uc', '##leo', '##side', '##s']\n",
      "['4']\n",
      "['AM', '##PA']\n",
      "['ka', '##inate']\n",
      "['c', '##lon', '##id', '##ine']\n",
      "['nor', '##ad', '##rena', '##line']\n",
      "['group']\n",
      "['do', '##pa', '##mine']\n",
      "['l', '##ys', '##op', '##hos', '##pha', '##ti', '##date']\n",
      "['L', '##ys', '##op', '##hos', '##pha', '##ti', '##date']\n",
      "['LP', '##A']\n",
      "['t', '##yr', '##os', '##ine']\n",
      "['th', '##re', '##oni', '##ne']\n",
      "['t', '##yr', '##ph', '##ost', '##in']\n",
      "['AG']\n",
      "['But', '##an']\n",
      "['p', '##hos', '##pha', '##ti', '##date']\n",
      "['but', '##an']\n",
      "['N', '##uc', '##leo', '##tide']\n",
      "['n', '##uc', '##leo', '##tide', '##s']\n",
      "['ATP']\n",
      "['ad', '##eni', '##ne']\n",
      "['n', '##uc', '##leo', '##tide']\n",
      "['g', '##uan', '##ine']\n",
      "['ad', '##eno', '##sin', '##e']\n",
      "['A', '##pp']\n",
      "['AD', '##P']\n",
      "['M', '##g', '##2', '+']\n",
      "['p']\n",
      "['PC', '##MB', '##S']\n",
      "['GDP']\n",
      "['M', '##g', '##G', '##DP']\n",
      "['M', '##g', '##A', '##pp']\n",
      "['M', '##g', '##AT', '##P']\n",
      "['ma', '##gnesium']\n",
      "['M', '##g', '##G', '##TP']\n",
      "['Lu', '##bro', '##l']\n",
      "['M', '##n', '##G', '##TP']\n",
      "['2', ',', '4']\n",
      "['trim', '##eth', '##op', '##rim']\n",
      "['T', '##MP']\n",
      "['p', '##iri', '##tre', '##xi', '##m']\n",
      "['PT', '##X']\n",
      "['car', '##box', '##y', '##phe', '##ny', '##l']\n",
      "['ben', '##zy', '##l']\n",
      "['car', '##box', '##y']\n",
      "['un', '##sat', '##ura', '##ted']\n",
      "['au', '##gus', '##t', '##my', '##cin']\n",
      "['h', '##y', '##pox', '##ant', '##hine']\n",
      "['a', '##za', '##thi', '##op', '##rine']\n",
      "['9']\n",
      "['5', \"'\"]\n",
      "['intermediate', '##s']\n",
      "['an', '##gus', '##t', '##my', '##cin']\n",
      "['F', '##E', '##20', '##0', '##48', '##6']\n",
      "['de', '##gar', '##eli', '##x']\n",
      "['a', '##zal', '##ine']\n",
      "['8']\n",
      "['6', ',', '8']\n",
      "['c', '##ip', '##ro', '##f', '##lo', '##xa', '##cin']\n",
      "['8', '##H']\n",
      "['series']\n",
      "['and', '##rogen', '##s']\n",
      "['And', '##rogen', '##s']\n",
      "['and', '##rogen']\n",
      "['Test', '##osterone']\n",
      "['di', '##hy', '##dr', '##otes', '##tos', '##tero', '##ne']\n",
      "['and', '##ros', '##ten', '##ed', '##ione']\n",
      "['e', '##pit', '##est', '##osterone']\n",
      "['test', '##osterone']\n",
      "['p', '##yra', '##n']\n",
      "['th', '##io', '##py', '##ran']\n",
      "['p', '##yr', '##id', '##in']\n",
      "['p', '##yra', '##non', '##es']\n",
      "['th', '##io', '##py', '##rano', '##nes']\n",
      "['et', '##op', '##os', '##ide']\n",
      "['12']\n",
      "['12', ',', '21']\n",
      "['intermediate']\n",
      "['21']\n",
      "['analogue']\n",
      "['er', '##yt', '##hr', '##omy', '##cin']\n",
      "['e', '##pox', '##y']\n",
      "['2', ',', '3']\n",
      "['am', '##phe', '##tamine', '##s']\n",
      "['L', '##SD']\n",
      "['7']\n",
      "['D', '##O', '##B']\n",
      "['di', '##hy', '##dr', '##of', '##ura', '##n']\n",
      "['groups']\n",
      "['atom']\n",
      "['1', ',', '4']\n",
      "['h', '##ydro', '##ind', '##olo', '##quin', '##ones']\n",
      "['ben', '##zo', '##fu', '##ran']\n",
      "['ben', '##zo', '##thi', '##op', '##hen', '##e']\n",
      "['ace', '##tate', '##s']\n",
      "['al', '##ky', '##l']\n",
      "['c', '##y', '##c', '##lo', '##he', '##xi', '##mi', '##de']\n",
      "['act', '##ino', '##my', '##cin']\n",
      "['is', '##op', '##ren', '##oids']\n",
      "['ch', '##ole', '##ster', '##ol']\n",
      "['H', '##O', '##I']\n",
      "['vinyl']\n",
      "['i', '##od', '##oh', '##yd', '##rin', '##s']\n",
      "['met', '##han', '##olic']\n",
      "['sodium']\n",
      "['met', '##han', '##ol']\n",
      "['IV', '##D', '##U']\n",
      "['a', '##cy', '##c', '##lov', '##ir']\n",
      "['C']\n",
      "['2', \"'\"]\n",
      "['CH']\n",
      "['me', '##l', '##pha', '##lan']\n",
      "['de', '##ox', '##yu', '##rid', '##ine']\n",
      "['h', '##yd', '##raz', '##ones']\n",
      "['beta']\n",
      "['alpha']\n",
      "['p', '##hos', '##phorus']\n",
      "['-']\n",
      "['h', '##ydro', '##xy']\n",
      "['h', '##ydro', '##bro', '##mic']\n",
      "['me', '##zer', '##ein']\n",
      "['ME', '##Z']\n",
      "['me', '##lani', '##n']\n",
      "['CH', '##3', '##SN', '##a']\n",
      "['ben', '##za', '##zo', '##cine', '##s']\n",
      "['S']\n",
      "['N', '##PS']\n",
      "['p', '##hen', '##yl', '##sul', '##fen', '##yl']\n",
      "['PS']\n",
      "['C', '##m', '##PS']\n",
      "['p', '##NP', '##S']\n",
      "['D', '##NP', '##S']\n",
      "['A', '##ac', '##C', '##m', '##ES']\n",
      "['A', '##ac', '##PS']\n",
      "['te', '##rt']\n",
      "['C', '##m', '##ES']\n",
      "['b', '##oro', '##n']\n",
      "['t', '##ri', '##f', '##lu', '##oro', '##ace', '##tic']\n",
      "['trim', '##eth', '##yl', '##si', '##ly', '##l']\n",
      "['ace', '##ton', '##it', '##ril', '##e']\n",
      "['Me', '##3', '##S', '##i', '##I']\n",
      "['CH', '##3', '##C', '##N']\n",
      "['met', '##hyl']\n",
      "['na', '##lo', '##xon', '##e']\n",
      "['a', '##cy', '##c', '##lic']\n",
      "['T', '##rp']\n",
      "['m', '##oi', '##eti', '##es']\n",
      "['22']\n",
      "['25']\n",
      "['c', '##y', '##c', '##lop', '##rop', '##yl']\n",
      "['s', '##tero', '##ls']\n",
      "['D']\n",
      "['L']\n",
      "['g', '##ly', '##cer', '##ol']\n",
      "['t', '##rig', '##ly', '##cer', '##ides']\n",
      "['glucose']\n",
      "['g', '##ly', '##co', '##gen']\n",
      "['d']\n",
      "['l']\n",
      "['g', '##ly', '##cer', '##ald', '##eh', '##yd', '##e']\n",
      "['carbon']\n",
      "['t', '##rig', '##ly', '##cer', '##ide']\n",
      "['car', '##min', '##omy', '##cin', '##one']\n",
      "['car', '##min', '##omi', '##cin']\n",
      "['B', '##r', '##2']\n",
      "['ace', '##ty', '##l']\n",
      "['14']\n",
      "['potassium']\n",
      "['ace', '##tic']\n",
      "['p', '##yr', '##id', '##ine']\n",
      "['he', '##xa']\n",
      "['a', '##gly', '##con', '##es']\n",
      "['car', '##min', '##omy', '##cin']\n",
      "['p', '##hen', '##ox', '##y', '##ben', '##zam', '##ine']\n",
      "['P', '##O', '##B']\n",
      "['p', '##raz', '##os', '##in']\n",
      "['pen', '##to', '##bar', '##bit', '##al']\n",
      "['prop', '##rano', '##lo', '##l']\n",
      "['P', '##raz', '##os', '##in']\n",
      "['N', '##6']\n",
      "['N', '##M']\n",
      "['i', '##od', '##ine']\n",
      "['amino', '##ace', '##to']\n",
      "['substituted']\n",
      "['amid', '##e']\n",
      "['nitrogen']\n",
      "['ch', '##lor', '##oa', '##ce', '##ty', '##l']\n",
      "['diet', '##hyl', '##amine']\n",
      "['p', '##ht', '##hali', '##mi', '##des']\n",
      "['am', '##ines']\n",
      "['lid', '##oc', '##aine']\n",
      "['ha', '##lide']\n",
      "['ch', '##lor', '##of', '##orm']\n",
      "['p', '##oly', '##ade', '##ny', '##lic']\n",
      "['am', '##monia']\n",
      "['3']\n",
      "['t', '##ira', '##pa', '##zam', '##ine']\n",
      "['ch', '##lor', '##ine']\n",
      "['ben', '##zal', '##de', '##hy', '##de']\n",
      "['O']\n",
      "['me', '##rca', '##pt', '##oe', '##than', '##ol']\n",
      "['ni', '##tro', '##xy', '##l']\n",
      "['car', '##beth', '##ox', '##y']\n",
      "['am', '##ox', '##ici', '##llin']\n",
      "['c', '##lav', '##ula', '##nic']\n",
      "['CA']\n",
      "['t', '##ica', '##rc', '##ill', '##in']\n",
      "['su', '##l', '##ba', '##ct', '##am']\n",
      "['pipe', '##rac', '##ill', '##in']\n",
      "['ta', '##zo', '##ba', '##ct', '##am']\n",
      "['pen', '##ici', '##llin', '##s']\n",
      "['c', '##ep', '##hal', '##oth', '##in']\n",
      "['c', '##ef', '##ox', '##iti', '##n']\n",
      "['c', '##ef', '##ota', '##xi', '##me']\n",
      "['Piper', '##ac', '##ill', '##in']\n",
      "['di', '##acy', '##l', '##gly', '##cer', '##ol']\n",
      "['D', '##AG']\n",
      "['la', '##cton', '##es']\n",
      "['4', ',', '4']\n",
      "['did', '##eo', '##xy']\n",
      "['t', '##ri']\n",
      "['di']\n",
      "['th', '##re', '##o']\n",
      "['iron']\n",
      "['complexes']\n",
      "['h', '##ydro', '##xy', '##py', '##rid', '##ino', '##nes']\n",
      "['h', '##ydro', '##xy', '##py', '##rid', '##ino', '##ne']\n",
      "['De', '##fer', '##ip', '##rone']\n",
      "['1', ',', '2']\n"
     ]
    }
   ],
   "source": [
    "print(train_df)\n",
    "for word in list(train_df[train_df['labels']=='B']['words'].unique()):\n",
    "    print(tokenizer.tokenize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(901,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_ent = pd.DataFrame(train_df[train_df['labels']!='O']['words'].unique())\n",
    "dev_ent = pd.DataFrame(dev_df[dev_df['labels']!='O']['words'].unique())\n",
    "test_ent = test_df[test_df['labels']!='O']['words'].unique().shape\n",
    "same_dev = pd.merge(train_ent,dev_ent)\n",
    "# print(train_ent.values[:,0].tolist())\n",
    "print(test_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-(650, 1), dev-(86, 1), common-(41, 1)\n",
      "O    14943\n",
      "I     2789\n",
      "B      852\n",
      "Name: labels, dtype: int64\n",
      "-                      896\n",
      ")                      213\n",
      "(                      210\n",
      "[                       88\n",
      "]                       88\n",
      "                      ... \n",
      "1,2,4                    1\n",
      "iodide                   1\n",
      "I                        1\n",
      "dibutyrate               1\n",
      "bisindolilmaleimide      1\n",
      "Name: words, Length: 650, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'train-{train_ent.shape}, dev-{dev_ent.shape}, common-{same_dev.shape}')\n",
    "# print(same_dev)\n",
    "print(train_df['labels'].value_counts())\n",
    "print(train_df[train_df['labels']!='O']['words'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_data(ENT, DATASET):\n",
    "    import os\n",
    "    data_dir = os.path.join(data_path, ENT, DATASET)\n",
    "    params[\"entity_type\"] = ENT\n",
    "    params[\"dataset\"] = DATASET\n",
    "\n",
    "    params[\"LOWER_CASE\"] = False\n",
    "    params[\"LOAD_BEST_MODEL\"] = True\n",
    "    params[\"MAX_LEN\"] = 256\n",
    "    params[\"BATCH_SIZE\"] = 32\n",
    "    params[\"EPOCH_TOP\"] = 100\n",
    "    params[\"EPOCH_END2END\"] = 100\n",
    "\n",
    "    params[\"EXP_NAME\"] = \"test\"\n",
    "    params[\"WORKING_DIR\"] = \"sbksvol/nikhil/\" + params[\"EXP_NAME\"] + \"/\"\n",
    "    params[\"CACHE_DIR\"] = params[\"WORKING_DIR\"] + \"NER_out_test/\"\n",
    "\n",
    "    # Where model checkpoints are stored.\n",
    "    params[\"OUTPUT_DIR\"] = params[\"WORKING_DIR\"] + \"model_output_test/\"\n",
    "    params[\"TRAIN_ARGS_FILE\"] = params[\"WORKING_DIR\"] + \"train_args_test.json\"\n",
    "    params[\"DATA_PATH\"] = data_path\n",
    "\n",
    "    train_df, test_df, dev_df, labels, num_labels, label_map, data_dir, wt = prepare_data()\n",
    "\n",
    "    data_args, model_args, config, tokenizer = prepare_config_and_tokenizer(\n",
    "        data_dir, labels, num_labels, label_map)\n",
    "\n",
    "    # ## Create Dataset Objects\n",
    "    # print(Split.dev)\n",
    "\n",
    "#     train_dataset = NerDataset(\n",
    "#         data_dir=data_args['data_dir'],\n",
    "#         tokenizer=tokenizer,\n",
    "#         labels=labels,\n",
    "#         model_type=config.model_type,\n",
    "#         max_seq_length=data_args['max_seq_length'],\n",
    "#         overwrite_cache=data_args['overwrite_cache'],  # True\n",
    "#         mode=Split.train, data_size=100)\n",
    "\n",
    "#     val_dataset = NerDataset(\n",
    "#         data_dir=data_args['data_dir'],\n",
    "#         tokenizer=tokenizer,\n",
    "#         labels=labels,\n",
    "#         model_type=config.model_type,\n",
    "#         max_seq_length=data_args['max_seq_length'],\n",
    "#         overwrite_cache=data_args['overwrite_cache'],  # True\n",
    "#         mode=Split.dev, data_size=100)\n",
    "\n",
    "#     test_dataset = NerDataset(\n",
    "#         data_dir=data_args['data_dir'],\n",
    "#         tokenizer=tokenizer,\n",
    "#         labels=labels,\n",
    "#         model_type=config.model_type,\n",
    "#         max_seq_length=data_args['max_seq_length'],\n",
    "#         overwrite_cache=data_args['overwrite_cache'],  # True\n",
    "#         mode=Split.test, data_size=100)\n",
    "#     dataset = [train_dataset,val_dataset,test_dataset]\n",
    "    dataset =[]\n",
    "    data_df = [train_df, dev_df, test_df]\n",
    "    return dataset,tokenizer, data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ent(ENT, DATASET):\n",
    "    dataset,tokenizer,_ = get_data(ENT, DATASET)\n",
    "    ents = {0:[], 1:[], 2:[]}\n",
    "    toks = {0:[], 1:[], 2:[]}\n",
    "    for i in range(3):\n",
    "        for j in range(dataset[i].__len__()):\n",
    "            sent = dataset[i].__getitem__(j);\n",
    "            curr = []\n",
    "            flag=0\n",
    "            l_ids = np.array(sent.label_ids)\n",
    "            i_ids = np.array(sent.input_ids)\n",
    "            tok0 = i_ids[l_ids==0]\n",
    "            tok1 = i_ids[l_ids==1]\n",
    "            tok2 = i_ids[l_ids==2]\n",
    "            toks[0].extend(tokenizer.convert_ids_to_tokens(tok0))\n",
    "            toks[1].extend(tokenizer.convert_ids_to_tokens(tok1))\n",
    "            toks[2].extend(tokenizer.convert_ids_to_tokens(tok2))\n",
    "            for k in range(len(sent.label_ids)):\n",
    "                if sent.attention_mask[k]==0:\n",
    "                    break\n",
    "                if sent.label_ids[k] == 2 or (sent.label_ids[k] == -100 and flag==0):\n",
    "                    if len(curr)>0:\n",
    "                        ents[i].append(tokenizer.convert_ids_to_tokens(curr))\n",
    "                        curr=[]\n",
    "                    flag=0\n",
    "                else:\n",
    "                    curr.append(sent.input_ids[k])\n",
    "                    if sent.label_ids[k] == 0:\n",
    "                        flag=1\n",
    "    return ents, toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unk(words):\n",
    "    res = {}\n",
    "    for word in words:\n",
    "        tok = tokenizer.tokenize(word)\n",
    "        if 'UNK' in tok:\n",
    "            res[word] = tok\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_sents, num_dev_sents, num_test_sents =  1418 173 589\n",
      "First 10 words in test data:\n",
      "   sentence_id            words labels\n",
      "0            0  BackgroundUsing      O\n",
      "1            0       antibodies      O\n",
      "2            0               to      O\n",
      "3            0         specific      O\n",
      "4            0          protein      O\n",
      "5            0         antigens      O\n",
      "6            0               is      O\n",
      "7            0              the      O\n",
      "8            0           method      O\n",
      "9            0               of      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [6.35873016e+01 4.45111111e+03 3.35115149e-01]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  121 22 61\n",
      "First 10 words in test data:\n",
      "   sentence_id       words labels\n",
      "0            0          By      O\n",
      "1            0    Northern      O\n",
      "2            0        blot      O\n",
      "3            0    analysis      O\n",
      "4            0           ,      O\n",
      "5            0         the      O\n",
      "6            0  expression      O\n",
      "7            0          of      O\n",
      "8            0          IL      O\n",
      "9            0           -      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [ 6.85612536 43.36036036  0.35322178]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  7151 1280 3381\n",
      "First 10 words in test data:\n",
      "   sentence_id         words labels\n",
      "0            0        Glioma      O\n",
      "1            0         tumor      O\n",
      "2            0          stem      O\n",
      "3            0             -      O\n",
      "4            0          like      O\n",
      "5            0         cells      O\n",
      "6            0       promote      O\n",
      "7            0         tumor      O\n",
      "8            0  angiogenesis      O\n",
      "9            0           and      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [191.98882114 199.27953586   0.33447352]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  11167 1834 5548\n",
      "First 10 words in test data:\n",
      "   sentence_id              words labels\n",
      "0            0         Reactivity      O\n",
      "1            0                 of      O\n",
      "2            0        lymphocytes      B\n",
      "3            0                 to      O\n",
      "4            0                  a      O\n",
      "5            0       progesterone      O\n",
      "6            0  receptor-specific      O\n",
      "7            0         monoclonal      O\n",
      "8            0           antibody      O\n",
      "9            0                  .      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [43.244892   22.06194624  0.34111659]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  8399 1414 4424\n",
      "First 10 words in test data:\n",
      "   sentence_id         words labels\n",
      "0            0       Torsade      O\n",
      "1            0            de      O\n",
      "2            0       pointes      O\n",
      "3            0   ventricular      O\n",
      "4            0   tachycardia      O\n",
      "5            0        during      O\n",
      "6            0           low      O\n",
      "7            0          dose      O\n",
      "8            0  intermittent      O\n",
      "9            0    dobutamine      B\n",
      "unique labels: ['B', 'I', 'O'] with weights [ 6.95170341 58.7734925   0.35221984]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  29633 4870 14882\n",
      "First 10 words in test data:\n",
      "   sentence_id       words labels\n",
      "0            0  Production      O\n",
      "1            0          of      O\n",
      "2            0     enzymes      O\n",
      "3            0          in      O\n",
      "4            0       seeds      O\n",
      "5            0         and      O\n",
      "6            0       their      O\n",
      "7            0         use      O\n",
      "8            1           A      O\n",
      "9            1      method      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [ 7.45846164 15.77231222  0.35682148]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  7951 1425 3659\n",
      "First 10 words in test data:\n",
      "   sentence_id        words labels\n",
      "0            0       [0001]      O\n",
      "1            0   OLIGOMERIC      B\n",
      "2            0  AMIDOAMINES      I\n",
      "3            0           OR      O\n",
      "4            0   AMIDOQUATS      B\n",
      "5            0          FOR      O\n",
      "6            0       FABRIC      O\n",
      "7            0           OR      O\n",
      "8            0         HAIR      O\n",
      "9            1    TREATMENT      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [ 6.87986483 15.65940714  0.35832162]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  53363 8823 26751\n",
      "First 10 words in test data:\n",
      "   sentence_id            words labels\n",
      "0            0             Fish      O\n",
      "1            0         contains      O\n",
      "2            0             both      O\n",
      "3            0       beneficial      O\n",
      "4            0       substances      O\n",
      "5            0              e.g      O\n",
      "6            0                .      O\n",
      "7            1  docosahexaenoic      B\n",
      "8            1            acids      I\n",
      "9            1              but      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [ 9.23341676 30.78675862  0.3497462 ]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  592 102 280\n",
      "First 10 words in test data:\n",
      "   sentence_id            words labels\n",
      "0            0              The      O\n",
      "1            0        synthesis      O\n",
      "2            0              and      O\n",
      "3            0  pharmacological      O\n",
      "4            0          profile      O\n",
      "5            0               of      O\n",
      "6            0                a      O\n",
      "7            0           series      O\n",
      "8            0               of      O\n",
      "9            0  neuroprotective      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [7.27073552 2.22110673 0.41455308]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  12574 2519 5038\n",
      "First 10 words in test data:\n",
      "   sentence_id        words labels\n",
      "0            0     Physical      O\n",
      "1            0      mapping      O\n",
      "2            0          220      O\n",
      "3            0           kb      O\n",
      "4            0  centromeric      O\n",
      "5            0           of      O\n",
      "6            0          the      O\n",
      "7            0        human      B\n",
      "8            0          MHC      I\n",
      "9            0          and      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [7.79550789 5.35958801 0.37242013]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  661 111 331\n",
      "First 10 words in test data:\n",
      "   sentence_id            words labels\n",
      "0            0  Transcriptional      O\n",
      "1            0       expression      O\n",
      "2            0               of      O\n",
      "3            0       TNFR1(p55)      B\n",
      "4            0                ,      O\n",
      "5            0               as      O\n",
      "6            0             well      O\n",
      "7            0               as      O\n",
      "8            0             that      O\n",
      "9            0               of      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [2.47228493 6.93910595 0.40792931]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  1253 422 504\n",
      "First 10 words in test data:\n",
      "   sentence_id            words labels\n",
      "0            0  BackgroundUsing      O\n",
      "1            0       antibodies      O\n",
      "2            0               to      O\n",
      "3            0         specific      O\n",
      "4            0          protein      O\n",
      "5            0         antigens      O\n",
      "6            0               is      O\n",
      "7            0              the      O\n",
      "8            0           method      O\n",
      "9            0               of      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [13.96698113 60.12182741  0.34343376]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  3286 555 1632\n",
      "First 10 words in test data:\n",
      "   sentence_id      words labels\n",
      "0            0  Selection      O\n",
      "1            0         of      O\n",
      "2            0      cDNAs      O\n",
      "3            0   encoding      O\n",
      "4            0   putative      O\n",
      "5            0       type      O\n",
      "6            0         II      O\n",
      "7            0   membrane      O\n",
      "8            0   proteins      O\n",
      "9            0         on      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [ 7.54568574 33.00675407  0.35246302]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  22206 3565 10448\n",
      "First 10 words in test data:\n",
      "   sentence_id      words labels\n",
      "0            0   Skeletal      B\n",
      "1            0     muscle      I\n",
      "2            0       type      I\n",
      "3            0  ryanodine      I\n",
      "4            0   receptor      I\n",
      "5            0         is      O\n",
      "6            0   involved      O\n",
      "7            0         in      O\n",
      "8            0    calcium      O\n",
      "9            0  signaling      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [ 5.82540914 10.78940483  0.36554321]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  306 46 137\n",
      "First 10 words in test data:\n",
      "   sentence_id             words labels\n",
      "0            0  Phosphoinositide      O\n",
      "1            0        hydrolysis      O\n",
      "2            0                 ,      O\n",
      "3            0         resulting      O\n",
      "4            0                in      O\n",
      "5            0          inositol      B\n",
      "6            0     trisphosphate      I\n",
      "7            0                 (      O\n",
      "8            0               IP3      O\n",
      "9            0                 )      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [ 4.58003766 19.3015873   0.36632023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_sents, num_dev_sents, num_test_sents =  11167 1834 5548\n",
      "First 10 words in test data:\n",
      "   sentence_id              words labels\n",
      "0            0         Reactivity      O\n",
      "1            0                 of      O\n",
      "2            0        lymphocytes      O\n",
      "3            0                 to      O\n",
      "4            0                  a      O\n",
      "5            0       progesterone      B\n",
      "6            0  receptor-specific      I\n",
      "7            0         monoclonal      I\n",
      "8            0           antibody      I\n",
      "9            0                  .      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [5.34969037 6.517409   0.37599101]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  576 86 293\n",
      "First 10 words in test data:\n",
      "   sentence_id             words labels\n",
      "0            0         Molecular      O\n",
      "1            0           cloning      O\n",
      "2            0               and      O\n",
      "3            0       biochemical      O\n",
      "4            0  characterization      O\n",
      "5            0                of      O\n",
      "6            0                 a      O\n",
      "7            0         truncated      O\n",
      "8            0                 ,      O\n",
      "9            0          secreted      B\n",
      "unique labels: ['B', 'I', 'O'] with weights [ 3.79735807 17.38867439  0.37325267]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  1486 205 768\n",
      "First 10 words in test data:\n",
      "   sentence_id      words labels\n",
      "0            0       This      O\n",
      "1            0        can      O\n",
      "2            0  translate      O\n",
      "3            0       into      O\n",
      "4            0     global      O\n",
      "5            0    effects      O\n",
      "6            0         on      O\n",
      "7            0   cellular      O\n",
      "8            0     health      O\n",
      "9            0        and      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [22.4805915  80.54304636  0.33977762]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  690 105 348\n",
      "First 10 words in test data:\n",
      "   sentence_id          words labels\n",
      "0            0     Uncoupling      B\n",
      "1            0      protein-2      I\n",
      "2            0  polymorphisms      O\n",
      "3            0             in      O\n",
      "4            0           type      O\n",
      "5            0              2      O\n",
      "6            0       diabetes      O\n",
      "7            0              ,      O\n",
      "8            0        obesity      O\n",
      "9            0              ,      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [10.72772586 29.89236111  0.34802822]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  5585 800 1906\n",
      "First 10 words in test data:\n",
      "   sentence_id   words labels\n",
      "0            0      **      O\n",
      "1            0  IGNORE      O\n",
      "2            0    LINE      O\n",
      "3            0      **      O\n",
      "4            1      **      O\n",
      "5            1  IGNORE      O\n",
      "6            1    LINE      O\n",
      "7            1      **      O\n",
      "8            2      **      O\n",
      "9            2  IGNORE      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [ 12.58347017 166.5942029    0.34310872]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  12729 2345 6480\n",
      "First 10 words in test data:\n",
      "   sentence_id      words labels\n",
      "0            0    Scp160p      O\n",
      "1            0          ,      O\n",
      "2            0          a      O\n",
      "3            0   multiple      O\n",
      "4            0  KH-domain      O\n",
      "5            0    protein      O\n",
      "6            0          ,      O\n",
      "7            0         is      O\n",
      "8            0          a      O\n",
      "9            0  component      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [ 54.07836188 109.77777778   0.33642859]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  576 86 293\n",
      "First 10 words in test data:\n",
      "   sentence_id             words labels\n",
      "0            0         Molecular      O\n",
      "1            0           cloning      O\n",
      "2            0               and      O\n",
      "3            0       biochemical      O\n",
      "4            0  characterization      O\n",
      "5            0                of      O\n",
      "6            0                 a      O\n",
      "7            0         truncated      O\n",
      "8            0                 ,      O\n",
      "9            0          secreted      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [ 28.68577495 180.14666667   0.33788481]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  1486 205 768\n",
      "First 10 words in test data:\n",
      "   sentence_id      words labels\n",
      "0            0       This      O\n",
      "1            0        can      O\n",
      "2            0  translate      O\n",
      "3            0       into      O\n",
      "4            0     global      O\n",
      "5            0    effects      O\n",
      "6            0         on      O\n",
      "7            0   cellular      O\n",
      "8            0     health      O\n",
      "9            0        and      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [3.15896104e+01 1.35133333e+03 3.36972182e-01]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  4734 763 2363\n",
      "First 10 words in test data:\n",
      "   sentence_id             words labels\n",
      "0            0         Isolation      O\n",
      "1            0               and      O\n",
      "2            0  characterization      O\n",
      "3            0                of      O\n",
      "4            0               MAT      O\n",
      "5            0             genes      O\n",
      "6            0                in      O\n",
      "7            0               the      O\n",
      "8            0         symbiotic      O\n",
      "9            0        ascomycete      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [17.8801036  19.30980586  0.34574742]\n",
      "num_train_sents, num_dev_sents, num_test_sents =  5585 800 1906\n",
      "First 10 words in test data:\n",
      "   sentence_id   words labels\n",
      "0            0      **      O\n",
      "1            0  IGNORE      O\n",
      "2            0    LINE      O\n",
      "3            0      **      O\n",
      "4            1      **      O\n",
      "5            1  IGNORE      O\n",
      "6            1    LINE      O\n",
      "7            1      **      O\n",
      "8            2      **      O\n",
      "9            2  IGNORE      O\n",
      "unique labels: ['B', 'O'] with weights [6.92469880e+02 5.00361287e-01]\n",
      "{'Cellline': {'cellfinder': {'train': {}, 'dev': {}, 'test': {}}, 'cll': {'train': {}, 'dev': {}, 'test': {}}, 'gellus': {'train': {}, 'dev': {}, 'test': {}}, 'jnlpba': {'train': {}, 'dev': {}, 'test': {}}}, 'Chemicals': {'cdr': {'train': {}, 'dev': {}, 'test': {}}, 'cemp': {'train': {}, 'dev': {}, 'test': {}}, 'chebi': {'train': {}, 'dev': {}, 'test': {}}, 'chemdner': {'train': {}, 'dev': {}, 'test': {}}, 'scai_chemicals': {'train': {}, 'dev': {}, 'test': {}}}, 'Gene': {'BC2GM': {'train': {}, 'dev': {}, 'test': {}}, 'bioinfer': {'train': {}, 'dev': {}, 'test': {}}, 'cellfinder': {'train': {}, 'dev': {}, 'test': {}}, 'deca': {'train': {}, 'dev': {}, 'test': {}}, 'fsu': {'train': {}, 'dev': {}, 'test': {}}, 'iepa': {'train': {}, 'dev': {}, 'test': {}}, 'jnlpba': {'train': {}, 'dev': {}, 'test': {}}, 'loctext': {'train': {}, 'dev': {}, 'test': {}}, 'miRNA': {'train': {}, 'dev': {}, 'test': {}}, 'osiris': {'train': {}, 'dev': {}, 'test': {}}, 'variome': {'train': {}, 'dev': {}, 'test': {}}}, 'Species': {'linneaus': {'train': {}, 'dev': {}, 'test': {}}, 'loctext': {'train': {}, 'dev': {}, 'test': {}}, 'miRNA': {'train': {}, 'dev': {}, 'test': {}}, 's800': {'train': {}, 'dev': {}, 'test': {}}, 'variome': {'train': {}, 'dev': {}, 'test': {}}}}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {}\n",
    "DATASET = {\n",
    "    \"Cellline\":[\"cellfinder\",\"cll\",\"gellus\",\"jnlpba\"],\n",
    "    \"Chemicals\":[\"cdr\",\"cemp\",\"chebi\",\"chemdner\",\"scai_chemicals\"],\n",
    "    \"Gene\":[\"BC2GM\",\"bioinfer\",\"cellfinder\",\"deca\",\"fsu\",\"iepa\",\"jnlpba\",\"loctext\",\"miRNA\",\"osiris\",\"variome\"],\n",
    "    \"Species\":[\"linneaus\",\"loctext\",\"miRNA\",\"s800\",\"variome\"]}\n",
    "for key, item in DATASET.items():\n",
    "    data[key]={}\n",
    "    for val in item:\n",
    "        _,_,data_df = get_data(key,val)\n",
    "        [train_df,dev_df,test_df] = data_df\n",
    "        data[key][val]={}\n",
    "        for catg in ['B']:\n",
    "#             train_ent = train_df[train_df['labels']==catg]['words'].unique().shape[0]\n",
    "#             dev_ent = dev_df[dev_df['labels']==catg]['words'].unique().shape[0]\n",
    "#             test_ent = test_df[test_df['labels']==catg]['words'].unique().shape[0]\n",
    "#             data[key][val].update({f\"{catg}\":int(train_ent)+int(dev_ent)+int(test_ent)})\n",
    "            train_ent = pd.DataFrame(train_df[train_df['labels']!='O']['words'].unique()).values[:,0].tolist()\n",
    "            train_ent = get_unk(train_ent)\n",
    "            dev_ent = pd.DataFrame(dev_df[dev_df['labels']!='O']['words'].unique()).values[:,0].tolist()\n",
    "            dev_ent = get_unk(dev_ent)\n",
    "            test_ent = pd.DataFrame(test_df[test_df['labels']!='O']['words'].unique()).values[:,0].tolist()\n",
    "            test_ent = get_unk(test_ent)\n",
    "            data[key][val] = {\"train\":train_ent,\"dev\":dev_ent,\"test\":test_ent}\n",
    "import json\n",
    "with open('data_unk.json','w') as f:\n",
    "    json.dump(data,f)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(ents, toks):\n",
    "    ans = {}\n",
    "    ans['all']={\n",
    "        0:pd.Series([\"\".join(ent) for ent in ents[0]],name='A'),\n",
    "        1:pd.Series([\"\".join(ent) for ent in ents[1]],name='A'),\n",
    "        2:pd.Series([\"\".join(ent) for ent in ents[2]],name='A')\n",
    "    }\n",
    "    ans['dist']={\n",
    "        0:pd.Series(toks[0],name='A'),\n",
    "        1:pd.Series(toks[1],name='A'),\n",
    "        2:pd.Series(toks[2],name='A')\n",
    "    }\n",
    "    unique_2 = pd.Series(ans['dist'][2].unique(),name='A')\n",
    "    same_0 = pd.merge(ans['dist'][0],unique_2,on='A',how='inner')['A'].value_counts()\n",
    "    same_1 = pd.merge(ans['dist'][1],unique_2,how='inner')['A'].value_counts()\n",
    "    ans['same_first']={\n",
    "        '0':[same_0/ans['dist'][0].shape[0]*100],\n",
    "        '1':[same_1/ans['dist'][1].shape[0]*100]\n",
    "    }\n",
    "    \n",
    "    unique_1 = pd.Series(ans['all'][1].unique(),name='A')\n",
    "    unique_2 = pd.Series(ans['all'][2].unique(),name='A')\n",
    "    same_01 = pd.merge(ans['all'][0],unique_1, how='inner')['A'].value_counts()\n",
    "    same_02 = pd.merge(ans['all'][0],unique_2,how='inner')['A'].value_counts()\n",
    "    ans['same_all']={\n",
    "        '01':[same_01/ans['all'][0].shape[0]*100],\n",
    "        '02':[same_02/ans['all'][0].shape[0]*100]\n",
    "    }\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_sents, num_dev_sents, num_test_sents =  1418 173 589\n",
      "First 10 words in test data:\n",
      "   sentence_id            words labels\n",
      "0            0  BackgroundUsing      O\n",
      "1            0       antibodies      O\n",
      "2            0               to      O\n",
      "3            0         specific      O\n",
      "4            0          protein      O\n",
      "5            0         antigens      O\n",
      "6            0               is      O\n",
      "7            0              the      O\n",
      "8            0           method      O\n",
      "9            0               of      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [6.35873016e+01 4.45111111e+03 3.35115149e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1112 17:00:19.597760 139703026820928 utils_ner.py:113] Creating features from dataset file at /sbksvol/nikhil/NER_data/Cellline/cellfinder\n",
      "I1112 17:00:19.637270 139703026820928 utils_ner.py:322] Writing example 0 of 1417\n",
      "I1112 17:00:19.646562 139703026820928 utils_ner.py:407] *** Example ***\n",
      "I1112 17:00:19.650306 139703026820928 utils_ner.py:408] guid: train-1\n",
      "I1112 17:00:19.655494 139703026820928 utils_ner.py:409] tokens: [CLS] Background ##H ##uman em ##b ##ryo ##nic stem cells provide access to the earliest stages of human development and may serve as a source of specialized cells for re ##gene ##rative medicine . [SEP]\n",
      "I1112 17:00:19.659187 139703026820928 utils_ner.py:410] input_ids: 101 24570 3048 19147 9712 1830 26503 7770 8175 3652 2194 2469 1106 1103 5041 5251 1104 1769 1718 1105 1336 2867 1112 170 2674 1104 7623 3652 1111 1231 27054 15306 5182 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:19.661957 139703026820928 utils_ner.py:411] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:19.666221 139703026820928 utils_ner.py:412] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:19.667713 139703026820928 utils_ner.py:413] label_ids: -100 2 -100 -100 2 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I1112 17:00:19.675614 139703026820928 utils_ner.py:407] *** Example ***\n",
      "I1112 17:00:19.677110 139703026820928 utils_ner.py:408] guid: train-2\n",
      "I1112 17:00:19.678019 139703026820928 utils_ner.py:409] tokens: [CLS] Thus , it becomes crucial to develop protocols for the directed differentiation of em ##b ##ryo ##nic stem cells into tissue - restricted precursor ##s . Method ##s and Finding ##s Here , we present culture conditions for the der ##ivation of unlimited numbers of pure me ##sen ##chy ##mal precursor ##s from human em ##b ##ryo ##nic stem cells and demonstrate multi ##line ##age differentiation into fat , cart ##ila ##ge , bone , and skeletal muscle cells . Con ##c ##lusion ##O ##ur findings will help to el ##uc ##ida ##te the mechanism of me ##so ##der ##m specification during em ##b ##ryo ##nic stem cell differentiation and provide a platform to efficiently generate specialized human me ##sen ##chy ##mal cell types for future clinical applications . Lo ##ren ##z St ##ude ##r and colleagues describe the use of em ##b ##ryo ##nic stem cells to derive me ##sen ##chy ##mal precursor ##s and then fat , cart ##ila ##ge , bone , and skeletal muscle cells . [SEP]\n",
      "I1112 17:00:19.678804 139703026820928 utils_ner.py:410] input_ids: 101 4516 117 1122 3316 10268 1106 3689 19755 1111 1103 2002 23510 1104 9712 1830 26503 7770 8175 3652 1154 7918 118 7458 15985 1116 119 20569 1116 1105 18036 1116 3446 117 1195 1675 2754 2975 1111 1103 4167 16617 1104 22921 2849 1104 5805 1143 3792 8992 7435 15985 1116 1121 1769 9712 1830 26503 7770 8175 3652 1105 10541 4321 2568 2553 23510 1154 7930 117 12411 8009 2176 117 6028 117 1105 23400 6484 3652 119 16752 1665 17855 2346 2149 9505 1209 1494 1106 8468 21977 6859 1566 1103 6978 1104 1143 7301 2692 1306 14911 1219 9712 1830 26503 7770 8175 2765 23510 1105 2194 170 3482 1106 19723 9509 7623 1769 1143 3792 8992 7435 2765 3322 1111 2174 7300 4683 119 10605 5123 1584 1457 10308 1197 1105 8304 5594 1103 1329 1104 9712 1830 26503 7770 8175 3652 1106 20292 1143 3792 8992 7435 15985 1116 1105 1173 7930 117 12411 8009 2176 117 6028 117 1105 23400 6484 3652 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:19.679286 139703026820928 utils_ner.py:411] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:19.680291 139703026820928 utils_ner.py:412] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:19.681987 139703026820928 utils_ner.py:413] label_ids: -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 2 2 2 2 -100 -100 2 -100 2 -100 -100 2 2 -100 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 -100 -100 -100 2 -100 2 2 2 -100 -100 -100 2 2 2 2 2 -100 -100 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 2 2 2 2 2 -100 -100 -100 2 2 2 2 -100 -100 -100 2 2 2 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 2 2 2 2 2 2 -100 -100 -100 -100 2 -100 -100 2 2 2 2 2 2 2 -100 -100 -100 2 2 2 2 2 -100 -100 -100 2 -100 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1112 17:00:19.684475 139703026820928 utils_ner.py:407] *** Example ***\n",
      "I1112 17:00:19.685402 139703026820928 utils_ner.py:408] guid: train-3\n",
      "I1112 17:00:19.686453 139703026820928 utils_ner.py:409] tokens: [CLS] Em ##b ##ryo ##nic stem ( E ##S ) cells are p ##lu ##rip ##ote ##nt cells derived from the inner cell mass of the blast ##oc ##ys ##t that can be maintained in culture for an extended period of time without losing differentiation potential . [SEP]\n",
      "I1112 17:00:19.686974 139703026820928 utils_ner.py:410] input_ids: 101 18653 1830 26503 7770 8175 113 142 1708 114 3652 1132 185 7535 16669 11860 2227 3652 4408 1121 1103 5047 2765 3367 1104 1103 9232 13335 6834 1204 1115 1169 1129 4441 1107 2754 1111 1126 2925 1669 1104 1159 1443 3196 23510 3209 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:19.687666 139703026820928 utils_ner.py:411] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:19.688214 139703026820928 utils_ner.py:412] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:19.688778 139703026820928 utils_ner.py:413] label_ids: -100 2 -100 -100 -100 2 2 2 -100 2 2 2 2 -100 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I1112 17:00:19.690974 139703026820928 utils_ner.py:407] *** Example ***\n",
      "I1112 17:00:19.691714 139703026820928 utils_ner.py:408] guid: train-4\n",
      "I1112 17:00:19.692219 139703026820928 utils_ner.py:409] tokens: [CLS] The successful isolation of human E ##S cells ( h ##ES ##Cs ) has raised the hope that these cells may provide a universal tissue source to treat many human diseases . [SEP]\n",
      "I1112 17:00:19.692742 139703026820928 utils_ner.py:410] input_ids: 101 1109 2265 13345 1104 1769 142 1708 3652 113 177 9919 18363 114 1144 2120 1103 2810 1115 1292 3652 1336 2194 170 8462 7918 2674 1106 7299 1242 1769 8131 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:19.693580 139703026820928 utils_ner.py:411] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:19.694272 139703026820928 utils_ner.py:412] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:19.694630 139703026820928 utils_ner.py:413] label_ids: -100 2 2 2 2 2 2 -100 2 2 2 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I1112 17:00:19.695662 139703026820928 utils_ner.py:407] *** Example ***\n",
      "I1112 17:00:19.695922 139703026820928 utils_ner.py:408] guid: train-5\n",
      "I1112 17:00:19.696168 139703026820928 utils_ner.py:409] tokens: [CLS] However , directed differentiation of h ##ES ##Cs into specific tissue types poses a formidable challenge . [SEP]\n",
      "I1112 17:00:19.696444 139703026820928 utils_ner.py:410] input_ids: 101 1438 117 2002 23510 1104 177 9919 18363 1154 2747 7918 3322 25366 170 20466 4506 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:19.696717 139703026820928 utils_ner.py:411] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1112 17:00:19.696991 139703026820928 utils_ner.py:412] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:19.697265 139703026820928 utils_ner.py:413] label_ids: -100 2 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I1112 17:00:22.011307 139703026820928 utils_ner.py:139] Saving features into cached file /sbksvol/nikhil/NER_data/Cellline/cellfinder/cached_train_BertTokenizer_256\n",
      "I1112 17:00:23.175516 139703026820928 utils_ner.py:113] Creating features from dataset file at /sbksvol/nikhil/NER_data/Cellline/cellfinder\n",
      "I1112 17:00:23.187297 139703026820928 utils_ner.py:322] Writing example 0 of 172\n",
      "I1112 17:00:23.190563 139703026820928 utils_ner.py:407] *** Example ***\n",
      "I1112 17:00:23.191483 139703026820928 utils_ner.py:408] guid: dev-1\n",
      "I1112 17:00:23.192414 139703026820928 utils_ner.py:409] tokens: [CLS] Background ##H ##uman stem cells are viewed as a possible source of neurons for a cell - based therapy of ne ##uro ##de ##gene ##rative disorders , such as Parkinson ' s disease . [SEP]\n",
      "I1112 17:00:23.192862 139703026820928 utils_ner.py:410] input_ids: 101 24570 3048 19147 8175 3652 1132 6497 1112 170 1936 2674 1104 16993 1111 170 2765 118 1359 7606 1104 24928 11955 2007 27054 15306 11759 117 1216 1112 22195 112 188 3653 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:23.193265 139703026820928 utils_ner.py:411] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:23.193639 139703026820928 utils_ner.py:412] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:23.193989 139703026820928 utils_ner.py:413] label_ids: -100 2 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 -100 -100 -100 -100 2 2 2 2 2 2 -100 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I1112 17:00:23.195903 139703026820928 utils_ner.py:407] *** Example ***\n",
      "I1112 17:00:23.196232 139703026820928 utils_ner.py:408] guid: dev-2\n",
      "I1112 17:00:23.196552 139703026820928 utils_ner.py:409] tokens: [CLS] Several protocols that generate different types of neurons from human stem cells ( h ##SC ##s ) have been developed . [SEP]\n",
      "I1112 17:00:23.196909 139703026820928 utils_ner.py:410] input_ids: 101 3728 19755 1115 9509 1472 3322 1104 16993 1121 1769 8175 3652 113 177 10844 1116 114 1138 1151 1872 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:23.197489 139703026820928 utils_ner.py:411] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:23.197880 139703026820928 utils_ner.py:412] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:23.198240 139703026820928 utils_ner.py:413] label_ids: -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1112 17:00:23.201188 139703026820928 utils_ner.py:407] *** Example ***\n",
      "I1112 17:00:23.201511 139703026820928 utils_ner.py:408] guid: dev-3\n",
      "I1112 17:00:23.201901 139703026820928 utils_ner.py:409] tokens: [CLS] Nevertheless , the cellular mechanisms that under ##lie the development of neurons in v ##it ##ro as they are subjected to the specific differentiation protocols are often poorly understood . Results We have designed a focused DNA ( o ##li ##gon ##uc ##leo ##tide - based ) large - scale micro ##arra ##y platform ( named \" \" \" \" N ##eur ##o ##S ##tem \" Chip \" \" \" ) and used it to study gene expression patterns in h ##SC ##s as they differentiate into neurons . [SEP]\n",
      "I1112 17:00:23.202252 139703026820928 utils_ner.py:410] input_ids: 101 8094 117 1103 14391 10748 1115 1223 7174 1103 1718 1104 16993 1107 191 2875 2180 1112 1152 1132 13927 1106 1103 2747 23510 19755 1132 1510 9874 4628 119 16005 1284 1138 2011 170 3378 5394 113 184 2646 7528 21977 26918 23767 118 1359 114 1415 118 3418 17599 25203 1183 3482 113 1417 107 107 107 107 151 8816 1186 1708 18408 107 20379 107 107 107 114 1105 1215 1122 1106 2025 5565 2838 6692 1107 177 10844 1116 1112 1152 23159 1154 16993 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:23.202618 139703026820928 utils_ner.py:411] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:23.202975 139703026820928 utils_ner.py:412] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:23.203305 139703026820928 utils_ner.py:413] label_ids: -100 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 2 2 -100 -100 2 -100 -100 2 2 2 2 -100 -100 -100 2 -100 -100 -100 -100 2 -100 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I1112 17:00:23.204681 139703026820928 utils_ner.py:407] *** Example ***\n",
      "I1112 17:00:23.204986 139703026820928 utils_ner.py:408] guid: dev-4\n",
      "I1112 17:00:23.205290 139703026820928 utils_ner.py:409] tokens: [CLS] We have selected genes that are relevant to cells ( i ) being stem cells , ( ii ) becoming neurons , and ( ii ##i ) being neurons . [SEP]\n",
      "I1112 17:00:23.205645 139703026820928 utils_ner.py:410] input_ids: 101 1284 1138 2700 9077 1115 1132 7503 1106 3652 113 178 114 1217 8175 3652 117 113 25550 114 2479 16993 117 1105 113 25550 1182 114 1217 16993 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:23.205988 139703026820928 utils_ner.py:411] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:23.206319 139703026820928 utils_ner.py:412] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:23.206676 139703026820928 utils_ner.py:413] label_ids: -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I1112 17:00:23.207918 139703026820928 utils_ner.py:407] *** Example ***\n",
      "I1112 17:00:23.208217 139703026820928 utils_ner.py:408] guid: dev-5\n",
      "I1112 17:00:23.208518 139703026820928 utils_ner.py:409] tokens: [CLS] The N ##eur ##o ##S ##tem Chip has over 1 , 300 pre - selected gene targets and multiple controls spotted in q ##uad ##rup ##lica ##tes ( ~ 46 , 000 spots total ) . [SEP]\n",
      "I1112 17:00:23.208860 139703026820928 utils_ner.py:410] input_ids: 101 1109 151 8816 1186 1708 18408 20379 1144 1166 122 117 3127 3073 118 2700 5565 7539 1105 2967 7451 6910 1107 186 18413 20910 9538 3052 113 199 3993 117 1288 7152 1703 114 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1112 17:00:23.209188 139703026820928 utils_ner.py:411] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:23.209526 139703026820928 utils_ner.py:412] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:23.209887 139703026820928 utils_ner.py:413] label_ids: -100 2 2 -100 -100 -100 -100 2 2 2 2 -100 -100 2 -100 -100 2 2 2 2 2 2 2 2 -100 -100 -100 -100 2 2 -100 -100 -100 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I1112 17:00:23.547697 139703026820928 utils_ner.py:139] Saving features into cached file /sbksvol/nikhil/NER_data/Cellline/cellfinder/cached_dev_BertTokenizer_256\n",
      "I1112 17:00:24.223775 139703026820928 utils_ner.py:113] Creating features from dataset file at /sbksvol/nikhil/NER_data/Cellline/cellfinder\n",
      "I1112 17:00:24.262006 139703026820928 utils_ner.py:322] Writing example 0 of 588\n",
      "I1112 17:00:24.265837 139703026820928 utils_ner.py:407] *** Example ***\n",
      "I1112 17:00:24.268226 139703026820928 utils_ner.py:408] guid: test-1\n",
      "I1112 17:00:24.269136 139703026820928 utils_ner.py:409] tokens: [CLS] Background ##U ##sing antibodies to specific protein anti ##gens is the method of choice to assign and identify cell lineage through simultaneous analysis of surface molecules and in ##tra ##cellular markers . [SEP]\n",
      "I1112 17:00:24.269706 139703026820928 utils_ner.py:410] input_ids: 101 24570 2591 4253 26491 1106 2747 4592 2848 21144 1110 1103 3442 1104 3026 1106 27430 1105 6183 2765 14209 1194 19648 3622 1104 2473 10799 1105 1107 4487 18091 18004 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:24.270057 139703026820928 utils_ner.py:411] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:24.270387 139703026820928 utils_ner.py:412] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:24.270701 139703026820928 utils_ner.py:413] label_ids: -100 2 -100 -100 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I1112 17:00:24.274957 139703026820928 utils_ner.py:407] *** Example ***\n",
      "I1112 17:00:24.276084 139703026820928 utils_ner.py:408] guid: test-2\n",
      "I1112 17:00:24.276507 139703026820928 utils_ner.py:409] tokens: [CLS] Em ##b ##ryo ##nic stem cell research can be benefited from using antibodies specific to transcription ##al factors / markers that contribute to the \" \" \" \" stem ##ness \" \" \" \" p ##hen ##otype or critical for cell lineage . Results In this report , we have developed and valid ##ated antibodies ( either mon ##oc ##lon ##al or p ##oly ##c ##lon ##al ) specific to human em ##b ##ryo ##nic stem cell anti ##gens and early differentiation transcription ##al factors / markers that are critical for cell differentiation into definite lineage . Con ##c ##lusion ##T ##hes ##e antibodies enable stem cell biologist ##s to convenient ##ly identify stem cell characteristics and to quantitative ##ly assess differentiation . [SEP]\n",
      "I1112 17:00:24.277011 139703026820928 utils_ner.py:410] input_ids: 101 18653 1830 26503 7770 8175 2765 1844 1169 1129 21495 1121 1606 26491 2747 1106 15416 1348 5320 120 18004 1115 8681 1106 1103 107 107 107 107 8175 1757 107 107 107 107 185 10436 27172 1137 3607 1111 2765 14209 119 16005 1130 1142 2592 117 1195 1138 1872 1105 9221 2913 26491 113 1719 19863 13335 4934 1348 1137 185 23415 1665 4934 1348 114 2747 1106 1769 9712 1830 26503 7770 8175 2765 2848 21144 1105 1346 23510 15416 1348 5320 120 18004 1115 1132 3607 1111 2765 23510 1154 16428 14209 119 16752 1665 17855 1942 16090 1162 26491 9396 8175 2765 24742 1116 1106 14785 1193 6183 8175 2765 5924 1105 1106 25220 1193 15187 23510 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1112 17:00:24.277471 139703026820928 utils_ner.py:411] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:24.277853 139703026820928 utils_ner.py:412] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:24.278239 139703026820928 utils_ner.py:413] label_ids: -100 2 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 -100 2 -100 -100 2 2 2 2 2 -100 -100 -100 2 -100 2 -100 -100 -100 2 -100 -100 2 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 -100 2 2 -100 2 -100 -100 -100 2 2 -100 -100 -100 -100 2 2 2 2 2 -100 -100 -100 2 2 2 -100 2 2 2 2 -100 2 -100 -100 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 2 2 2 2 2 -100 2 2 -100 2 2 2 2 2 2 2 -100 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I1112 17:00:24.279812 139703026820928 utils_ner.py:407] *** Example ***\n",
      "I1112 17:00:24.280164 139703026820928 utils_ner.py:408] guid: test-3\n",
      "I1112 17:00:24.280434 139703026820928 utils_ner.py:409] tokens: [CLS] Although the stem cell concept was introduced decades ago , to date , stem cells can only be defined functional ##ly , not m ##or ##phological ##ly or p ##hen ##otypic ##ally . [SEP]\n",
      "I1112 17:00:24.280774 139703026820928 utils_ner.py:410] input_ids: 101 1966 1103 8175 2765 3400 1108 2234 4397 2403 117 1106 2236 117 8175 3652 1169 1178 1129 3393 8458 1193 117 1136 182 1766 26920 1193 1137 185 10436 27202 2716 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:24.281278 139703026820928 utils_ner.py:411] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:24.281892 139703026820928 utils_ner.py:412] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:24.282420 139703026820928 utils_ner.py:413] label_ids: -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 2 2 2 -100 -100 -100 2 2 -100 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I1112 17:00:24.283289 139703026820928 utils_ner.py:407] *** Example ***\n",
      "I1112 17:00:24.283751 139703026820928 utils_ner.py:408] guid: test-4\n",
      "I1112 17:00:24.284322 139703026820928 utils_ner.py:409] tokens: [CLS] Two functions define stem cells . [SEP]\n",
      "I1112 17:00:24.284795 139703026820928 utils_ner.py:410] input_ids: 101 1960 4226 9410 8175 3652 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:24.285179 139703026820928 utils_ner.py:411] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:24.285589 139703026820928 utils_ner.py:412] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:24.286015 139703026820928 utils_ner.py:413] label_ids: -100 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1112 17:00:24.287282 139703026820928 utils_ner.py:407] *** Example ***\n",
      "I1112 17:00:24.287574 139703026820928 utils_ner.py:408] guid: test-5\n",
      "I1112 17:00:24.287854 139703026820928 utils_ner.py:409] tokens: [CLS] First ##ly , they are self - renew ##ing , thus able to prop ##aga ##te to generate additional stem cells . [SEP]\n",
      "I1112 17:00:24.288154 139703026820928 utils_ner.py:410] input_ids: 101 1752 1193 117 1152 1132 2191 118 23421 1158 117 2456 1682 1106 21146 15446 1566 1106 9509 2509 8175 3652 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:24.288454 139703026820928 utils_ner.py:411] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:24.288816 139703026820928 utils_ner.py:412] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1112 17:00:24.289122 139703026820928 utils_ner.py:413] label_ids: -100 2 -100 2 2 2 2 -100 -100 -100 2 2 2 2 2 -100 -100 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I1112 17:00:25.419844 139703026820928 utils_ner.py:139] Saving features into cached file /sbksvol/nikhil/NER_data/Cellline/cellfinder/cached_test_BertTokenizer_256\n"
     ]
    }
   ],
   "source": [
    "ENT = \"Chemicals\"\n",
    "DATASET = \"scai-chemical\"\n",
    "ans = get_ent(ENT,DATASET)\n",
    "res = process(ans[0],ans[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H          23.463687\n",
      "B          20.111732\n",
      "Cy         12.849162\n",
      "SD          7.262570\n",
      "N           6.145251\n",
      "h           5.307263\n",
      "K           3.910615\n",
      "PA          3.351955\n",
      "C           1.955307\n",
      "SA          1.675978\n",
      "cell        1.675978\n",
      "S           1.675978\n",
      "O           1.675978\n",
      "I           1.117318\n",
      "MC          0.837989\n",
      "cells       0.837989\n",
      "D           0.558659\n",
      "variant     0.279330\n",
      "6           0.279330\n",
      "7           0.279330\n",
      "CC          0.279330\n",
      "β           0.279330\n",
      "F           0.279330\n",
      "UC          0.279330\n",
      "He          0.279330\n",
      "29          0.279330\n",
      "9           0.279330\n",
      "Name: A, dtype: float64 97.20670391061455 97.20670391061455\n"
     ]
    }
   ],
   "source": [
    "idx = 30\n",
    "print(res['same_first']['0'][0][:idx],res['same_first']['0'][0][:idx].sum(), res['same_first']['0'][0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ch##ole##ster##ol       0.387597\n",
      "t##rig##ly##cer##ide    0.129199\n",
      "glucose                 0.129199\n",
      "Name: A, dtype: float64 0.6459948320413438\n"
     ]
    }
   ],
   "source": [
    "idx=50\n",
    "# res['same_all']['01'][0].sort_values(inplace=True)\n",
    "print(res['same_all']['01'][0][:idx],res['same_all']['01'][0][:idx].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nums(ENT,DATASET):\n",
    "    import os\n",
    "    data_dir = os.path.join(data_path, ENT, DATASET)\n",
    "    params[\"entity_type\"] = ENT\n",
    "    params[\"dataset\"] = DATASET\n",
    "\n",
    "    params[\"LOWER_CASE\"] = False\n",
    "    params[\"LOAD_BEST_MODEL\"] = True\n",
    "    params[\"MAX_LEN\"] = 256\n",
    "    params[\"BATCH_SIZE\"] = 32\n",
    "    params[\"EPOCH_TOP\"] = 100\n",
    "    params[\"EPOCH_END2END\"] = 100\n",
    "\n",
    "    params[\"EXP_NAME\"] = \"test\"\n",
    "    params[\"WORKING_DIR\"] = \"sbksvol/nikhil/\" + params[\"EXP_NAME\"] + \"/\"\n",
    "    params[\"CACHE_DIR\"] = params[\"WORKING_DIR\"] + \"NER_out_test/\"\n",
    "\n",
    "    # Where model checkpoints are stored.\n",
    "    params[\"OUTPUT_DIR\"] = params[\"WORKING_DIR\"] + \"model_output_test/\"\n",
    "    params[\"TRAIN_ARGS_FILE\"] = params[\"WORKING_DIR\"] + \"train_args_test.json\"\n",
    "    params[\"DATA_PATH\"] = data_path\n",
    "\n",
    "    train_df, test_df, dev_df, labels, num_labels, label_map, data_dir, wt = prepare_data()\n",
    "\n",
    "    data_args, model_args, config, tokenizer = prepare_config_and_tokenizer(\n",
    "        data_dir, labels, num_labels, label_map)\n",
    "\n",
    "    # ## Create Dataset Objects\n",
    "    # print(Split.dev)\n",
    "\n",
    "    train_dataset = NerDataset(\n",
    "        data_dir=data_args['data_dir'],\n",
    "        tokenizer=tokenizer,\n",
    "        labels=labels,\n",
    "        model_type=config.model_type,\n",
    "        max_seq_length=data_args['max_seq_length'],\n",
    "        overwrite_cache=data_args['overwrite_cache'],  # True\n",
    "        mode=Split.train, data_size=100)\n",
    "\n",
    "    val_dataset = NerDataset(\n",
    "        data_dir=data_args['data_dir'],\n",
    "        tokenizer=tokenizer,\n",
    "        labels=labels,\n",
    "        model_type=config.model_type,\n",
    "        max_seq_length=data_args['max_seq_length'],\n",
    "        overwrite_cache=data_args['overwrite_cache'],  # True\n",
    "        mode=Split.dev, data_size=100)\n",
    "\n",
    "    test_dataset = NerDataset(\n",
    "        data_dir=data_args['data_dir'],\n",
    "        tokenizer=tokenizer,\n",
    "        labels=labels,\n",
    "        model_type=config.model_type,\n",
    "        max_seq_length=data_args['max_seq_length'],\n",
    "        overwrite_cache=data_args['overwrite_cache'],  # True\n",
    "        mode=Split.test, data_size=100)\n",
    "    dataset = [train_dataset,val_dataset,test_dataset]\n",
    "    for j in range(3):\n",
    "        sent_cnt = {'B':0,'O':0,'I':0}\n",
    "        tag_cnt = {'B':0,'O':0,'I':0}\n",
    "        sent_len = []\n",
    "        for i in range(dataset[j].__len__()):\n",
    "            sent = dataset[j].__getitem__(i)\n",
    "        #     print(sent)\n",
    "            if 0 in sent.label_ids:\n",
    "                sent_cnt['B']+=1\n",
    "                tag_cnt['B']+=np.sum(np.array(sent.label_ids)==0)\n",
    "            if 1 in sent.label_ids:\n",
    "                sent_cnt['I']+=1\n",
    "                tag_cnt['I']+=np.sum(np.array(sent.label_ids)==1)\n",
    "            if 2 in sent.label_ids:\n",
    "                sent_cnt['O']+=1\n",
    "                tag_cnt['O']+=np.sum(np.array(sent.label_ids)==2)\n",
    "            sent_len.append(np.sum(np.array(sent.attention_mask)))\n",
    "        print(ENT,DATASET,\"\\n\")\n",
    "        print(\"Sent Count\",sent_cnt, f'%B-{sent_cnt[\"B\"]/sent_cnt[\"O\"]}',f'%I-{sent_cnt[\"I\"]/sent_cnt[\"O\"]}',\"\\n\")\n",
    "        print(\"Tag Count\",tag_cnt, f'%B-{tag_cnt[\"B\"]/tag_cnt[\"O\"]}',f'%I-{tag_cnt[\"I\"]/tag_cnt[\"O\"]}',\"\\n\")\n",
    "        print(f'mean-{np.mean(sent_len)}, std-{np.std(sent_len)}, percentile[25,50,75,90,100]-{np.percentile(sent_len,[25,50,75,90,100])}',\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_sents, num_dev_sents, num_test_sents =  3676 951 1609\n",
      "First 10 words in test data:\n",
      "   sentence_id      words labels\n",
      "0            0    Scp160p      O\n",
      "1            0          ,      O\n",
      "2            0          a      O\n",
      "3            0   multiple      O\n",
      "4            0  KH-domain      O\n",
      "5            0    protein      O\n",
      "6            0          ,      O\n",
      "7            0         is      O\n",
      "8            0          a      O\n",
      "9            0  component      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [ 50.77444052 189.06798246   0.33613265]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:21.157372 140054965573440 filelock.py:274] Lock 140052483773720 acquired on /sbksvol/nikhil/NER_data/Species/linneaus/cached_train_BertTokenizer_256.lock\n",
      "I0830 03:05:21.159084 140054965573440 utils_ner.py:111] Creating features from dataset file at /sbksvol/nikhil/NER_data/Species/linneaus\n",
      "I0830 03:05:21.263454 140054965573440 utils_ner.py:299] Writing example 0 of 3675\n",
      "I0830 03:05:21.266353 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:21.267159 140054965573440 utils_ner.py:385] guid: train-1\n",
      "I0830 03:05:21.267636 140054965573440 utils_ner.py:386] tokens: [CLS] In spite of medical help : the puzzle of an eighteenth - century Prime Minister ' s illness . [SEP]\n",
      "I0830 03:05:21.283430 140054965573440 utils_ner.py:387] input_ids: 101 1130 8438 1104 2657 1494 131 1103 13544 1104 1126 14772 118 1432 3460 2110 112 188 6946 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:21.284064 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:21.284422 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:21.284757 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 -100 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:21.285255 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:21.285568 140054965573440 utils_ner.py:385] guid: train-2\n",
      "I0830 03:05:21.285851 140054965573440 utils_ner.py:386] tokens: [CLS] A ##bs ##tract [SEP]\n",
      "I0830 03:05:21.286168 140054965573440 utils_ner.py:387] input_ids: 101 138 4832 15017 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:21.286512 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:21.286835 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:21.287162 140054965573440 utils_ner.py:390] label_ids: -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:21.287927 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:21.288221 140054965573440 utils_ner.py:385] guid: train-3\n",
      "I0830 03:05:21.288521 140054965573440 utils_ner.py:386] tokens: [CLS] Medical History , 1990 , 34 : 178 - 184 . [SEP]\n",
      "I0830 03:05:21.288848 140054965573440 utils_ner.py:387] input_ids: 101 3875 2892 117 1997 117 3236 131 20977 118 21421 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:21.289178 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:21.289531 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:21.289874 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 2 2 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:21.290737 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:21.291039 140054965573440 utils_ner.py:385] guid: train-4\n",
      "I0830 03:05:21.291322 140054965573440 utils_ner.py:386] tokens: [CLS] IN SP ##IT ##E OF ME ##DI ##CA ##L H ##EL ##P : THE P ##U ##Z ##Z ##LE OF AN [SEP]\n",
      "I0830 03:05:21.291680 140054965573440 utils_ner.py:387] input_ids: 101 15969 16625 12150 2036 11345 22157 17243 11356 2162 145 21678 2101 131 7462 153 2591 5301 5301 17516 11345 23096 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:21.292006 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:21.292326 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:21.292670 140054965573440 utils_ner.py:390] label_ids: -100 2 2 -100 -100 2 2 -100 -100 -100 2 -100 -100 2 2 2 -100 -100 -100 -100 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:21.293336 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:21.293663 140054965573440 utils_ner.py:385] guid: train-5\n",
      "I0830 03:05:21.293948 140054965573440 utils_ner.py:386] tokens: [CLS] E ##IG ##HT ##EE ##NT ##H - CE ##NT ##UR ##Y PR ##IM ##E MI ##NI ##ST ##ER ' S [SEP]\n",
      "I0830 03:05:21.294279 140054965573440 utils_ner.py:387] input_ids: 101 142 23413 18784 27073 15681 3048 118 9855 15681 19556 3663 11629 13371 2036 26574 27451 9272 9637 112 156 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:21.294640 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:21.294961 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:21.295279 140054965573440 utils_ner.py:390] label_ids: -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 2 -100 -100 2 -100 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:25.694730 140054965573440 utils_ner.py:134] Saving features into cached file /sbksvol/nikhil/NER_data/Species/linneaus/cached_train_BertTokenizer_256\n",
      "I0830 03:05:27.076916 140054965573440 filelock.py:318] Lock 140052483773720 released on /sbksvol/nikhil/NER_data/Species/linneaus/cached_train_BertTokenizer_256.lock\n",
      "I0830 03:05:27.280354 140054965573440 filelock.py:274] Lock 140052528928752 acquired on /sbksvol/nikhil/NER_data/Species/linneaus/cached_dev_BertTokenizer_256.lock\n",
      "I0830 03:05:27.281318 140054965573440 utils_ner.py:111] Creating features from dataset file at /sbksvol/nikhil/NER_data/Species/linneaus\n",
      "I0830 03:05:27.310066 140054965573440 utils_ner.py:299] Writing example 0 of 950\n",
      "I0830 03:05:27.311520 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:27.311949 140054965573440 utils_ner.py:385] guid: dev-1\n",
      "I0830 03:05:27.312322 140054965573440 utils_ner.py:386] tokens: [CLS] HP ##1 m ##od ##ulates the transcription of cell - cycle regulator ##s in Dr ##oso ##phi ##la me ##lan ##oga ##ster [SEP]\n",
      "I0830 03:05:27.312925 140054965573440 utils_ner.py:387] input_ids: 101 18444 1475 182 5412 20267 1103 15416 1104 2765 118 5120 27335 1116 1107 1987 22354 27008 1742 1143 4371 23282 4648 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:27.313517 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:27.313933 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:27.314357 140054965573440 utils_ner.py:390] label_ids: -100 2 -100 2 -100 -100 2 2 2 2 -100 -100 2 -100 2 0 -100 -100 -100 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:27.314912 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:27.315290 140054965573440 utils_ner.py:385] guid: dev-2\n",
      "I0830 03:05:27.315689 140054965573440 utils_ner.py:386] tokens: [CLS] A ##bs ##tract [SEP]\n",
      "I0830 03:05:27.316018 140054965573440 utils_ner.py:387] input_ids: 101 138 4832 15017 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:27.316347 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:27.316754 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:27.317078 140054965573440 utils_ner.py:390] label_ids: -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:27.318785 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:27.319308 140054965573440 utils_ner.py:385] guid: dev-3\n",
      "I0830 03:05:27.319595 140054965573440 utils_ner.py:386] tokens: [CLS] He ##tero ##ch ##roma ##tin protein 1 ( HP ##1 ) was originally described as a non - his ##tone ch ##rom ##oso ##mal protein and is required for transcription ##al gene si ##len ##cing and the formation of he ##tero ##ch ##roma ##tin . [SEP]\n",
      "I0830 03:05:27.319925 140054965573440 utils_ner.py:387] input_ids: 101 1124 25710 1732 18885 6105 4592 122 113 18444 1475 114 1108 2034 1758 1112 170 1664 118 1117 4793 22572 16071 22354 7435 4592 1105 1110 2320 1111 15416 1348 5565 27466 7836 4869 1105 1103 3855 1104 1119 25710 1732 18885 6105 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:27.320228 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:27.320549 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:27.322333 140054965573440 utils_ner.py:390] label_ids: -100 2 -100 -100 -100 -100 2 2 2 2 -100 2 2 2 2 2 2 2 -100 -100 -100 2 -100 -100 -100 2 2 2 2 2 2 -100 2 2 -100 -100 2 2 2 2 2 -100 -100 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:27.323813 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:27.324119 140054965573440 utils_ner.py:385] guid: dev-4\n",
      "I0830 03:05:27.324378 140054965573440 utils_ner.py:386] tokens: [CLS] Although it is localized primarily at per ##ice ##nt ##ric he ##tero ##ch ##roma ##tin , a scattered distribution over a large number of e ##uch ##romatic lo ##ci is also evident . [SEP]\n",
      "I0830 03:05:27.324712 140054965573440 utils_ner.py:387] input_ids: 101 1966 1122 1110 25813 3120 1120 1679 4396 2227 4907 1119 25710 1732 18885 6105 117 170 7648 3735 1166 170 1415 1295 1104 174 9827 16341 25338 6617 1110 1145 10238 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:27.325009 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:27.325320 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:27.325647 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 2 -100 -100 -100 2 -100 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 -100 -100 2 -100 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:27.327560 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:27.327918 140054965573440 utils_ner.py:385] guid: dev-5\n",
      "I0830 03:05:27.328208 140054965573440 utils_ner.py:386] tokens: [CLS] Here , we provide evidence that Dr ##oso ##phi ##la HP ##1 is essential for the maintenance of active transcription of e ##uch ##romatic genes functional ##ly involved in cell - cycle progression , including those required for DNA replication and mit ##osis . [SEP]\n",
      "I0830 03:05:27.328554 140054965573440 utils_ner.py:387] input_ids: 101 3446 117 1195 2194 2554 1115 1987 22354 27008 1742 18444 1475 1110 6818 1111 1103 5972 1104 2327 15416 1104 174 9827 16341 9077 8458 1193 2017 1107 2765 118 5120 16147 117 1259 1343 2320 1111 5394 25544 1105 26410 11776 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:27.328885 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:27.329190 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:27.330574 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 0 -100 -100 -100 2 -100 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 -100 2 2 2 -100 -100 2 2 2 2 2 2 2 2 2 2 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:28.336653 140054965573440 utils_ner.py:134] Saving features into cached file /sbksvol/nikhil/NER_data/Species/linneaus/cached_dev_BertTokenizer_256\n",
      "I0830 03:05:28.847803 140054965573440 filelock.py:318] Lock 140052528928752 released on /sbksvol/nikhil/NER_data/Species/linneaus/cached_dev_BertTokenizer_256.lock\n",
      "I0830 03:05:29.056683 140054965573440 filelock.py:274] Lock 140052445554896 acquired on /sbksvol/nikhil/NER_data/Species/linneaus/cached_test_BertTokenizer_256.lock\n",
      "I0830 03:05:29.058892 140054965573440 utils_ner.py:111] Creating features from dataset file at /sbksvol/nikhil/NER_data/Species/linneaus\n",
      "I0830 03:05:29.113148 140054965573440 utils_ner.py:299] Writing example 0 of 1608\n",
      "I0830 03:05:29.115662 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:29.116016 140054965573440 utils_ner.py:385] guid: test-1\n",
      "I0830 03:05:29.116329 140054965573440 utils_ner.py:386] tokens: [CLS] Sc ##p ##16 ##0 ##p , a multiple K ##H - domain protein , is a component of m ##R ##NP complexes in yeast [SEP]\n",
      "I0830 03:05:29.116914 140054965573440 utils_ner.py:387] input_ids: 101 20452 1643 16229 1568 1643 117 170 2967 148 3048 118 5777 4592 117 1110 170 6552 1104 182 2069 14576 16575 1107 25693 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:29.117250 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:29.117659 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:29.118102 140054965573440 utils_ner.py:390] label_ids: -100 2 -100 -100 -100 -100 2 2 2 2 -100 -100 -100 2 2 2 2 2 2 2 -100 -100 2 2 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:29.118675 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:29.119052 140054965573440 utils_ner.py:385] guid: test-2\n",
      "I0830 03:05:29.119390 140054965573440 utils_ner.py:386] tokens: [CLS] A ##bs ##tract [SEP]\n",
      "I0830 03:05:29.119705 140054965573440 utils_ner.py:387] input_ids: 101 138 4832 15017 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:29.120009 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:29.120441 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:29.120762 140054965573440 utils_ner.py:390] label_ids: -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:29.122904 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:29.123200 140054965573440 utils_ner.py:385] guid: test-3\n",
      "I0830 03:05:29.123497 140054965573440 utils_ner.py:386] tokens: [CLS] Sc ##p ##16 ##0 ##p is a 160 k ##D ##a protein in the yeast Sa ##cc ##har ##omy ##ces c ##ere ##vis ##iae that contains 14 repeats of the h ##n ##R ##NP K - ho ##mology ( K ##H ) domain , and demonstrates significant sequence ho ##mology to a family of proteins collectively known as v ##igi ##lins . [SEP]\n",
      "I0830 03:05:29.123911 140054965573440 utils_ner.py:387] input_ids: 101 20452 1643 16229 1568 1643 1110 170 7690 180 2137 1161 4592 1107 1103 25693 17784 19515 7111 18574 7723 172 9014 9356 21845 1115 2515 1489 19811 1104 1103 177 1179 2069 14576 148 118 16358 19969 113 148 3048 114 5777 117 1105 17798 2418 4954 16358 19969 1106 170 1266 1104 7865 14998 1227 1112 191 24874 17828 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:29.124370 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:29.124823 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:29.125264 140054965573440 utils_ner.py:390] label_ids: -100 2 -100 -100 -100 -100 2 2 2 2 -100 -100 2 2 2 0 0 -100 -100 -100 -100 1 -100 -100 -100 2 2 2 2 2 2 2 -100 -100 -100 2 -100 -100 -100 2 2 -100 -100 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:29.126952 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:29.127274 140054965573440 utils_ner.py:385] guid: test-4\n",
      "I0830 03:05:29.127601 140054965573440 utils_ner.py:386] tokens: [CLS] As a first step towards defining the function of Sc ##p ##16 ##0 ##p , we have characterized the sub ##cellular distribution and in v ##ivo interactions of this protein . [SEP]\n",
      "I0830 03:05:29.128058 140054965573440 utils_ner.py:387] input_ids: 101 1249 170 1148 2585 2019 13682 1103 3053 1104 20452 1643 16229 1568 1643 117 1195 1138 6858 1103 4841 18091 3735 1105 1107 191 15435 10393 1104 1142 4592 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:29.128509 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:29.130245 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:29.130682 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 2 2 2 2 2 2 -100 2 2 2 2 -100 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:29.132234 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:29.132679 140054965573440 utils_ner.py:385] guid: test-5\n",
      "I0830 03:05:29.133062 140054965573440 utils_ner.py:386] tokens: [CLS] Using su ##c ##rose gradient fraction ##ation studies we have demonstrated that Sc ##p ##16 ##0 ##p in c ##yt ##op ##las ##mic l ##ys ##ates is predominantly associated with p ##oly ##ri ##bos ##ome ##s . [SEP]\n",
      "I0830 03:05:29.133511 140054965573440 utils_ner.py:387] input_ids: 101 7993 28117 1665 10127 19848 13394 1891 2527 1195 1138 7160 1115 20452 1643 16229 1568 1643 1107 172 25669 4184 7580 7257 181 6834 5430 1110 8941 2628 1114 185 23415 2047 18071 6758 1116 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:29.133912 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:29.134305 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:29.134681 140054965573440 utils_ner.py:390] label_ids: -100 2 2 -100 -100 2 2 -100 2 2 2 2 2 2 -100 -100 -100 -100 2 2 -100 -100 -100 -100 2 -100 -100 2 2 2 2 2 -100 -100 -100 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:31.067172 140054965573440 utils_ner.py:134] Saving features into cached file /sbksvol/nikhil/NER_data/Species/linneaus/cached_test_BertTokenizer_256\n",
      "I0830 03:05:31.945132 140054965573440 filelock.py:318] Lock 140052445554896 released on /sbksvol/nikhil/NER_data/Species/linneaus/cached_test_BertTokenizer_256.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species linneaus \n",
      "\n",
      "Sent Count {'B': 401, 'O': 3675, 'I': 104} %B-0.1091156462585034 %I-0.028299319727891157 \n",
      "\n",
      "Tag Count {'B': 566, 'O': 85426, 'I': 152} %B-0.006625617493503149 %I-0.0017793177721068527 \n",
      "\n",
      "mean-36.40190476190476, std-23.50036263530692, percentile[25,50,75,90,100]-[ 21.  34.  48.  64. 256.] \n",
      "\n",
      "Species linneaus \n",
      "\n",
      "Sent Count {'B': 65, 'O': 950, 'I': 8} %B-0.06842105263157895 %I-0.008421052631578947 \n",
      "\n",
      "Tag Count {'B': 85, 'O': 19830, 'I': 9} %B-0.004286434694906707 %I-0.000453857791225416 \n",
      "\n",
      "mean-31.126315789473683, std-22.243789011418166, percentile[25,50,75,90,100]-[ 15.  28.  42.  58. 249.] \n",
      "\n",
      "Species linneaus \n",
      "\n",
      "Sent Count {'B': 188, 'O': 1608, 'I': 68} %B-0.11691542288557213 %I-0.04228855721393035 \n",
      "\n",
      "Tag Count {'B': 278, 'O': 39002, 'I': 123} %B-0.007127839597969335 %I-0.00315368442643967 \n",
      "\n",
      "mean-38.458333333333336, std-22.565950998925434, percentile[25,50,75,90,100]-[ 23.  36.  51.  65. 185.] \n",
      "\n",
      "num_train_sents, num_dev_sents, num_test_sents =  576 86 293\n",
      "First 10 words in test data:\n",
      "   sentence_id             words labels\n",
      "0            0         Molecular      O\n",
      "1            0           cloning      O\n",
      "2            0               and      O\n",
      "3            0       biochemical      O\n",
      "4            0  characterization      O\n",
      "5            0                of      O\n",
      "6            0                 a      O\n",
      "7            0         truncated      O\n",
      "8            0                 ,      O\n",
      "9            0          secreted      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [ 28.68577495 180.14666667   0.33788481]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:36.159404 140054965573440 filelock.py:274] Lock 140054794797744 acquired on /sbksvol/nikhil/NER_data/Species/loctext/cached_train_BertTokenizer_256.lock\n",
      "I0830 03:05:36.162157 140054965573440 utils_ner.py:111] Creating features from dataset file at /sbksvol/nikhil/NER_data/Species/loctext\n",
      "I0830 03:05:36.186352 140054965573440 utils_ner.py:299] Writing example 0 of 575\n",
      "I0830 03:05:36.188238 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:36.188631 140054965573440 utils_ner.py:385] guid: train-1\n",
      "I0830 03:05:36.189946 140054965573440 utils_ner.py:386] tokens: [CLS] Disc ##ret ##e domains media ##te the light - re ##sp ##ons ##ive nuclear and c ##yt ##op ##las ##mic local ##ization of Arab ##ido ##psis CO ##P ##1 . [SEP]\n",
      "I0830 03:05:36.190540 140054965573440 utils_ner.py:387] input_ids: 101 14856 8127 1162 13770 2394 1566 1103 1609 118 1231 20080 4199 2109 4272 1105 172 25669 4184 7580 7257 1469 2734 1104 4699 12894 17990 18732 2101 1475 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:36.190877 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:36.191183 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:36.191496 140054965573440 utils_ner.py:390] label_ids: -100 2 -100 -100 2 2 -100 2 2 -100 -100 -100 -100 -100 2 2 2 -100 -100 -100 -100 2 -100 2 0 -100 -100 2 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:36.192964 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:36.193251 140054965573440 utils_ner.py:385] guid: train-2\n",
      "I0830 03:05:36.193536 140054965573440 utils_ner.py:386] tokens: [CLS] The Arab ##ido ##psis CO ##NS ##TI ##TU ##TI ##VE P ##H ##OT ##OM ##OR ##P ##H ##O ##GE ##NI ##C ##1 ( CO ##P ##1 ) protein plays a critical role in the repression of photo ##mor ##ph ##ogen ##esis during Arab ##ido ##psis seed ##ling development . [SEP]\n",
      "I0830 03:05:36.193845 140054965573440 utils_ner.py:387] input_ids: 101 1109 4699 12894 17990 18732 12412 21669 27074 21669 17145 153 3048 14697 13041 9565 2101 3048 2346 16523 27451 1658 1475 113 18732 2101 1475 114 4592 2399 170 3607 1648 1107 1103 26275 1104 6307 26271 7880 19790 16317 1219 4699 12894 17990 6478 1979 1718 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:36.194144 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:36.194446 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:36.194745 140054965573440 utils_ner.py:390] label_ids: -100 2 0 -100 -100 2 -100 -100 -100 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 2 2 -100 -100 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 2 0 -100 -100 2 -100 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:36.196648 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:36.196933 140054965573440 utils_ner.py:385] guid: train-3\n",
      "I0830 03:05:36.197198 140054965573440 utils_ner.py:386] tokens: [CLS] We investigated the control of CO ##P ##1 partition ##ing between nucleus and c ##yt ##op ##las ##m , which has been implicated in the regulation of CO ##P ##1 activity , by using fusion proteins between CO ##P ##1 and beta - g ##lu ##cu ##ron ##idas ##e or the green fluorescent protein . [SEP]\n",
      "I0830 03:05:36.197503 140054965573440 utils_ner.py:387] input_ids: 101 1284 10788 1103 1654 1104 18732 2101 1475 16416 1158 1206 14297 1105 172 25669 4184 7580 1306 117 1134 1144 1151 22512 1107 1103 8585 1104 18732 2101 1475 3246 117 1118 1606 11970 7865 1206 18732 2101 1475 1105 11933 118 176 7535 10182 3484 23358 1162 1137 1103 2448 26163 4592 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:36.197790 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:36.198086 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:36.198390 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 -100 -100 2 -100 2 2 2 2 -100 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 2 -100 -100 2 2 -100 -100 -100 -100 -100 -100 -100 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:36.200178 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:36.200492 140054965573440 utils_ner.py:385] guid: train-4\n",
      "I0830 03:05:36.200762 140054965573440 utils_ner.py:386] tokens: [CLS] Trans ##ient expression ass ##ays using on ##ion e ##pid ##er ##mal cells and data from h ##y ##po ##cot ##yl cells of stab ##ly transformed Arab ##ido ##psis demonstrated that CO ##P ##1 carries a single , bi ##par ##tite nuclear local ##ization signal that functions independently of light . [SEP]\n",
      "I0830 03:05:36.201064 140054965573440 utils_ner.py:387] input_ids: 101 13809 9080 2838 3919 22979 1606 1113 1988 174 25786 1200 7435 3652 1105 2233 1121 177 1183 5674 18982 7777 3652 1104 19428 1193 8272 4699 12894 17990 7160 1115 18732 2101 1475 7450 170 1423 117 16516 17482 23723 4272 1469 2734 4344 1115 4226 8942 1104 1609 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:36.201359 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:36.201665 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:36.201966 140054965573440 utils_ner.py:390] label_ids: -100 2 -100 2 2 -100 2 0 -100 2 -100 -100 -100 2 2 2 2 2 -100 -100 -100 -100 2 2 2 -100 2 0 -100 -100 2 2 2 -100 -100 2 2 2 2 2 -100 -100 2 2 -100 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:36.203572 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:36.203854 140054965573440 utils_ner.py:385] guid: train-5\n",
      "I0830 03:05:36.204120 140054965573440 utils_ner.py:386] tokens: [CLS] Nuclear exclusion was mediated by a novel and distinct signal , bordering the zinc - finger and coil ##ed - coil motifs , that was able to red ##ire ##ct a he ##tero ##log ##ous nuclear protein to the c ##yt ##op ##las ##m . [SEP]\n",
      "I0830 03:05:36.204424 140054965573440 utils_ner.py:387] input_ids: 101 11560 18434 1108 22060 1118 170 2281 1105 4966 4344 117 20268 1103 19159 118 3602 1105 20614 1174 118 20614 18916 117 1115 1108 1682 1106 1894 5817 5822 170 1119 25710 13791 2285 4272 4592 1106 1103 172 25669 4184 7580 1306 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:36.204720 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:36.205013 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:36.205308 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 -100 -100 -100 2 2 2 2 2 2 2 -100 -100 2 2 -100 -100 -100 2 2 2 2 2 -100 -100 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:36.847492 140054965573440 utils_ner.py:134] Saving features into cached file /sbksvol/nikhil/NER_data/Species/loctext/cached_train_BertTokenizer_256\n",
      "I0830 03:05:37.264576 140054965573440 filelock.py:318] Lock 140054794797744 released on /sbksvol/nikhil/NER_data/Species/loctext/cached_train_BertTokenizer_256.lock\n",
      "I0830 03:05:37.460340 140054965573440 filelock.py:274] Lock 140052527134648 acquired on /sbksvol/nikhil/NER_data/Species/loctext/cached_dev_BertTokenizer_256.lock\n",
      "I0830 03:05:37.461858 140054965573440 utils_ner.py:111] Creating features from dataset file at /sbksvol/nikhil/NER_data/Species/loctext\n",
      "I0830 03:05:37.466429 140054965573440 utils_ner.py:299] Writing example 0 of 85\n",
      "I0830 03:05:37.468930 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:37.469966 140054965573440 utils_ner.py:385] guid: dev-1\n",
      "I0830 03:05:37.470640 140054965573440 utils_ner.py:386] tokens: [CLS] Ye ##ast mitochondrial de ##hy ##dr ##ogen ##ases are associated in a su ##pra ##mo ##le ##cular complex . [SEP]\n",
      "I0830 03:05:37.471220 140054965573440 utils_ner.py:387] input_ids: 101 15821 12788 27850 1260 7889 23632 19790 23105 1132 2628 1107 170 28117 20488 3702 1513 11702 2703 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:37.471775 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:37.472288 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:37.473209 140054965573440 utils_ner.py:390] label_ids: -100 0 -100 2 2 -100 -100 -100 -100 2 2 2 2 2 -100 -100 -100 -100 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:37.476271 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:37.477013 140054965573440 utils_ner.py:385] guid: dev-2\n",
      "I0830 03:05:37.477429 140054965573440 utils_ner.py:386] tokens: [CLS] Sep ##ara ##tion of yeast mitochondrial complexes by color ##less native p ##oly ##ac ##ryl ##ami ##de gel electro ##ph ##ores ##is led to the identification of a su ##pra ##mo ##le ##cular structure exhibiting N ##AD ##H - de ##hy ##dr ##ogen ##ase activity . [SEP]\n",
      "I0830 03:05:37.477898 140054965573440 utils_ner.py:387] input_ids: 101 23181 4626 2116 1104 25693 27850 16575 1118 2942 2008 2900 185 23415 7409 19944 11787 2007 27426 24266 7880 12238 1548 1521 1106 1103 9117 1104 170 28117 20488 3702 1513 11702 2401 25988 151 14569 3048 118 1260 7889 23632 19790 6530 3246 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:37.478337 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:37.478770 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:37.479247 140054965573440 utils_ner.py:390] label_ids: -100 2 -100 -100 2 0 2 2 2 2 -100 2 2 -100 -100 -100 -100 -100 2 2 -100 -100 -100 2 2 2 2 2 2 2 -100 -100 -100 -100 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:37.480896 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:37.481274 140054965573440 utils_ner.py:385] guid: dev-3\n",
      "I0830 03:05:37.481672 140054965573440 utils_ner.py:386] tokens: [CLS] Co ##mpo ##nent ##s of this complex were identified by N - terminal Ed ##man degradation and matrix - assisted laser des ##or ##ption ion ##ization mass s ##pect ##rome ##try . [SEP]\n",
      "I0830 03:05:37.482079 140054965573440 utils_ner.py:387] input_ids: 101 3291 24729 21222 1116 1104 1142 2703 1127 3626 1118 151 118 6020 5316 1399 18126 1105 8952 118 6842 10221 3532 1766 18225 14469 2734 3367 188 26426 11457 6013 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:37.482481 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:37.482875 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:37.483285 140054965573440 utils_ner.py:390] label_ids: -100 2 -100 -100 -100 2 2 2 2 2 2 2 -100 -100 2 -100 2 2 2 -100 -100 2 2 -100 -100 2 -100 2 2 -100 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:37.489015 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:37.489406 140054965573440 utils_ner.py:385] guid: dev-4\n",
      "I0830 03:05:37.489783 140054965573440 utils_ner.py:386] tokens: [CLS] The complex was found to contain the five known inter ##me ##mb ##rane space - facing de ##hy ##dr ##ogen ##ases , namely two external N ##AD ##H - de ##hy ##dr ##ogen ##ases N ##de ##1 ##p and N ##de ##2 ##p , g ##ly ##cer ##ol - 3 - phosphate de ##hy ##dr ##ogen ##ase G ##ut ##2 ##p , D - and L - la ##ct ##ate - de ##hy ##dr ##ogen ##ases D ##ld ##1 ##p and Cy ##b ##2 ##p , the matrix - facing N ##AD ##H - de ##hy ##dr ##ogen ##ase N ##di ##1 ##p , two probable fl ##av ##op ##rote ##ins Y ##OR ##35 ##6 ##W ##p and Y ##PR ##00 ##4 ##C ##p , four t ##rica ##rb ##ox ##yl ##ic acids cycle enzymes ( ma ##late de ##hy ##dr ##ogen ##ase M ##dh ##1 ##p , c ##it ##rate s ##ynth ##ase C ##it ##1 ##p , su ##cci ##nate de ##hy ##dr ##ogen ##ase S ##dh ##1 ##p , and f ##uma ##rate h ##yd ##rata ##se Fu ##m ##1 ##p ) , and the ace ##tal ##de ##hy ##de de ##hy ##dr ##ogen ##ase Al ##d ##4 ##p . [SEP]\n",
      "I0830 03:05:37.490207 140054965573440 utils_ner.py:387] input_ids: 101 1109 2703 1108 1276 1106 4651 1103 1421 1227 9455 3263 12913 18194 2000 118 4749 1260 7889 23632 19790 23105 117 8199 1160 6298 151 14569 3048 118 1260 7889 23632 19790 23105 151 2007 1475 1643 1105 151 2007 1477 1643 117 176 1193 14840 4063 118 124 118 19273 1260 7889 23632 19790 6530 144 3818 1477 1643 117 141 118 1105 149 118 2495 5822 2193 118 1260 7889 23632 19790 23105 141 5253 1475 1643 1105 27688 1830 1477 1643 117 1103 8952 118 4749 151 14569 3048 118 1260 7889 23632 19790 6530 151 3309 1475 1643 117 1160 16950 22593 23140 4184 21020 4935 162 9565 19297 1545 2924 1643 1105 162 22861 7629 1527 1658 1643 117 1300 189 15353 26281 10649 7777 1596 13087 5120 17664 113 12477 8052 1260 7889 23632 19790 6530 150 17868 1475 1643 117 172 2875 5498 188 26588 6530 140 2875 1475 1643 117 28117 19557 13978 1260 7889 23632 19790 6530 156 17868 1475 1643 117 1105 175 10161 5498 177 19429 15471 2217 14763 1306 1475 1643 114 117 1105 1103 20839 6163 2007 7889 2007 1260 7889 23632 19790 6530 2586 1181 1527 1643 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:37.490654 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:37.491088 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:37.491641 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 2 -100 -100 2 -100 -100 -100 -100 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 2 -100 -100 -100 2 2 -100 -100 -100 2 2 -100 -100 -100 -100 -100 -100 -100 2 -100 -100 -100 -100 2 -100 -100 -100 2 2 -100 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 2 -100 -100 -100 2 2 -100 -100 -100 2 2 2 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 2 -100 -100 -100 2 2 2 2 -100 -100 -100 -100 2 -100 -100 -100 -100 -100 2 2 -100 -100 -100 -100 -100 2 2 2 -100 -100 -100 -100 -100 2 2 2 2 2 -100 2 -100 -100 -100 -100 2 -100 -100 -100 2 2 -100 -100 2 -100 -100 2 -100 -100 -100 2 2 -100 -100 2 -100 -100 -100 -100 2 -100 -100 -100 2 2 2 -100 -100 2 -100 -100 -100 2 -100 -100 -100 2 2 2 2 2 -100 -100 -100 -100 2 -100 -100 -100 -100 2 -100 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:37.493345 140054965573440 utils_ner.py:384] *** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:37.493764 140054965573440 utils_ner.py:385] guid: dev-5\n",
      "I0830 03:05:37.494137 140054965573440 utils_ner.py:386] tokens: [CLS] The association of these proteins is discussed in terms of N ##AD ##H - channel ##ing . [SEP]\n",
      "I0830 03:05:37.494611 140054965573440 utils_ner.py:387] input_ids: 101 1109 3852 1104 1292 7865 1110 6352 1107 2538 1104 151 14569 3048 118 3094 1158 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:37.495045 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:37.495473 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:37.495889 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:37.617535 140054965573440 utils_ner.py:134] Saving features into cached file /sbksvol/nikhil/NER_data/Species/loctext/cached_dev_BertTokenizer_256\n",
      "I0830 03:05:37.973324 140054965573440 filelock.py:318] Lock 140052527134648 released on /sbksvol/nikhil/NER_data/Species/loctext/cached_dev_BertTokenizer_256.lock\n",
      "I0830 03:05:38.217860 140054965573440 filelock.py:274] Lock 140052191759160 acquired on /sbksvol/nikhil/NER_data/Species/loctext/cached_test_BertTokenizer_256.lock\n",
      "I0830 03:05:38.219004 140054965573440 utils_ner.py:111] Creating features from dataset file at /sbksvol/nikhil/NER_data/Species/loctext\n",
      "I0830 03:05:38.226770 140054965573440 utils_ner.py:299] Writing example 0 of 292\n",
      "I0830 03:05:38.228287 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:38.228827 140054965573440 utils_ner.py:385] guid: test-1\n",
      "I0830 03:05:38.229150 140054965573440 utils_ner.py:386] tokens: [CLS] Molecular c ##lon ##ing and bio ##chemical characterization of a t ##runcated , secret ##ed member of the human family of C ##a ##2 + - activated C ##l - channels . [SEP]\n",
      "I0830 03:05:38.229518 140054965573440 utils_ner.py:387] input_ids: 101 22175 172 4934 1158 1105 25128 16710 27419 1104 170 189 28098 117 3318 1174 1420 1104 1103 1769 1266 1104 140 1161 1477 116 118 9618 140 1233 118 6412 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:38.229871 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:38.230209 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:38.230548 140054965573440 utils_ner.py:390] label_ids: -100 2 2 -100 -100 2 2 -100 2 2 2 2 -100 2 2 -100 2 2 2 0 2 2 2 -100 -100 -100 -100 -100 2 -100 -100 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:38.232675 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:38.233014 140054965573440 utils_ner.py:385] guid: test-2\n",
      "I0830 03:05:38.233313 140054965573440 utils_ner.py:386] tokens: [CLS] A novel family of chloride channel proteins has recently been discovered including two b ##ov ##ine ( Lu - EC ##AM - 1 , b ##CL ##CA ##1 ) , one m ##uri ##ne ( m ##CL ##CA ##1 ) , and two human ( h ##CL ##CA ##1 and h ##CL ##CA ##2 ) members . [SEP]\n",
      "I0830 03:05:38.233675 140054965573440 utils_ner.py:387] input_ids: 101 138 2281 1266 1104 21256 3094 7865 1144 3055 1151 2751 1259 1160 171 3292 2042 113 14557 118 16028 10964 118 122 117 171 26351 11356 1475 114 117 1141 182 8212 1673 113 182 26351 11356 1475 114 117 1105 1160 1769 113 177 26351 11356 1475 1105 177 26351 11356 1477 114 1484 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:38.234021 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:38.234364 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:38.234719 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 2 2 2 2 2 2 2 0 -100 -100 2 2 -100 -100 -100 -100 -100 2 2 -100 -100 -100 2 2 2 0 -100 -100 2 -100 -100 -100 -100 2 2 2 2 0 2 -100 -100 -100 -100 2 2 -100 -100 -100 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:38.236193 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:38.236523 140054965573440 utils_ner.py:385] guid: test-3\n",
      "I0830 03:05:38.236823 140054965573440 utils_ner.py:386] tokens: [CLS] Here , we describe the c ##lon ##ing , expression , and molecular characterization of a t ##runcated human ho ##mo ##log , tentatively named h ##CL ##CA ##3 . [SEP]\n",
      "I0830 03:05:38.237147 140054965573440 utils_ner.py:387] input_ids: 101 3446 117 1195 5594 1103 172 4934 1158 117 2838 117 1105 9546 27419 1104 170 189 28098 1769 16358 3702 13791 117 22285 1417 177 26351 11356 1495 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:38.237485 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:38.237820 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:38.238164 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 2 2 -100 0 2 -100 -100 2 2 2 2 -100 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:38.240289 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:38.240617 140054965573440 utils_ner.py:385] guid: test-4\n",
      "I0830 03:05:38.240924 140054965573440 utils_ner.py:386] tokens: [CLS] It was clone ##d from a human s ##ple ##en c ##D ##NA library and is expressed in numerous tissues including lung , t ##rac ##hea , s ##ple ##en , thy ##mus , and ma ##mma ##ry g ##land as determined by reverse trans ##cript ##ase - polymer ##ase chain reaction . [SEP]\n",
      "I0830 03:05:38.241260 140054965573440 utils_ner.py:387] input_ids: 101 1135 1108 22121 1181 1121 170 1769 188 7136 1424 172 2137 11185 3340 1105 1110 4448 1107 2567 14749 1259 13093 117 189 19366 13836 117 188 7136 1424 117 21153 6308 117 1105 12477 12917 1616 176 1931 1112 3552 1118 7936 14715 13590 6530 118 21176 6530 4129 3943 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:38.241609 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:38.241945 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:38.242275 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 -100 2 2 0 2 -100 -100 2 -100 -100 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 -100 -100 2 2 -100 2 2 2 -100 -100 2 -100 2 2 2 2 2 -100 -100 -100 -100 -100 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:38.245350 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:38.245673 140054965573440 utils_ner.py:385] guid: test-5\n",
      "I0830 03:05:38.245951 140054965573440 utils_ner.py:386] tokens: [CLS] Unlike all previously known C ##LC ##A family members which consistently en ##code an approximately 125 - k ##D ##a trans ##me ##mb ##rane protein that is c ##lea ##ved to form a he ##tero ##di ##mer of two proteins of approximately 90 and 35 k ##D ##a , the 3 . 6 - k ##b h ##CL ##CA ##3 m ##RNA en ##codes a 37 - k ##D ##a g ##ly ##co ##p ##rote ##in that corresponds to the N - terminal extra ##cellular domain of its ho ##mo ##log ##s . [SEP]\n",
      "I0830 03:05:38.246273 140054965573440 utils_ner.py:387] input_ids: 101 5472 1155 2331 1227 140 12674 1592 1266 1484 1134 10887 4035 13775 1126 2324 8347 118 180 2137 1161 14715 3263 12913 18194 4592 1115 1110 172 19094 5790 1106 1532 170 1119 25710 3309 4027 1104 1160 7865 1104 2324 3078 1105 2588 180 2137 1161 117 1103 124 119 127 118 180 1830 177 26351 11356 1495 182 15654 4035 25634 170 3413 118 180 2137 1161 176 1193 2528 1643 21020 1394 1115 15497 1106 1103 151 118 6020 3908 18091 5777 1104 1157 16358 3702 13791 1116 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:38.246621 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:38.246966 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:38.247311 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 -100 -100 2 2 2 2 2 -100 2 2 2 -100 -100 -100 -100 2 -100 -100 -100 2 2 2 2 -100 -100 2 2 2 2 -100 -100 -100 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 -100 -100 -100 -100 -100 2 -100 -100 -100 2 -100 2 -100 2 2 -100 -100 -100 -100 2 -100 -100 -100 -100 -100 2 2 2 2 2 -100 -100 2 -100 2 2 2 2 -100 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:38.694792 140054965573440 utils_ner.py:134] Saving features into cached file /sbksvol/nikhil/NER_data/Species/loctext/cached_test_BertTokenizer_256\n",
      "I0830 03:05:38.992043 140054965573440 filelock.py:318] Lock 140052191759160 released on /sbksvol/nikhil/NER_data/Species/loctext/cached_test_BertTokenizer_256.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species loctext \n",
      "\n",
      "Sent Count {'B': 125, 'O': 575, 'I': 22} %B-0.21739130434782608 %I-0.03826086956521739 \n",
      "\n",
      "Tag Count {'B': 157, 'O': 13329, 'I': 25} %B-0.01177882811913872 %I-0.0018756095731112612 \n",
      "\n",
      "mean-39.883478260869566, std-15.809693946573097, percentile[25,50,75,90,100]-[ 29.  37.  49.  59. 140.] \n",
      "\n",
      "Species loctext \n",
      "\n",
      "Sent Count {'B': 21, 'O': 85, 'I': 3} %B-0.24705882352941178 %I-0.03529411764705882 \n",
      "\n",
      "Tag Count {'B': 27, 'O': 2019, 'I': 3} %B-0.01337295690936107 %I-0.0014858841010401188 \n",
      "\n",
      "mean-40.83529411764706, std-24.7666032527313, percentile[25,50,75,90,100]-[ 27.  36.  48.  57. 205.] \n",
      "\n",
      "Species loctext \n",
      "\n",
      "Sent Count {'B': 77, 'O': 292, 'I': 17} %B-0.2636986301369863 %I-0.05821917808219178 \n",
      "\n",
      "Tag Count {'B': 92, 'O': 6877, 'I': 18} %B-0.013377926421404682 %I-0.0026174203867965684 \n",
      "\n",
      "mean-40.5958904109589, std-17.76215723628806, percentile[25,50,75,90,100]-[ 28.   37.5  49.   63.9 121. ] \n",
      "\n",
      "num_train_sents, num_dev_sents, num_test_sents =  1486 205 768\n",
      "First 10 words in test data:\n",
      "   sentence_id      words labels\n",
      "0            0       This      O\n",
      "1            0        can      O\n",
      "2            0  translate      O\n",
      "3            0       into      O\n",
      "4            0     global      O\n",
      "5            0    effects      O\n",
      "6            0         on      O\n",
      "7            0   cellular      O\n",
      "8            0     health      O\n",
      "9            0        and      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [3.15896104e+01 1.35133333e+03 3.36972182e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:42.934034 140054965573440 filelock.py:274] Lock 140053157733264 acquired on /sbksvol/nikhil/NER_data/Species/miRNA/cached_train_BertTokenizer_256.lock\n",
      "I0830 03:05:42.935216 140054965573440 utils_ner.py:111] Creating features from dataset file at /sbksvol/nikhil/NER_data/Species/miRNA\n",
      "I0830 03:05:42.973735 140054965573440 utils_ner.py:299] Writing example 0 of 1485\n",
      "I0830 03:05:42.975974 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:42.976575 140054965573440 utils_ner.py:385] guid: train-1\n",
      "I0830 03:05:42.977022 140054965573440 utils_ner.py:386] tokens: [CLS] Blood collection for RNA studies is a new direction . [SEP]\n",
      "I0830 03:05:42.977495 140054965573440 utils_ner.py:387] input_ids: 101 5657 2436 1111 13254 2527 1110 170 1207 2447 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:42.977921 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:42.978326 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:42.978746 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:42.980471 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:42.981111 140054965573440 utils_ner.py:385] guid: train-2\n",
      "I0830 03:05:42.981510 140054965573440 utils_ner.py:386] tokens: [CLS] Gene ##tic studies of longitudinal p ##hen ##otype ##s hold promise for el ##uc ##ida ##ting disease mechanisms and risk , development of therapeutic strategies , and re ##fining selection criteria for clinical trials . [SEP]\n",
      "I0830 03:05:42.981941 140054965573440 utils_ner.py:387] input_ids: 101 9066 2941 2527 1104 23191 185 10436 27172 1116 2080 4437 1111 8468 21977 6859 1916 3653 10748 1105 3187 117 1718 1104 20340 10700 117 1105 1231 24110 4557 9173 1111 7300 7356 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:42.982393 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:42.982863 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:42.983334 140054965573440 utils_ner.py:390] label_ids: -100 2 -100 2 2 2 2 -100 -100 -100 2 2 2 2 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:42.985093 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:42.985510 140054965573440 utils_ner.py:385] guid: train-3\n",
      "I0830 03:05:42.985915 140054965573440 utils_ner.py:386] tokens: [CLS] Gen ##ome - wide array data have been publicly released and updated , and several ne ##uro ##ima ##ging G ##WA ##S have recently been reported examining base ##line magnetic resonance imaging measures as quantitative p ##hen ##otype ##s . [SEP]\n",
      "I0830 03:05:42.986331 140054965573440 utils_ner.py:387] input_ids: 101 9198 6758 118 2043 9245 2233 1138 1151 6783 1308 1105 8054 117 1105 1317 24928 11955 8628 3375 144 11840 1708 1138 3055 1151 2103 13766 2259 2568 8364 20370 14377 5252 1112 25220 185 10436 27172 1116 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:42.986739 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:42.987161 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:42.987559 140054965573440 utils_ner.py:390] label_ids: -100 2 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 2 -100 -100 2 2 2 2 2 2 -100 2 2 2 2 2 2 2 -100 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:42.989339 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:42.989730 140054965573440 utils_ner.py:385] guid: train-4\n",
      "I0830 03:05:42.990124 140054965573440 utils_ner.py:386] tokens: [CLS] Other preliminary investigations include copy number variation in mild cognitive imp ##air ##ment and Alzheimer ' s disease and G ##WA ##S of base ##line c ##ere ##bro ##sp ##inal fluid bio ##mark ##ers and longitudinal changes on magnetic resonance imaging . [SEP]\n",
      "I0830 03:05:42.990545 140054965573440 utils_ner.py:387] input_ids: 101 2189 9889 11041 1511 5633 1295 8516 1107 10496 12176 24034 8341 1880 1105 24278 112 188 3653 1105 144 11840 1708 1104 2259 2568 172 9014 12725 20080 14196 8240 25128 8519 1468 1105 23191 2607 1113 8364 20370 14377 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:42.990947 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:42.991348 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:42.991762 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 -100 2 2 2 -100 -100 2 2 -100 2 -100 -100 -100 -100 2 2 -100 -100 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:42.993988 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:42.994366 140054965573440 utils_ner.py:385] guid: train-5\n",
      "I0830 03:05:42.994748 140054965573440 utils_ner.py:386] tokens: [CLS] The role of the Alzheimer ' s Disease N ##eur ##oi ##maging Initiative Gene ##tics Core is to facilitate the investigation of genetic influences on disease onset and trajectory as reflected in structural , functional , and molecular imaging changes ; fluid bio ##mark ##ers ; and cognitive status . [SEP]\n",
      "I0830 03:05:42.995135 140054965573440 utils_ner.py:387] input_ids: 101 1109 1648 1104 1103 24278 112 188 20012 151 8816 8136 26772 13508 9066 7376 15052 1110 1106 11000 1103 4449 1104 7434 7751 1113 3653 15415 1105 25882 1112 7226 1107 8649 117 8458 117 1105 9546 14377 2607 132 8240 25128 8519 1468 132 1105 12176 2781 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:42.995538 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:42.995942 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:42.996335 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 -100 2 2 -100 -100 -100 2 2 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:44.667452 140054965573440 utils_ner.py:134] Saving features into cached file /sbksvol/nikhil/NER_data/Species/miRNA/cached_train_BertTokenizer_256\n",
      "I0830 03:05:45.287126 140054965573440 filelock.py:318] Lock 140053157733264 released on /sbksvol/nikhil/NER_data/Species/miRNA/cached_train_BertTokenizer_256.lock\n",
      "I0830 03:05:45.481561 140054965573440 filelock.py:274] Lock 140052405942088 acquired on /sbksvol/nikhil/NER_data/Species/miRNA/cached_dev_BertTokenizer_256.lock\n",
      "I0830 03:05:45.483114 140054965573440 utils_ner.py:111] Creating features from dataset file at /sbksvol/nikhil/NER_data/Species/miRNA\n",
      "I0830 03:05:45.488761 140054965573440 utils_ner.py:299] Writing example 0 of 204\n",
      "I0830 03:05:45.492287 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:45.492842 140054965573440 utils_ner.py:385] guid: dev-1\n",
      "I0830 03:05:45.493281 140054965573440 utils_ner.py:386] tokens: [CLS] We report here on a patient who was treated for an at ##y ##pical brain men ##ing ##io ##ma with multiple surge ##ries and multiple sessions of stereo ##ta ##ctic radios ##urger ##y with good control of his brain disease . [SEP]\n",
      "I0830 03:05:45.493669 140054965573440 utils_ner.py:387] input_ids: 101 1284 2592 1303 1113 170 5351 1150 1108 5165 1111 1126 1120 1183 15328 3575 1441 1158 2660 1918 1114 2967 12814 3377 1105 2967 6887 1104 17000 1777 11143 26150 23872 1183 1114 1363 1654 1104 1117 3575 3653 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:45.493984 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:45.494292 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:45.494615 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 0 2 2 2 2 2 2 -100 -100 2 2 -100 -100 -100 2 2 2 -100 2 2 2 2 2 -100 -100 2 -100 -100 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:45.495782 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:45.496062 140054965573440 utils_ner.py:385] guid: dev-2\n",
      "I0830 03:05:45.496327 140054965573440 utils_ner.py:386] tokens: [CLS] Thirteen years after diagnosis , he developed bilateral large sa ##c ##roi ##lia ##c and abdominal meta ##sta ##ses . [SEP]\n",
      "I0830 03:05:45.496643 140054965573440 utils_ner.py:387] input_ids: 101 20404 1201 1170 12645 117 1119 1872 20557 1415 21718 1665 21418 4567 1665 1105 24716 27154 8419 8830 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:45.496938 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:45.497230 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:45.498510 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 2 2 2 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:45.499471 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:45.499743 140054965573440 utils_ner.py:385] guid: dev-3\n",
      "I0830 03:05:45.500000 140054965573440 utils_ner.py:386] tokens: [CLS] Extra ##c ##rani ##al meta ##sta ##ses from brain men ##ing ##io ##mas is a rare , but well - documented entity . [SEP]\n",
      "I0830 03:05:45.500294 140054965573440 utils_ner.py:387] input_ids: 101 18684 1665 23851 1348 27154 8419 8830 1121 3575 1441 1158 2660 7941 1110 170 4054 117 1133 1218 118 8510 9127 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:45.500609 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:45.500935 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:45.501235 140054965573440 utils_ner.py:390] label_ids: -100 2 -100 -100 -100 2 -100 -100 2 2 2 -100 -100 -100 2 2 2 2 2 2 -100 -100 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:45.502418 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:45.502699 140054965573440 utils_ner.py:385] guid: dev-4\n",
      "I0830 03:05:45.502978 140054965573440 utils_ner.py:386] tokens: [CLS] Met ##ast ##ases occur mostly in the lungs , p ##le ##ura and liver , but may also affect l ##ymph nodes and bones . [SEP]\n",
      "I0830 03:05:45.503269 140054965573440 utils_ner.py:387] input_ids: 101 19415 12788 23105 4467 2426 1107 1103 8682 117 185 1513 4084 1105 11911 117 1133 1336 1145 6975 181 25698 15029 1105 6476 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:45.503575 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:45.503871 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:45.504184 140054965573440 utils_ner.py:390] label_ids: -100 2 -100 -100 2 2 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 2 -100 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:45.504996 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:45.505258 140054965573440 utils_ner.py:385] guid: dev-5\n",
      "I0830 03:05:45.506672 140054965573440 utils_ner.py:386] tokens: [CLS] Extra ##ne ##ural meta ##sta ##ses from c ##rani ##al men ##ing ##io ##ma : a case report . [SEP]\n",
      "I0830 03:05:45.507244 140054965573440 utils_ner.py:387] input_ids: 101 18684 1673 12602 27154 8419 8830 1121 172 23851 1348 1441 1158 2660 1918 131 170 1692 2592 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:45.507601 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:45.508054 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:45.508398 140054965573440 utils_ner.py:390] label_ids: -100 2 -100 -100 2 -100 -100 2 2 -100 -100 2 -100 -100 -100 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:45.772805 140054965573440 utils_ner.py:134] Saving features into cached file /sbksvol/nikhil/NER_data/Species/miRNA/cached_dev_BertTokenizer_256\n",
      "I0830 03:05:46.028584 140054965573440 filelock.py:318] Lock 140052405942088 released on /sbksvol/nikhil/NER_data/Species/miRNA/cached_dev_BertTokenizer_256.lock\n",
      "I0830 03:05:46.220378 140054965573440 filelock.py:274] Lock 140052865284864 acquired on /sbksvol/nikhil/NER_data/Species/miRNA/cached_test_BertTokenizer_256.lock\n",
      "I0830 03:05:46.221268 140054965573440 utils_ner.py:111] Creating features from dataset file at /sbksvol/nikhil/NER_data/Species/miRNA\n",
      "I0830 03:05:46.247371 140054965573440 utils_ner.py:299] Writing example 0 of 767\n",
      "I0830 03:05:46.248777 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:46.249106 140054965573440 utils_ner.py:385] guid: test-1\n",
      "I0830 03:05:46.249476 140054965573440 utils_ner.py:386] tokens: [CLS] This can translate into global effects on cellular health and differentiation state . [SEP]\n",
      "I0830 03:05:46.249933 140054965573440 utils_ner.py:387] input_ids: 101 1188 1169 19396 1154 4265 3154 1113 14391 2332 1105 23510 1352 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:46.250357 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:46.250769 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:46.251182 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:46.252886 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:46.253278 140054965573440 utils_ner.py:385] guid: test-2\n",
      "I0830 03:05:46.253670 140054965573440 utils_ner.py:386] tokens: [CLS] Recently , several reports have identified crucial roles for mi ##RNA ##s in controlling the production , differentiation , and health of my ##elin ##ating cells of the ma ##mmal ##ian nervous system . [SEP]\n",
      "I0830 03:05:46.254143 140054965573440 utils_ner.py:387] input_ids: 101 15088 117 1317 3756 1138 3626 10268 3573 1111 1940 15654 1116 1107 9783 1103 1707 117 23510 117 1105 2332 1104 1139 24247 3798 3652 1104 1103 12477 27568 1811 5604 1449 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:46.254642 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:46.255098 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:46.260348 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 2 -100 -100 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:46.262840 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:46.263326 140054965573440 utils_ner.py:385] guid: test-3\n",
      "I0830 03:05:46.263684 140054965573440 utils_ner.py:386] tokens: [CLS] As single mi ##RNA ##s are often predicted to target up to hundreds of individual trans ##cripts , mi ##RNA ##s are able to broadly affect the overall protein expression state of the cell . [SEP]\n",
      "I0830 03:05:46.264100 140054965573440 utils_ner.py:387] input_ids: 101 1249 1423 1940 15654 1116 1132 1510 10035 1106 4010 1146 1106 5229 1104 2510 14715 20506 117 1940 15654 1116 1132 1682 1106 14548 6975 1103 2905 4592 2838 1352 1104 1103 2765 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:46.264480 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:46.264904 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:46.265277 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 -100 -100 2 2 2 2 2 2 2 2 2 2 2 -100 2 2 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:46.267639 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:46.268016 140054965573440 utils_ner.py:385] guid: test-4\n",
      "I0830 03:05:46.270247 140054965573440 utils_ner.py:386] tokens: [CLS] In this review , we will discuss how individual mi ##RNA ##s regulate these various processes , and also how mi ##RNA production in general is required for several stages of my ##elin generation and maintenance . [SEP]\n",
      "I0830 03:05:46.270726 140054965573440 utils_ner.py:387] input_ids: 101 1130 1142 3189 117 1195 1209 6265 1293 2510 1940 15654 1116 16146 1292 1672 5669 117 1105 1145 1293 1940 15654 1707 1107 1704 1110 2320 1111 1317 5251 1104 1139 24247 3964 1105 5972 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:46.271122 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:46.271548 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:46.271944 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:46.274804 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:46.275419 140054965573440 utils_ner.py:385] guid: test-5\n",
      "I0830 03:05:46.275745 140054965573440 utils_ner.py:386] tokens: [CLS] Micro ##RNA ##s ( mi ##RNA ##s ) are a class of small ( approx . 22 n ##t ) non ##co ##ding RNA ##s that are capable of post - transcription ##ally si ##len ##cing m ##RNA ##s that contain sequences complementary to the mi ##RNA ##s ' 7 - to 8 - b ##p ' seed ' sequence . [SEP]\n",
      "I0830 03:05:46.276157 140054965573440 utils_ner.py:387] input_ids: 101 27730 15654 1116 113 1940 15654 1116 114 1132 170 1705 1104 1353 113 26403 119 1659 183 1204 114 1664 2528 3408 13254 1116 1115 1132 4451 1104 2112 118 15416 2716 27466 7836 4869 182 15654 1116 1115 4651 10028 24671 1106 1103 1940 15654 1116 112 128 118 1106 129 118 171 1643 112 6478 112 4954 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:46.276523 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:46.276905 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:46.277266 140054965573440 utils_ner.py:390] label_ids: -100 2 -100 -100 2 2 -100 -100 2 2 2 2 2 2 2 2 2 2 2 -100 2 2 -100 -100 2 -100 2 2 2 2 2 -100 -100 -100 2 -100 -100 2 -100 -100 2 2 2 2 2 2 2 -100 -100 2 2 -100 2 2 -100 -100 -100 2 -100 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:47.226989 140054965573440 utils_ner.py:134] Saving features into cached file /sbksvol/nikhil/NER_data/Species/miRNA/cached_test_BertTokenizer_256\n",
      "I0830 03:05:47.658445 140054965573440 filelock.py:318] Lock 140052865284864 released on /sbksvol/nikhil/NER_data/Species/miRNA/cached_test_BertTokenizer_256.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species miRNA \n",
      "\n",
      "Sent Count {'B': 333, 'O': 1485, 'I': 9} %B-0.22424242424242424 %I-0.006060606060606061 \n",
      "\n",
      "Tag Count {'B': 385, 'O': 36092, 'I': 9} %B-0.010667183863460047 %I-0.0002493627396652998 \n",
      "\n",
      "mean-38.37845117845118, std-16.930464054164826, percentile[25,50,75,90,100]-[ 27.  35.  46.  61. 137.] \n",
      "\n",
      "Species miRNA \n",
      "\n",
      "Sent Count {'B': 55, 'O': 204, 'I': 1} %B-0.2696078431372549 %I-0.004901960784313725 \n",
      "\n",
      "Tag Count {'B': 64, 'O': 5376, 'I': 1} %B-0.011904761904761904 %I-0.00018601190476190475 \n",
      "\n",
      "mean-41.259803921568626, std-19.15305105374691, percentile[25,50,75,90,100]-[ 27.   39.   50.   66.4 125. ] \n",
      "\n",
      "Species miRNA \n",
      "\n",
      "Sent Count {'B': 191, 'O': 767, 'I': 11} %B-0.24902216427640156 %I-0.014341590612777053 \n",
      "\n",
      "Tag Count {'B': 227, 'O': 18971, 'I': 15} %B-0.011965631753729377 %I-0.000790680512360972 \n",
      "\n",
      "mean-40.152542372881356, std-19.244300280228014, percentile[25,50,75,90,100]-[ 27.   37.   49.   65.4 179. ] \n",
      "\n",
      "num_train_sents, num_dev_sents, num_test_sents =  4734 763 2363\n",
      "First 10 words in test data:\n",
      "   sentence_id             words labels\n",
      "0            0         Isolation      O\n",
      "1            0               and      O\n",
      "2            0  characterization      O\n",
      "3            0                of      O\n",
      "4            0               MAT      O\n",
      "5            0             genes      O\n",
      "6            0                in      O\n",
      "7            0               the      O\n",
      "8            0         symbiotic      O\n",
      "9            0        ascomycete      O\n",
      "unique labels: ['B', 'I', 'O'] with weights [17.8801036  19.30980586  0.34574742]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:52.274733 140054965573440 filelock.py:274] Lock 140048696009784 acquired on /sbksvol/nikhil/NER_data/Species/s800/cached_train_BertTokenizer_256.lock\n",
      "I0830 03:05:52.276298 140054965573440 utils_ner.py:111] Creating features from dataset file at /sbksvol/nikhil/NER_data/Species/s800\n",
      "I0830 03:05:52.399313 140054965573440 utils_ner.py:299] Writing example 0 of 4733\n",
      "I0830 03:05:52.401302 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:52.402049 140054965573440 utils_ner.py:385] guid: train-1\n",
      "I0830 03:05:52.402481 140054965573440 utils_ner.py:386] tokens: [CLS] Anti ##mal ##aria ##l drug resistance of P ##las ##mo ##dium f ##al ##ci ##par ##um in India : changes over time and space . [SEP]\n",
      "I0830 03:05:52.402910 140054965573440 utils_ner.py:387] input_ids: 101 8329 7435 11315 1233 3850 4789 1104 153 7580 3702 10876 175 1348 6617 17482 1818 1107 1726 131 2607 1166 1159 1105 2000 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:52.403326 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:52.403726 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:52.404133 140054965573440 utils_ner.py:390] label_ids: -100 2 -100 -100 -100 2 2 2 0 -100 -100 -100 1 -100 -100 -100 -100 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:52.406067 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:52.406611 140054965573440 utils_ner.py:385] guid: train-2\n",
      "I0830 03:05:52.407110 140054965573440 utils_ner.py:386] tokens: [CLS] After the launch of the National Mal ##aria Control Programme in 1953 , the number of malaria cases reported in India fell to an all - time low of 0 * 1 million in 1965 . [SEP]\n",
      "I0830 03:05:52.407469 140054965573440 utils_ner.py:387] input_ids: 101 1258 1103 4286 1104 1103 1305 18880 11315 6342 11512 1107 3185 117 1103 1295 1104 23645 2740 2103 1107 1726 2204 1106 1126 1155 118 1159 1822 1104 121 115 122 1550 1107 2679 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:52.407807 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:52.408136 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:52.408461 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 -100 -100 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:52.409708 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:52.410013 140054965573440 utils_ner.py:385] guid: train-3\n",
      "I0830 03:05:52.410299 140054965573440 utils_ner.py:386] tokens: [CLS] However , the initial success could not be maintained and a re ##su ##rgen ##ce of malaria began in the late 1960s . [SEP]\n",
      "I0830 03:05:52.410635 140054965573440 utils_ner.py:387] input_ids: 101 1438 117 1103 3288 2244 1180 1136 1129 4441 1105 170 1231 6385 16648 2093 1104 23645 1310 1107 1103 1523 3266 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:52.410958 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:52.411279 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:52.411609 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:52.414012 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:52.416058 140054965573440 utils_ner.py:385] guid: train-4\n",
      "I0830 03:05:52.416350 140054965573440 utils_ner.py:386] tokens: [CLS] Resistance of P ##las ##mo ##dium f ##al ##ci ##par ##um to ch ##lor ##o ##quin ##e was first reported in 1973 and increases in anti ##mal ##aria ##l resistance , along with rapid urban ##isation and labour migration , complicated the challenge that India ' s large geographical area and population size already pose for malaria control . [SEP]\n",
      "I0830 03:05:52.416694 140054965573440 utils_ner.py:387] input_ids: 101 15598 1104 153 7580 3702 10876 175 1348 6617 17482 1818 1106 22572 10885 1186 12934 1162 1108 1148 2103 1107 2478 1105 6986 1107 2848 7435 11315 1233 4789 117 1373 1114 6099 3953 5771 1105 8596 10348 117 8277 1103 4506 1115 1726 112 188 1415 11610 1298 1105 1416 2060 1640 14131 1111 23645 1654 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:52.417013 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:52.417324 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:52.417666 140054965573440 utils_ner.py:390] label_ids: -100 2 2 0 -100 -100 -100 1 -100 -100 -100 -100 2 2 -100 -100 -100 -100 2 2 2 2 2 2 2 2 2 -100 -100 -100 2 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:52.419118 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:05:52.419424 140054965573440 utils_ner.py:385] guid: train-5\n",
      "I0830 03:05:52.419702 140054965573440 utils_ner.py:386] tokens: [CLS] Although several institutions have done drug - resistance monitoring in India , a complete analysis of country ##wide data across institutions does not exist . [SEP]\n",
      "I0830 03:05:52.420028 140054965573440 utils_ner.py:387] input_ids: 101 1966 1317 4300 1138 1694 3850 118 4789 9437 1107 1726 117 170 2335 3622 1104 1583 15665 2233 1506 4300 1674 1136 4056 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:52.420341 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:05:52.420677 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:05:52.421022 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:05:58.310878 140054965573440 utils_ner.py:134] Saving features into cached file /sbksvol/nikhil/NER_data/Species/s800/cached_train_BertTokenizer_256\n",
      "I0830 03:06:00.094224 140054965573440 filelock.py:318] Lock 140048696009784 released on /sbksvol/nikhil/NER_data/Species/s800/cached_train_BertTokenizer_256.lock\n",
      "I0830 03:06:00.298257 140054965573440 filelock.py:274] Lock 140052152267048 acquired on /sbksvol/nikhil/NER_data/Species/s800/cached_dev_BertTokenizer_256.lock\n",
      "I0830 03:06:00.300478 140054965573440 utils_ner.py:111] Creating features from dataset file at /sbksvol/nikhil/NER_data/Species/s800\n",
      "I0830 03:06:00.329468 140054965573440 utils_ner.py:299] Writing example 0 of 762\n",
      "I0830 03:06:00.331523 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:06:00.331897 140054965573440 utils_ner.py:385] guid: dev-1\n",
      "I0830 03:06:00.332218 140054965573440 utils_ner.py:386] tokens: [CLS] Mo ##rp ##hol ##ogical and molecular information of a new species of G ##ele ##ia ( C ##ili ##op ##hora , Ka ##ryo ##rel ##ict ##ea ) , with red ##es ##cription ##s of two Kent ##rop ##hor ##os species from China . [SEP]\n",
      "I0830 03:06:00.332583 140054965573440 utils_ner.py:387] input_ids: 101 12556 15615 14084 20946 1105 9546 1869 1104 170 1207 1530 1104 144 11194 1465 113 140 18575 4184 16426 117 14812 26503 9261 17882 4490 114 117 1114 1894 1279 27530 1116 1104 1160 5327 12736 13252 2155 1530 1121 1975 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:00.332950 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:00.333292 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:00.333642 140054965573440 utils_ner.py:390] label_ids: -100 2 -100 -100 -100 2 2 2 2 2 2 2 2 2 -100 -100 2 2 -100 -100 -100 2 2 -100 -100 -100 -100 2 2 2 2 -100 -100 -100 2 2 2 -100 -100 -100 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:06:00.334731 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:06:00.335049 140054965573440 utils_ner.py:385] guid: dev-2\n",
      "I0830 03:06:00.335344 140054965573440 utils_ner.py:386] tokens: [CLS] The morphology and in ##fra ##ci ##lia ##ture of three ka ##ryo ##rel ##ict ##ean c ##ilia ##tes , G ##ele ##ia sin ##ica s ##pec . no ##v . [SEP]\n",
      "I0830 03:06:00.335697 140054965573440 utils_ner.py:387] input_ids: 101 1109 22740 1105 1107 27476 6617 4567 5332 1104 1210 24181 26503 9261 17882 7766 172 26502 3052 117 144 11194 1465 11850 4578 188 25392 119 1185 1964 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:00.336043 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:00.336373 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:00.336719 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 -100 -100 -100 -100 2 2 2 -100 -100 -100 -100 2 -100 -100 2 0 -100 -100 1 -100 1 -100 -100 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:06:00.338415 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:06:00.338727 140054965573440 utils_ner.py:385] guid: dev-3\n",
      "I0830 03:06:00.339025 140054965573440 utils_ner.py:386] tokens: [CLS] and two poorly known Kent ##rop ##hor ##os species , K . fl ##av ##us and K . g ##rac ##ilis , isolated from the inter ##ti ##dal zone of a beach at Qing ##da ##o , China , were investigated . [SEP]\n",
      "I0830 03:06:00.339355 140054965573440 utils_ner.py:387] input_ids: 101 1105 1160 9874 1227 5327 12736 13252 2155 1530 117 148 119 22593 23140 1361 1105 148 119 176 19366 22279 117 6841 1121 1103 9455 3121 6919 4834 1104 170 4640 1120 13838 1810 1186 117 1975 117 1127 10788 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:00.339699 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:00.340027 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:00.340360 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 -100 -100 -100 2 2 0 -100 1 -100 -100 2 0 -100 1 -100 -100 2 2 2 2 2 -100 -100 2 2 2 2 2 2 -100 -100 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:06:00.343350 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:06:00.343666 140054965573440 utils_ner.py:385] guid: dev-4\n",
      "I0830 03:06:00.343971 140054965573440 utils_ner.py:386] tokens: [CLS] G ##ele ##ia sin ##ica s ##pec . no ##v . is distinguished from its con ##gene ##rs by the following combination of characters : body medium - sized and slender - cylindrical ; with a conspicuous pre ##bu ##cca ##l f ##ossa ; 28 - 34 so ##matic kin ##eti ##es ; about 40 short ad ##oral p ##oly ##kin ##eti ##es ; in ##tra ##bu ##cca ##l kin ##ety composed of 25 - 34 di ##kin ##eti ##ds ; par ##oral kin ##eti ##es composed of closely spaced di ##kin ##eti ##ds . [SEP]\n",
      "I0830 03:06:00.344311 140054965573440 utils_ner.py:387] input_ids: 101 144 11194 1465 11850 4578 188 25392 119 1185 1964 119 1110 6019 1121 1157 14255 27054 1733 1118 1103 1378 4612 1104 2650 131 1404 5143 118 6956 1105 11226 118 20684 132 1114 170 21382 3073 7925 19495 1233 175 25637 132 1743 118 3236 1177 10734 15190 26883 1279 132 1164 1969 1603 8050 17536 185 23415 4314 26883 1279 132 1107 4487 7925 19495 1233 15190 20656 2766 1104 1512 118 3236 4267 4314 26883 3680 132 14247 17536 15190 26883 1279 2766 1104 4099 22445 4267 4314 26883 3680 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:00.344658 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:00.344991 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:00.345638 140054965573440 utils_ner.py:390] label_ids: -100 0 -100 -100 1 -100 1 -100 -100 1 -100 -100 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 2 2 -100 -100 2 2 -100 -100 2 2 2 2 2 -100 -100 -100 2 -100 2 2 -100 -100 2 -100 2 -100 -100 2 2 2 2 2 -100 2 -100 -100 -100 -100 2 2 -100 -100 -100 -100 2 -100 2 2 2 -100 -100 2 -100 -100 -100 2 2 -100 2 -100 -100 2 2 2 2 2 -100 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:06:00.347188 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:06:00.347503 140054965573440 utils_ner.py:385] guid: dev-5\n",
      "I0830 03:06:00.347795 140054965573440 utils_ner.py:386] tokens: [CLS] The comparison with similar con ##gene ##rs clearly supports the validity of this new species based on m ##or ##phological and small subunit ( SS ##U ) r ##RNA gene sequence data . [SEP]\n",
      "I0830 03:06:00.348112 140054965573440 utils_ner.py:387] input_ids: 101 1109 7577 1114 1861 14255 27054 1733 3817 6253 1103 17782 1104 1142 1207 1530 1359 1113 182 1766 26920 1105 1353 27555 113 6663 2591 114 187 15654 5565 4954 2233 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:06:00.348431 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:00.348757 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:00.349083 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 2 2 -100 -100 2 -100 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:06:01.400467 140054965573440 utils_ner.py:134] Saving features into cached file /sbksvol/nikhil/NER_data/Species/s800/cached_dev_BertTokenizer_256\n",
      "I0830 03:06:01.857876 140054965573440 filelock.py:318] Lock 140052152267048 released on /sbksvol/nikhil/NER_data/Species/s800/cached_dev_BertTokenizer_256.lock\n",
      "I0830 03:06:02.056797 140054965573440 filelock.py:274] Lock 140053244370112 acquired on /sbksvol/nikhil/NER_data/Species/s800/cached_test_BertTokenizer_256.lock\n",
      "I0830 03:06:02.058378 140054965573440 utils_ner.py:111] Creating features from dataset file at /sbksvol/nikhil/NER_data/Species/s800\n",
      "I0830 03:06:02.124759 140054965573440 utils_ner.py:299] Writing example 0 of 2362\n",
      "I0830 03:06:02.126476 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:06:02.126833 140054965573440 utils_ner.py:385] guid: test-1\n",
      "I0830 03:06:02.127238 140054965573440 utils_ner.py:386] tokens: [CLS] Is ##ola ##tion and characterization of MA ##T genes in the s ##ym ##biotic as ##com ##y ##ce ##te Tu ##ber me ##lan ##os ##por ##um . [SEP]\n",
      "I0830 03:06:02.127698 140054965573440 utils_ner.py:387] input_ids: 101 2181 5326 2116 1105 27419 1104 9960 1942 9077 1107 1103 188 17162 22400 1112 8178 1183 2093 1566 17037 3169 1143 4371 2155 18876 1818 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:02.128118 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:02.128594 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:02.129008 140054965573440 utils_ner.py:390] label_ids: -100 2 -100 -100 2 2 2 2 -100 2 2 2 2 -100 -100 2 -100 -100 -100 -100 0 -100 1 -100 -100 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:06:02.130071 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:06:02.130475 140054965573440 utils_ner.py:385] guid: test-2\n",
      "I0830 03:06:02.130865 140054965573440 utils_ner.py:386] tokens: [CLS] * The genome of Tu ##ber me ##lan ##os ##por ##um has recently been sequence ##d . [SEP]\n",
      "I0830 03:06:02.131364 140054965573440 utils_ner.py:387] input_ids: 101 115 1109 15519 1104 17037 3169 1143 4371 2155 18876 1818 1144 3055 1151 4954 1181 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:02.131872 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:02.132354 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:06:02.132842 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 0 -100 1 -100 -100 -100 -100 2 2 2 2 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:06:02.134475 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:06:02.134933 140054965573440 utils_ner.py:385] guid: test-3\n",
      "I0830 03:06:02.135360 140054965573440 utils_ner.py:386] tokens: [CLS] Here , we used this information to identify genes involved in the reproductive processes of this edible fungus . [SEP]\n",
      "I0830 03:06:02.135831 140054965573440 utils_ner.py:387] input_ids: 101 3446 117 1195 1215 1142 1869 1106 6183 9077 2017 1107 1103 17505 5669 1104 1142 24525 18142 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:02.136298 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:02.136754 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:02.137189 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:06:02.140639 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:06:02.141534 140054965573440 utils_ner.py:385] guid: test-4\n",
      "I0830 03:06:02.142040 140054965573440 utils_ner.py:386] tokens: [CLS] The sequence ##d strain ( Mel ##28 ) possesses only one of the two master genes required for mating , that is , the gene that codes for the high mobility group ( HM ##G ) transcription factor ( MA ##T ##1 - 2 - 1 ) , whereas it lacks the gene that codes for the protein containing the alpha - box - domain ( MA ##T ##1 - 1 - 1 ) , suggesting that this fungus is he ##tero ##thal ##lic . [SEP]\n",
      "I0830 03:06:02.142431 140054965573440 utils_ner.py:387] input_ids: 101 1109 4954 1181 10512 113 11637 24606 114 15614 1178 1141 1104 1103 1160 3283 9077 2320 1111 16982 117 1115 1110 117 1103 5565 1115 9812 1111 1103 1344 16178 1372 113 25259 2349 114 15416 5318 113 9960 1942 1475 118 123 118 122 114 117 6142 1122 14756 1103 5565 1115 9812 1111 1103 4592 4051 1103 11164 118 2884 118 5777 113 9960 1942 1475 118 122 118 122 114 117 8783 1115 1142 18142 1110 1119 25710 17939 8031 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:02.142774 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:02.143138 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:02.143493 140054965573440 utils_ner.py:390] label_ids: -100 2 2 -100 2 2 2 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 2 -100 -100 -100 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 2 -100 -100 -100 -100 -100 -100 2 2 2 2 2 2 2 2 -100 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 03:06:02.145366 140054965573440 utils_ner.py:384] *** Example ***\n",
      "I0830 03:06:02.145692 140054965573440 utils_ner.py:385] guid: test-5\n",
      "I0830 03:06:02.146286 140054965573440 utils_ner.py:386] tokens: [CLS] * A PC ##R - based approach was initially employed to screen t ##ruff ##les for the presence of the MA ##T ##1 - 2 - 1 gene and am ##p ##lify the conserved regions flank ##ing the mating type ( MA ##T ) lo ##cus . [SEP]\n",
      "I0830 03:06:02.146675 140054965573440 utils_ner.py:387] input_ids: 101 115 138 7054 2069 118 1359 3136 1108 2786 4071 1106 3251 189 17669 2897 1111 1103 2915 1104 1103 9960 1942 1475 118 123 118 122 5565 1105 1821 1643 22881 1103 21996 4001 12509 1158 1103 16982 2076 113 9960 1942 114 25338 6697 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:02.147032 140054965573440 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:02.147377 140054965573440 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0830 03:06:02.147748 140054965573440 utils_ner.py:390] label_ids: -100 2 2 2 -100 -100 -100 2 2 2 2 2 2 2 -100 -100 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 2 2 2 -100 -100 2 2 2 2 -100 2 2 2 2 2 -100 2 2 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0830 03:06:05.255000 140054965573440 utils_ner.py:134] Saving features into cached file /sbksvol/nikhil/NER_data/Species/s800/cached_test_BertTokenizer_256\n",
      "I0830 03:06:06.207153 140054965573440 filelock.py:318] Lock 140053244370112 released on /sbksvol/nikhil/NER_data/Species/s800/cached_test_BertTokenizer_256.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species s800 \n",
      "\n",
      "Sent Count {'B': 1441, 'O': 4731, 'I': 1051} %B-0.30458676812513213 %I-0.22215176495455508 \n",
      "\n",
      "Tag Count {'B': 2188, 'O': 113151, 'I': 2026} %B-0.01933699216091771 %I-0.017905277019204427 \n",
      "\n",
      "mean-38.176420874709486, std-18.087608059138077, percentile[25,50,75,90,100]-[ 26.  35.  47.  61. 169.] \n",
      "\n",
      "Species s800 \n",
      "\n",
      "Sent Count {'B': 263, 'O': 762, 'I': 196} %B-0.3451443569553806 %I-0.2572178477690289 \n",
      "\n",
      "Tag Count {'B': 406, 'O': 18777, 'I': 352} %B-0.0216221973691218 %I-0.018746338605741066 \n",
      "\n",
      "mean-39.62992125984252, std-17.595246531911236, percentile[25,50,75,90,100]-[ 28.   37.   49.   61.9 129. ] \n",
      "\n",
      "Species s800 \n",
      "\n",
      "Sent Count {'B': 724, 'O': 2361, 'I': 523} %B-0.30664972469292673 %I-0.2215163066497247 \n",
      "\n",
      "Tag Count {'B': 1074, 'O': 56250, 'I': 964} %B-0.019093333333333334 %I-0.017137777777777776 \n",
      "\n",
      "mean-37.82895850973751, std-17.6196634445019, percentile[25,50,75,90,100]-[ 26.  36.  47.  59. 189.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_name = [\"linneaus\",\"loctext\",\"miRNA\",\"s800\"]\n",
    "for x in data_name:\n",
    "    get_nums(\"Species\",x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputFeatures(input_ids=[101, 25549, 118, 9618, 2765, 26258, 113, 6820, 12122, 114, 113, 2891, 1559, 1495, 118, 153, 2036, 132, 7642, 1813, 2107, 15016, 117, 1727, 4494, 117, 1756, 117, 1244, 1311, 114, 1108, 1982, 1113, 170, 12556, 2271, 2858, 113, 27688, 18778, 1891, 117, 3144, 6266, 117, 4369, 117, 1244, 1311, 114, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[-100, 2, -100, -100, 2, 2, 2, 2, -100, -100, 2, 0, -100, -100, -100, -100, -100, 2, 2, -100, -100, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, 2, 2, -100, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100])\n",
      "['[CLS]', 'Flow', '-', 'activated', 'cell', 'sorting', '(', 'FA', '##CS', ')', '(', 'CD', '##7', '##3', '-', 'P', '##E', ';', 'Ph', '##ar', '##M', '##ingen', ',', 'San', 'Diego', ',', 'California', ',', 'United', 'States', ')', 'was', 'performed', 'on', 'a', 'Mo', '##F', '##lo', '(', 'Cy', '##tom', '##ation', ',', 'Fort', 'Collins', ',', 'Colorado', ',', 'United', 'States', ')', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.__getitem__(15))\n",
    "print(tokenizer.convert_ids_to_tokens(train_dataset.__getitem__(15).input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing weights with xargs {'tf': False, 'top_model': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertNERTopModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (top_layers): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): FullyConnectedLayers(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=250, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
       "        (3): Dropout(p=0.2, inplace=False)\n",
       "        (4): Linear(in_features=250, out_features=3, bias=True)\n",
       "        (5): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.models_enum import ModelsType\n",
    "import models_factory\n",
    "import importlib\n",
    "importlib.reload(models_factory)\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizer\n",
    ")\n",
    "\n",
    "# config = BertConfig.from_json_file(\n",
    "#         \"/sbksvol/nikhil/model/bert_pt\")\n",
    "# config = BertConfig.from_json_file(\n",
    "#         \"/sbksvol/nikhil/model/biobert_v1.0_pubmed_pmc/bert_config.json\")\n",
    "\n",
    "m_path = \"/sbksvol/nikhil/model/biobert_v1.0_pubmed_pmc/\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "        \"/sbksvol/nikhil/model/biobert_v1.0_pubmed_pmc\",\n",
    "        cache_dir=\".\",\n",
    "        do_lower_case=params[\"LOWER_CASE\"]\n",
    "    )\n",
    "\n",
    "config = BertConfig.from_pretrained(\n",
    "        \"/sbksvol/nikhil/model/biobert_v1.0_pubmed_pmc/\",\n",
    "        num_labels=num_labels,\n",
    "        id2label=label_map,\n",
    "        label2id={label: i for i, label in enumerate(labels)},\n",
    "        cache_dir=\".\"\n",
    "    )\n",
    "\n",
    "model = models_factory.get_model(\n",
    "        model_path=\"/sbksvol/nikhil/Cellline_cellfinder_baseline1_v10_lr5_6_wd3_ft5_pt5_0208_1/model_output_test/checkpoint-572/\",\n",
    "        cache_dir=\".\",\n",
    "        config=None,\n",
    "        model_type=ModelsType.BASELINE,xargs={'tf':False,'top_model': 1})\n",
    "# model = BertNERCRFFCN.from_pretrained(\n",
    "#             \"/sbksvol/nikhil/gene_cell_fcn_crf_lr5_6_wd3/model_output_test/checkpoint-2613/\",\n",
    "#             xargs = {}\n",
    "#         )\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0816 23:19:46.890347 140585640822592 filelock.py:274] Lock 140580808862632 acquired on /sbksvol/nikhil/NER_data/Cellline/cellfinder/cached_test_BertTokenizer_256.lock\n",
      "I0816 23:19:46.891447 140585640822592 utils_ner.py:111] Creating features from dataset file at /sbksvol/nikhil/NER_data/Cellline/cellfinder\n",
      "I0816 23:19:46.932114 140585640822592 utils_ner.py:299] Writing example 0 of 588\n",
      "I0816 23:19:46.934863 140585640822592 utils_ner.py:384] *** Example ***\n",
      "I0816 23:19:46.936062 140585640822592 utils_ner.py:385] guid: test-1\n",
      "I0816 23:19:46.936953 140585640822592 utils_ner.py:386] tokens: [CLS] Background ##U ##sing antibodies to specific protein anti ##gens is the method of choice to assign and identify cell lineage through simultaneous analysis of surface molecules and in ##tra ##cellular markers . [SEP]\n",
      "I0816 23:19:46.938165 140585640822592 utils_ner.py:387] input_ids: 101 24570 2591 4253 26491 1106 2747 4592 2848 21144 1110 1103 3442 1104 3026 1106 27430 1105 6183 2765 14209 1194 19648 3622 1104 2473 10799 1105 1107 4487 18091 18004 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0816 23:19:46.940736 140585640822592 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0816 23:19:46.941858 140585640822592 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0816 23:19:46.942943 140585640822592 utils_ner.py:390] label_ids: -100 2 -100 -100 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0816 23:19:46.951677 140585640822592 utils_ner.py:384] *** Example ***\n",
      "I0816 23:19:46.952538 140585640822592 utils_ner.py:385] guid: test-2\n",
      "I0816 23:19:46.953365 140585640822592 utils_ner.py:386] tokens: [CLS] Em ##b ##ryo ##nic stem cell research can be benefited from using antibodies specific to transcription ##al factors / markers that contribute to the \" \" \" \" stem ##ness \" \" \" \" p ##hen ##otype or critical for cell lineage . Results In this report , we have developed and valid ##ated antibodies ( either mon ##oc ##lon ##al or p ##oly ##c ##lon ##al ) specific to human em ##b ##ryo ##nic stem cell anti ##gens and early differentiation transcription ##al factors / markers that are critical for cell differentiation into definite lineage . Con ##c ##lusion ##T ##hes ##e antibodies enable stem cell biologist ##s to convenient ##ly identify stem cell characteristics and to quantitative ##ly assess differentiation . [SEP]\n",
      "I0816 23:19:46.954277 140585640822592 utils_ner.py:387] input_ids: 101 18653 1830 26503 7770 8175 2765 1844 1169 1129 21495 1121 1606 26491 2747 1106 15416 1348 5320 120 18004 1115 8681 1106 1103 107 107 107 107 8175 1757 107 107 107 107 185 10436 27172 1137 3607 1111 2765 14209 119 16005 1130 1142 2592 117 1195 1138 1872 1105 9221 2913 26491 113 1719 19863 13335 4934 1348 1137 185 23415 1665 4934 1348 114 2747 1106 1769 9712 1830 26503 7770 8175 2765 2848 21144 1105 1346 23510 15416 1348 5320 120 18004 1115 1132 3607 1111 2765 23510 1154 16428 14209 119 16752 1665 17855 1942 16090 1162 26491 9396 8175 2765 24742 1116 1106 14785 1193 6183 8175 2765 5924 1105 1106 25220 1193 15187 23510 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0816 23:19:46.954924 140585640822592 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0816 23:19:46.955526 140585640822592 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0816 23:19:46.956144 140585640822592 utils_ner.py:390] label_ids: -100 2 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 -100 2 -100 -100 2 2 2 2 2 -100 -100 -100 2 -100 2 -100 -100 -100 2 -100 -100 2 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 -100 2 2 -100 2 -100 -100 -100 2 2 -100 -100 -100 -100 2 2 2 2 2 -100 -100 -100 2 2 2 -100 2 2 2 2 -100 2 -100 -100 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 2 2 2 2 2 -100 2 2 -100 2 2 2 2 2 2 2 -100 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0816 23:19:46.962878 140585640822592 utils_ner.py:384] *** Example ***\n",
      "I0816 23:19:46.963401 140585640822592 utils_ner.py:385] guid: test-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0816 23:19:46.963835 140585640822592 utils_ner.py:386] tokens: [CLS] Although the stem cell concept was introduced decades ago , to date , stem cells can only be defined functional ##ly , not m ##or ##phological ##ly or p ##hen ##otypic ##ally . [SEP]\n",
      "I0816 23:19:46.964307 140585640822592 utils_ner.py:387] input_ids: 101 1966 1103 8175 2765 3400 1108 2234 4397 2403 117 1106 2236 117 8175 3652 1169 1178 1129 3393 8458 1193 117 1136 182 1766 26920 1193 1137 185 10436 27202 2716 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0816 23:19:46.964771 140585640822592 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0816 23:19:46.965239 140585640822592 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0816 23:19:46.965721 140585640822592 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 2 2 2 -100 -100 -100 2 2 -100 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0816 23:19:46.966615 140585640822592 utils_ner.py:384] *** Example ***\n",
      "I0816 23:19:46.967023 140585640822592 utils_ner.py:385] guid: test-4\n",
      "I0816 23:19:46.967425 140585640822592 utils_ner.py:386] tokens: [CLS] Two functions define stem cells . [SEP]\n",
      "I0816 23:19:46.967913 140585640822592 utils_ner.py:387] input_ids: 101 1960 4226 9410 8175 3652 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0816 23:19:46.968363 140585640822592 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0816 23:19:46.968856 140585640822592 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0816 23:19:46.969337 140585640822592 utils_ner.py:390] label_ids: -100 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0816 23:19:46.970833 140585640822592 utils_ner.py:384] *** Example ***\n",
      "I0816 23:19:46.971262 140585640822592 utils_ner.py:385] guid: test-5\n",
      "I0816 23:19:46.971671 140585640822592 utils_ner.py:386] tokens: [CLS] First ##ly , they are self - renew ##ing , thus able to prop ##aga ##te to generate additional stem cells . [SEP]\n",
      "I0816 23:19:46.972162 140585640822592 utils_ner.py:387] input_ids: 101 1752 1193 117 1152 1132 2191 118 23421 1158 117 2456 1682 1106 21146 15446 1566 1106 9509 2509 8175 3652 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0816 23:19:46.972633 140585640822592 utils_ner.py:388] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0816 23:19:46.973125 140585640822592 utils_ner.py:389] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0816 23:19:46.973642 140585640822592 utils_ner.py:390] label_ids: -100 2 -100 2 2 2 2 -100 -100 -100 2 2 2 2 2 -100 -100 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "I0816 23:19:48.258299 140585640822592 utils_ner.py:134] Saving features into cached file /sbksvol/nikhil/NER_data/Cellline/cellfinder/cached_test_BertTokenizer_256\n",
      "I0816 23:19:48.569577 140585640822592 filelock.py:318] Lock 140580808862632 released on /sbksvol/nikhil/NER_data/Cellline/cellfinder/cached_test_BertTokenizer_256.lock\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 76.0%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _      0.975     0.622     0.760       127\n",
      "\n",
      "   micro avg      0.975     0.622     0.760       127\n",
      "   macro avg      0.975     0.622     0.760       127\n",
      "weighted avg      0.975     0.622     0.760       127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from run_v2 import run_test\n",
    "from transformers import Trainer\n",
    "from models.models_enum import ModelsType\n",
    "\n",
    "model.to('cuda')\n",
    "trainer = Trainer(model=model)\n",
    "params['model_type']=ModelsType.BASELINE\n",
    "\n",
    "test_dataset = NerDataset(\n",
    "    data_dir=data_args['data_dir'],\n",
    "    tokenizer=tokenizer,\n",
    "    labels=labels,\n",
    "    model_type=config.model_type,\n",
    "    max_seq_length=data_args['max_seq_length'],\n",
    "    overwrite_cache=data_args['overwrite_cache'],  # True\n",
    "    mode=Split.test, data_size=100)\n",
    "\n",
    "run_test(trainer,model,test_dataset, test_df,label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel, BertModel\n",
    "bert = BertModel(config, add_pooling_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_dataset.__getitem__(0)\n",
    "output = bert(torch.tensor(x.input_ids).unsqueeze(0),\n",
    "              attention_mask=torch.tensor(x.attention_mask).unsqueeze(0),\n",
    "              token_type_ids=torch.tensor(x.token_type_ids).unsqueeze(0),\n",
    "              output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "torch.Size([1, 256, 768])\n",
      "torch.Size([1, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "print(len(output.hidden_states))\n",
    "print(output.hidden_states[2].shape)\n",
    "print(output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(model=model)\n",
    "best_t =  trainer.hyperparameter_search(backend=\"ray\",\n",
    "    # Choose among many libraries:\n",
    "    # https://docs.ray.io/en/latest/tune/api_docs/suggestion.html\n",
    "    search_alg=HyperOptSearch(),\n",
    "    # Choose among schedulers:\n",
    "    # https://docs.ray.io/en/latest/tune/api_docs/schedulers.html\n",
    "    scheduler=AsyncHyperBand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 76.0%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _      0.975     0.622     0.760       127\n",
      "\n",
      "   micro avg      0.975     0.622     0.760       127\n",
      "   macro avg      0.975     0.622     0.760       127\n",
      "weighted avg      0.975     0.622     0.760       127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import run_v2\n",
    "import importlib\n",
    "importlib.reload(run_v2)\n",
    "run_v2.params = params\n",
    "run_v2.run_test(trainer,model,test_dataset, test_df,label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, gt = run_v2.get_predictions(trainer, model, test_dataset, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588 49\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for i,curr in enumerate(gt):\n",
    "    if 'B' in gt[i]:\n",
    "        #print(gt[i],pred[i])\n",
    "        cnt+=1\n",
    "print(len(gt),cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O' 'O' 'O' ... 'O' 'O' 'O']\n"
     ]
    }
   ],
   "source": [
    "print(np.hstack(gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   80,     0,    47],\n",
       "       [    0,     0,     2],\n",
       "       [    1,     0, 18763]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(np.hstack(gt), np.hstack(pred), labels=['B','I','O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['These']\n",
      "['results']\n",
      "['are']\n",
      "['summarized']\n",
      "['in']\n",
      "['Table']\n",
      "['2', '.', 'Table']\n",
      "['2', '##S', '##um', '##mar', '##y']\n",
      "['of']\n",
      "['antibodies']\n",
      "['detection']\n",
      "['in']\n",
      "['E', '##S']\n",
      "['and']\n",
      "['E', '##B']\n",
      "['samples', '.', 'Anti', '##body', '##ES', '##E', '##BR', '##ea', '##ct', '##ivity']\n",
      "['to']\n",
      "['mouse', '##I', '##so', '##type']\n",
      "['of']\n",
      "['mon', '##oc', '##lon', '##al']\n",
      "['anti', '##body']\n",
      "['(']\n",
      "['C', '##lone']\n",
      "['No', '.', ')', 'G', '##t']\n",
      "['×']\n",
      "['h', '##B', '##rac', '##hy', '##ury']\n",
      "['No', '##Y', '##es', '##NT', '*', 'Ms']\n",
      "['×']\n",
      "['h', '##DP', '##PA', '##5', '##Y', '##es', '##NT', '*', 'N', '##T', '*', 'N', '##D', '*', 'G', '##t']\n",
      "['×']\n",
      "['h', '##GA', '##TA', '##6', '##N', '##o', '##Y', '##es', '##NT', '*', 'G', '##t']\n",
      "['×']\n",
      "['h', '##N', '##ano', '##g', '##Y', '##es', '##D', '##own', '##N', '##o', '##G', '##t']\n",
      "['×']\n",
      "['h', '##O', '##ct']\n",
      "['3', '/', '4', '##Y', '##es', '##D', '##own', '##Y', '##es', '##G', '##t']\n",
      "['×']\n",
      "['h', '##PD', '##X', '-', '1', '##N', '##o', '##N', '##o', '##Y', '##es', '##G', '##t']\n",
      "['×']\n",
      "['h', '##SO', '##X', '##17', '##N', '##o', '##Y', '##es', '##Y', '##es', '##Ms']\n",
      "['×']\n",
      "['h', '##CD', '##9', '##Y', '##es', '##N', '##o', '##M', '##ini', '##mal', '##M', '##ouse']\n",
      "['I', '##g', '##G', '##2', '##B']\n",
      "['(']\n",
      "['clone']\n",
      "['209', '##30', '##6', ')', 'Ms']\n",
      "['×']\n",
      "['h', '##E', '-', 'ca', '##dh', '##eri', '##n', '##Y', '##es']\n",
      "['No', '##M', '##ini', '##mal', '##M', '##ouse']\n",
      "['I', '##g', '##G', '##2', '##B']\n",
      "['(']\n",
      "['clone']\n",
      "['1802', '##24', ')', 'Ms']\n",
      "['×']\n",
      "['h', '##GA', '##TA', '##1', '##N', '##o', '##Y', '##es', '##NT', '*', 'Rat']\n",
      "['I', '##g', '##G', '##2', '##B']\n",
      "['(']\n",
      "['clone']\n",
      "['234', '##7', '##32', ')', 'Ms']\n",
      "['×']\n",
      "['h', '##PO', '##D', '##X', '##L', '##Y', '##es', '##N', '##o', '##N', '##o', '##M', '##ouse']\n",
      "['I', '##g', '##G', '##2', '##A']\n",
      "['(']\n",
      "['clone']\n",
      "['222', '##32', '##8', ')', 'Ms']\n",
      "['×']\n",
      "['h', '##SO', '##X', '##2', '##Y', '##es', '##Y', '##es', '##Y', '##es', '##M', '##ouse']\n",
      "['I', '##g', '##G', '##2', '##A']\n",
      "['(']\n",
      "['clone']\n",
      "['245', '##6', '##10']\n",
      "[')', '*', 'N', '##T']\n",
      "[',']\n",
      "['Not']\n",
      "['tested']\n",
      "[';']\n",
      "['N', '##D']\n",
      "[',']\n",
      "['Not']\n",
      "['determined']\n",
      "['.']\n"
     ]
    }
   ],
   "source": [
    "sent = ['These', 'results', 'are', 'summarized', 'in', 'Table', '2.Table', '2Summary', 'of', 'antibodies', 'detection', 'in', 'ES', 'and', 'EB', 'samples.AntibodyESEBReactivity', 'to', 'mouseIsotype', 'of', 'monoclonal', 'antibody', '(', 'Clone', 'No.)Gt', '×', 'hBrachyury', 'NoYesNT*Ms', '×', 'hDPPA5YesNT*NT*ND*Gt', '×', 'hGATA6NoYesNT*Gt', '×', 'hNanogYesDownNoGt', '×', 'hOct', '3/4YesDownYesGt', '×', 'hPDX-1NoNoYesGt', '×', 'hSOX17NoYesYesMs', '×', 'hCD9YesNoMinimalMouse', 'IgG2B', '(', 'clone', '209306)Ms', '×', 'hE-cadherinYes', 'NoMinimalMouse', 'IgG2B', '(', 'clone', '180224)Ms', '×', 'hGATA1NoYesNT*Rat', 'IgG2B', '(', 'clone', '234732)Ms', '×', 'hPODXLYesNoNoMouse', 'IgG2A', '(', 'clone', '222328)Ms', '×', 'hSOX2YesYesYesMouse', 'IgG2A', '(', 'clone', '245610', ')*NT', ',', 'Not', 'tested', ';', 'ND', ',', 'Not', 'determined', '.']\n",
    "for word in sent:\n",
    "    print(tokenizer.tokenize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "print(pred.predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, label_ids, metrics = pred\n",
    "preds=np.argmax(output,axis=-1)\n",
    "batch_size, seq_len = preds.shape\n",
    "\n",
    "# preds -> list of token-level predictions shape = (batch_size, seq_len)\n",
    "preds_list = [[] for _ in range(batch_size)]\n",
    "for i in range(batch_size):\n",
    "    for j in range(len(preds[i])):\n",
    "        # ignore pad_tokens\n",
    "        if label_ids[i, j] != torch.nn.CrossEntropyLoss().ignore_index:\n",
    "            preds_list[i].append(label_map[preds[i][j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-67a9252f5624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "x=label_ids[0,:len(preds_list[0])]\n",
    "print(preds_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_combiner(df):\n",
    "        # 'words' and 'labels' are the column names in the CSV file\n",
    "        def tupple_function(x): return [(w, t) for w, t in zip(x[\"words\"].values.tolist(),\n",
    "                                                               x[\"labels\"].values.tolist())]\n",
    "        grouped = df.groupby(\"sentence_id\").apply(tupple_function)\n",
    "        return [s for s in grouped]\n",
    "\n",
    "testing_sentences = sentences_combiner(test_df)\n",
    "test_labels = [[w[1] for w in s] for s in testing_sentences]\n",
    "test_tokens = [[w[0] for w in s] for s in testing_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['BackgroundUsing', 'antibodies', 'to', 'specific', 'protein', 'antigens', 'is', 'the', 'method', 'of', 'choice', 'to', 'assign', 'and', 'identify', 'cell', 'lineage', 'through', 'simultaneous', 'analysis', 'of', 'surface', 'molecules', 'and', 'intracellular', 'markers', '.']\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[0])\n",
    "print(test_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"B\",\n",
      "    \"1\": \"I\",\n",
      "    \"2\": \"O\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B\": 0,\n",
      "    \"I\": 1,\n",
      "    \"O\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "PreTrainedTokenizer(name_or_path='dmis-lab/biobert-base-cased-v1.1', vocab_size=28996, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "print(config)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.bert.encoder.layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28996\n",
      "28996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['HP', '##WR', '##12', '##P', '##14', '##2']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer.add_tokens(list(train_df[train_df['labels']=='B']['words'].unique()))\n",
    "print(len(tokenizer))\n",
    "print(len(tokenizer.vocab))\n",
    "tokenizer.tokenize(\"HPWR12P142\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b5ad8ee515e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'B'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "for word in list(train_df[train_df['labels']=='B']['words'].unique()):\n",
    "    print(tokenizer.tokenize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(29363, 768)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2        0.1        0.06666667]\n"
     ]
    }
   ],
   "source": [
    "temp_df.groupby('labels')['labels'].count()['O']\n",
    "a = 5*np.array([1,2,3])\n",
    "print(1/a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_id         15\n",
       "words          CD73-PE\n",
       "labels               B\n",
       "Name: 548, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.loc[548]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sentence_id         words labels\n",
      "548             15       CD73-PE      B\n",
      "720             21        TGF-β3      B\n",
      "936             34          VCAM      B\n",
      "938             34        STRO-1      B\n",
      "940             34  ICAM-1(CD54)      B\n",
      "...            ...           ...    ...\n",
      "35273         1237          Oct4      B\n",
      "35275         1237         Nanog      B\n",
      "35279         1238         FOXa2      B\n",
      "35281         1238        HNF3B)      B\n",
      "35283         1238     Brachyury      B\n",
      "\n",
      "[848 rows x 3 columns]\n",
      "0        False\n",
      "1        False\n",
      "2        False\n",
      "3        False\n",
      "4        False\n",
      "         ...  \n",
      "35527    False\n",
      "35528    False\n",
      "35529    False\n",
      "35530    False\n",
      "35531    False\n",
      "Name: labels, Length: 35532, dtype: bool\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['CD73-PE', 'TGF-β3', 'VCAM', 'STRO-1', 'ICAM-1(CD54)', 'CD105',\n",
       "       'CD29', 'MF20', 'CD73', 'CD44', 'ALCAM(CD166', 'MyoD', 'nestin',\n",
       "       'vimentin', 'alpha', 'fast-switch', 'pan-cytokeratin', 'human',\n",
       "       'phosphatase', 'extracellular', 'proteoglycans', 'CD73+',\n",
       "       'CD105(SH2', 'CD106', 'CD29(integrin', 'β1', 'ICAM', 'CD34',\n",
       "       'CD45', 'CD14', 'pancytokeratin', 'desmin', 'Housekeeping',\n",
       "       'DSC54', 'neuropilin', 'hepatocyte', 'forkhead', 'notch', 'PPARγ',\n",
       "       'collagen', 'aggrecan', 'β-glycerolphosphate', 'bone-specific',\n",
       "       'bone', 'MyoD+', 'myosin', 'heavy', 'myogenin', 'Nanog', 'Oct-4',\n",
       "       'CD4', 'CCR5', 'CXCR4', 'HLA-DR', 'B7.1', 'GFP', 'siRNAs', 'bFGF',\n",
       "       'GM-CSF', 'M-CSF', 'derived', 'class', 'EGFP', 'CD34)', 'PECY5',\n",
       "       'MHC', 'LPS', 'IL-1', 'TNF-α', 'LPS.', 'TNFα', 'GFP-ES', 'MHCII',\n",
       "       'B7', 'molecules', 'anti-HIV', 'collagenase', 'CD34+',\n",
       "       'trypsin/EDTA', 'M-CSF.', 'PE-CD14', 'PE-HLA-DR', 'PECY5-CD4',\n",
       "       'PECY5-CCR5', 'PECY5-CXCR4', 'costimulatory', 'GFP-alone',\n",
       "       'anti-B7.1', 'Oct4', 'Myf-5', 'delta-Notch', 'Myf5', 'M-cadherin',\n",
       "       'Paired', 'Pax7', 'embryonic', 'desmin+', 'Notch',\n",
       "       'desmin+/Myf5+/BrdU+', 'Ki67', 'Oct4+', 'M-cadherin−/desmin+',\n",
       "       'M-cadherin−/desmin−', 'M-cadherin+/desmin+', 'desmin+/BrdU+',\n",
       "       'desmin-specific', 'nuclear', 'NuMA', 'eMyHC', 'desmin)',\n",
       "       'eMyHC-specific', 'NuMA+', 'NuMA−/desmin+', 'NuMA+/desmin+',\n",
       "       'NuMA+/Oct4+', 'Numa+/Oct4+', 'Collagenase', 'hbFGF', 'hbFGF.',\n",
       "       'Dispase', 'Trypsin/EDTA', 'Myf5/Pax7', 'BrdU', 'Desmin', 'FK506',\n",
       "       'DNaseI', 'DNaseI-hypersensitive', 'TAL1/SCL', 'DNaseI-digested',\n",
       "       'TAL1', 'acute', 'SCL', 'stem', 'RNaseA', 'proteinaseK', 'DNaseI.',\n",
       "       'T4', 'polymerase', 'Stil', 'DNaseI-treated', 'porphobilinogen',\n",
       "       'Hmbs', 'DNaseI-digestion', 'STIL', 'BglII', 'haematopoietic',\n",
       "       'endothelial-haematopoietic', 'erythroid', '+19', '+50', 'mouse',\n",
       "       'Activin', 'FGF2', 'protease', 'proteinase', 'Vent', 'β-actin',\n",
       "       'MAP', 'MAPK)', 'trypsin-', 'AKT', 'caveolin1', 'ERK1', 'GS15',\n",
       "       'ABP-280', 'B2', 'Karyopherin', 'BiP', 'Inhibitor', 'Caspase',\n",
       "       'OXA1Hs', 'protein', 'phosphatases', 'fibronectin', 'STAT3', 'FGF',\n",
       "       'PI3', 'Src', 'MAPK', 'GSK3', 'p38', 'cell', 'neurotensin',\n",
       "       'endothelin', 'thrombin', 'glial', 'Smad2/3', 'phospho-GSK3',\n",
       "       'Connexin', 'E-Cad', 'GDNFRα', 'HSP70', 'tyrosine', 'GTPases',\n",
       "       'AIM-1', 'NrBMXP51813MEK2P36506CaM', 'Kinase',\n",
       "       'alpha/SAPK2aQ16539Casein', 'CP17612Cdk5Q00535PKC',\n",
       "       'betaP05771Cdk7P50613PKC', 'deltaQ05655DAP', 'KinaseP53355PP2A',\n",
       "       'betaP18266RbP13405I', 'kappa',\n",
       "       'betaO14920Stat1A46159JAK1P23458Stat3P52631JNK1P45983VHRP51452MEK1Q02750',\n",
       "       'IKKgamma', 'E-CAD', 'Hsp70', 'CtBP1', 'CtBP2', 'GS-28', 'HDJ-2',\n",
       "       'Hsp40', 'heat', 'L-Caldesmon', 'Rabaptin', 'phosphorylated-p130',\n",
       "       'Cas', 'Crk-associated', 'Ras-GAP', 'phosphorylated', 'p21ras',\n",
       "       'ShcC', 'hESCs', 'Ras-GAP.', 'phospho-p130', 'TNIK', 'p130',\n",
       "       'Traf2', 'TNIK)', 'F-actin', 'p130Cas', 'TGFβ',\n",
       "       'GSK3β/Wnt/β-catenin', 'Jak/Stat', 'MAPK/ERK', 'Gap', 'Wnt',\n",
       "       'Stat1', 'SMADs', 'GSK3β', 'β-catenin', 'βStat1++',\n",
       "       'Jun+++Smad4/DPC4+++Endoglin+-WntCtBP2++', 'Catalytic',\n",
       "       'D3/CCND3++', 'beta++', 'Jun+++Casein',\n",
       "       'Receptor/PAR1/F2R+++SHPS-1/PTPNS1++', 'Jun+++Bcl-x/BCL2L1++',\n",
       "       'Bradykinin', 'Receptor', '3.4.24.16/NLN++', 'C++-PKA', 'RI',\n",
       "       'alpha++-C-Raf/RAF1++', 'iota++', 'beta/PRKCB1++',\n",
       "       'alpha/Akt+++GSK-3', 'Acid', 'Jun+++RAFT1/FRAP++',\n",
       "       'p170/PIK3C2A++PTP1B/PTPN1+++Dok1/p62dok+++PI3-Kinase', 'p110',\n",
       "       'alpha++-Yes', 'alpha/SAPK2a++-G3BP++', 'Inhibitor2/PPP1R2++',\n",
       "       'epsilon/YWHAE++', 'IGF', 'ERBB2', 'GPCR', 'GDNF', 'ZO1',\n",
       "       'occludin', 'EGF)-receptor', 'EGF-family', 'Heregulin', 'ERBB3',\n",
       "       'ERBB', 'ERBB2/3', 'accutase', 'Occludin', 'IGF1R', 'tight',\n",
       "       'transferrin', 'hergulin1β', 'activinA', 'LR3-IGF1', 'P190',\n",
       "       'Adaptin', 'STAT-3', 'PTP1D', 'Mek-2', 'RACK-1', 'GRB-2', 'Rap2',\n",
       "       'Exportin-1/CRM1', 'MCM', 'Nucleoporin', 'α-tubulin', 'Actin',\n",
       "       'KNP-1/HES1', 'NTF2', 'p190', 'Hip1R', 'Transportin',\n",
       "       'Calreticulin', 'Arp3', 'eIF-6', 'horse', 'peroxidase',\n",
       "       'Rabaptin-5', 'phospho-Ras-GAP', 'Shc-C', 'epidermal', 'EGF)',\n",
       "       'basic', 'leukemia', 'LIF', 'EGF', 'LIF.', 'trypsin-EDTA.', '3CB2',\n",
       "       'brachyury', 'foxa2', 'Vimentin', 'Notch1', 'neural', 'β-tubulin',\n",
       "       'medium-size', 'NF-M)', 'microtubule-associated', 'MAP-2', 'GFAP',\n",
       "       'myelin', 'MBP', 'glutamic', 'GAD', 'TH)', 'nestin+', 'TuJ1',\n",
       "       'galactocerebrocide', 'GC', 'GFAP+', 'nestin.10.1371/journal',\n",
       "       'NCAM', 'MBP)', 'TH.', 'GC-expressing', 'TuJ1+', 'hNuc', 'GluT1',\n",
       "       'doublecortin', 'CNPase-expressing', 'glutamate', 'Ki67+', 'hNuc+',\n",
       "       'hNuc+(', 'CNPase', 'GAD65/67', 'Sox1', 'heparin', 'insulin',\n",
       "       'trypsin', 'trypsin-EDTA', 'Anti-human', 'Anti-TuJ1',\n",
       "       'anti-GAD65/67', 'Anti-glial', 'Anti-galactocerebrocide',\n",
       "       'Anti-CNPase', 'Anti-Glucose', 'Transporter', 'Glut-1',\n",
       "       'Anti-Nestin', 'Anti-vimentin', 'Anti-3CB2', 'Anti-doublecortin',\n",
       "       'Anti-Ki67', 'RNase', 'Transcriptase', 'MAP2', 'N-CAM', 'Nestin',\n",
       "       'NF-M', 'Notch-1', 'FOXa2', 'HNF3B)', 'Brachyury'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = temp_df.copy()\n",
    "print(train_df[train_df['labels']=='B'])\n",
    "print(train_df['labels']=='B')\n",
    "train_df.loc[temp_df['labels']=='B',['words']] = temp_df[temp_df['labels']=='B']['words'].apply(lambda x:x.replace('-','-'))\n",
    "train_df[train_df['labels']=='B']['words'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 11164, 100, 100, 1769, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 23555, 100, 100, 100, 100, 100, 6028, 100, 100, 2302, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 4408, 1705, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 10799, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 4272, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 12104, 100, 8175, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 10322, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 4592, 100, 100, 100, 100, 100, 100, 100, 100, 100, 2765, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 3208, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 16580, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 3600, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 3241, 100, 100, 100, 100, 100, 100, 3501, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 18250, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 26825, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'alpha', '[UNK]', '[UNK]', 'human', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'notch', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'bone', '[UNK]', '[UNK]', 'heavy', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'derived', 'class', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'molecules', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'nuclear', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'acute', '[UNK]', 'stem', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'mouse', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'protein', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'cell', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'heat', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'Gap', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'tight', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'horse', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'basic', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'neural', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'insulin', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]']\n",
      "['CD', '##7', '##3', '-', 'P', '##E']\n",
      "['T', '##G', '##F', '-', 'β', '##3']\n",
      "['VC', '##AM']\n",
      "['ST', '##RO', '-', '1']\n",
      "['I', '##CA', '##M', '-', '1', '(', 'CD', '##5', '##4', ')']\n",
      "['CD', '##10', '##5']\n",
      "['CD', '##29']\n",
      "['M', '##F', '##20']\n",
      "['CD', '##7', '##3']\n",
      "['CD', '##44']\n",
      "['AL', '##CA', '##M', '(', 'CD', '##16', '##6']\n",
      "['My', '##o', '##D']\n",
      "['nest', '##in']\n",
      "['v', '##iment', '##in']\n",
      "['alpha']\n",
      "['fast', '-', 'switch']\n",
      "['pan', '-', 'c', '##yt', '##oker', '##ati', '##n']\n",
      "['human']\n",
      "['p', '##hos', '##pha', '##tase']\n",
      "['extra', '##cellular']\n",
      "['pro', '##te', '##og', '##ly', '##cans']\n",
      "['CD', '##7', '##3', '+']\n",
      "['CD', '##10', '##5', '(', 'SH', '##2']\n",
      "['CD', '##10', '##6']\n",
      "['CD', '##29', '(', 'in', '##te', '##g', '##rin']\n",
      "['β', '##1']\n",
      "['I', '##CA', '##M']\n",
      "['CD', '##34']\n",
      "['CD', '##45']\n",
      "['CD', '##14']\n",
      "['pan', '##cy', '##tok', '##era', '##tin']\n",
      "['des', '##min']\n",
      "['House', '##keeping']\n",
      "['DS', '##C', '##5', '##4']\n",
      "['ne', '##uro', '##pi', '##lin']\n",
      "['he', '##pa', '##to', '##cy', '##te']\n",
      "['fork', '##head']\n",
      "['notch']\n",
      "['PP', '##AR', '##γ']\n",
      "['co', '##lla', '##gen']\n",
      "['a', '##gg', '##re', '##can']\n",
      "['β', '-', 'g', '##ly', '##cer', '##ol', '##ph', '##os', '##phate']\n",
      "['bone', '-', 'specific']\n",
      "['bone']\n",
      "['My', '##o', '##D', '+']\n",
      "['my', '##os', '##in']\n",
      "['heavy']\n",
      "['my', '##ogen', '##in']\n",
      "['Nan', '##og']\n",
      "['Oct', '-', '4']\n",
      "['CD', '##4']\n",
      "['CC', '##R', '##5']\n",
      "['C', '##X', '##CR', '##4']\n",
      "['H', '##LA', '-', 'DR']\n",
      "['B', '##7', '.', '1']\n",
      "['G', '##FP']\n",
      "['si', '##RNA', '##s']\n",
      "['b', '##F', '##G', '##F']\n",
      "['GM', '-', 'CS', '##F']\n",
      "['M', '-', 'CS', '##F']\n",
      "['derived']\n",
      "['class']\n",
      "['E', '##G', '##FP']\n",
      "['CD', '##34', ')']\n",
      "['P', '##EC', '##Y', '##5']\n",
      "['M', '##HC']\n",
      "['LP', '##S']\n",
      "['IL', '-', '1']\n",
      "['T', '##NF', '-', 'α']\n",
      "['LP', '##S', '.']\n",
      "['T', '##NF', '##α']\n",
      "['G', '##FP', '-', 'E', '##S']\n",
      "['M', '##HC', '##II']\n",
      "['B', '##7']\n",
      "['molecules']\n",
      "['anti', '-', 'HIV']\n",
      "['co', '##lla', '##gen', '##ase']\n",
      "['CD', '##34', '+']\n",
      "['try', '##ps', '##in', '/', 'E', '##D', '##TA']\n",
      "['M', '-', 'CS', '##F', '.']\n",
      "['P', '##E', '-', 'CD', '##14']\n",
      "['P', '##E', '-', 'H', '##LA', '-', 'DR']\n",
      "['P', '##EC', '##Y', '##5', '-', 'CD', '##4']\n",
      "['P', '##EC', '##Y', '##5', '-', 'CC', '##R', '##5']\n",
      "['P', '##EC', '##Y', '##5', '-', 'C', '##X', '##CR', '##4']\n",
      "['cost', '##im', '##ulatory']\n",
      "['G', '##FP', '-', 'alone']\n",
      "['anti', '-', 'B', '##7', '.', '1']\n",
      "['Oct', '##4']\n",
      "['My', '##f', '-', '5']\n",
      "['delta', '-', 'Not', '##ch']\n",
      "['My', '##f', '##5']\n",
      "['M', '-', 'ca', '##dh', '##eri', '##n']\n",
      "['Pa', '##ired']\n",
      "['Pa', '##x', '##7']\n",
      "['em', '##b', '##ryo', '##nic']\n",
      "['des', '##min', '+']\n",
      "['Not', '##ch']\n",
      "['des', '##min', '+', '/', 'My', '##f', '##5', '+', '/', 'B', '##rd', '##U', '+']\n",
      "['Ki', '##6', '##7']\n",
      "['Oct', '##4', '+']\n",
      "['M', '-', 'ca', '##dh', '##eri', '##n', '##−', '/', 'des', '##min', '+']\n",
      "['M', '-', 'ca', '##dh', '##eri', '##n', '##−', '/', 'des', '##min', '##−']\n",
      "['M', '-', 'ca', '##dh', '##eri', '##n', '+', '/', 'des', '##min', '+']\n",
      "['des', '##min', '+', '/', 'B', '##rd', '##U', '+']\n",
      "['des', '##min', '-', 'specific']\n",
      "['nuclear']\n",
      "['N', '##u', '##MA']\n",
      "['e', '##M', '##y', '##HC']\n",
      "['des', '##min', ')']\n",
      "['e', '##M', '##y', '##HC', '-', 'specific']\n",
      "['N', '##u', '##MA', '+']\n",
      "['N', '##u', '##MA', '##−', '/', 'des', '##min', '+']\n",
      "['N', '##u', '##MA', '+', '/', 'des', '##min', '+']\n",
      "['N', '##u', '##MA', '+', '/', 'Oct', '##4', '+']\n",
      "['N', '##uma', '+', '/', 'Oct', '##4', '+']\n",
      "['Col', '##lage', '##nas', '##e']\n",
      "['h', '##b', '##F', '##G', '##F']\n",
      "['h', '##b', '##F', '##G', '##F', '.']\n",
      "['Di', '##sp', '##ase']\n",
      "['Try', '##ps', '##in', '/', 'E', '##D', '##TA']\n",
      "['My', '##f', '##5', '/', 'Pa', '##x', '##7']\n",
      "['B', '##rd', '##U']\n",
      "['Des', '##min']\n",
      "['FK', '##50', '##6']\n",
      "['D', '##N', '##ase', '##I']\n",
      "['D', '##N', '##ase', '##I', '-', 'h', '##yper', '##sen', '##sitive']\n",
      "['T', '##AL', '##1', '/', 'SC', '##L']\n",
      "['D', '##N', '##ase', '##I', '-', 'dig', '##ested']\n",
      "['T', '##AL', '##1']\n",
      "['acute']\n",
      "['SC', '##L']\n",
      "['stem']\n",
      "['R', '##N', '##ase', '##A']\n",
      "['protein', '##ase', '##K']\n",
      "['D', '##N', '##ase', '##I', '.']\n",
      "['T', '##4']\n",
      "['polymer', '##ase']\n",
      "['St', '##il']\n",
      "['D', '##N', '##ase', '##I', '-', 'treated']\n",
      "['p', '##or', '##ph', '##ob', '##ili', '##no', '##gen']\n",
      "['H', '##mbs']\n",
      "['D', '##N', '##ase', '##I', '-', 'dig', '##est', '##ion']\n",
      "['ST', '##IL']\n",
      "['B', '##g', '##l', '##II']\n",
      "['ha', '##ema', '##top', '##oi', '##etic']\n",
      "['end', '##oth', '##eli', '##al', '-', 'ha', '##ema', '##top', '##oi', '##etic']\n",
      "['er', '##yt', '##hr', '##oid']\n",
      "['+', '19']\n",
      "['+', '50']\n",
      "['mouse']\n",
      "['Act', '##iv', '##in']\n",
      "['F', '##G', '##F', '##2']\n",
      "['pro', '##te', '##ase']\n",
      "['protein', '##ase']\n",
      "['V', '##ent']\n",
      "['β', '-', 'act', '##in']\n",
      "['MA', '##P']\n",
      "['MA', '##P', '##K', ')']\n",
      "['try', '##ps', '##in', '-']\n",
      "['AK', '##T']\n",
      "['cave', '##olin', '##1']\n",
      "['ER', '##K', '##1']\n",
      "['G', '##S', '##15']\n",
      "['AB', '##P', '-', '280']\n",
      "['B', '##2']\n",
      "['Ka', '##ryo', '##pher', '##in']\n",
      "['B', '##i', '##P']\n",
      "['In', '##hibit', '##or']\n",
      "['C', '##as', '##pas', '##e']\n",
      "['O', '##X', '##A', '##1', '##H', '##s']\n",
      "['protein']\n",
      "['p', '##hos', '##pha', '##tase', '##s']\n",
      "['fi', '##bro', '##nect', '##in']\n",
      "['ST', '##AT', '##3']\n",
      "['F', '##G', '##F']\n",
      "['P', '##I', '##3']\n",
      "['Sr', '##c']\n",
      "['MA', '##P', '##K']\n",
      "['G', '##S', '##K', '##3']\n",
      "['p', '##38']\n",
      "['cell']\n",
      "['ne', '##uro', '##tens', '##in']\n",
      "['end', '##oth', '##elin']\n",
      "['th', '##rom', '##bin']\n",
      "['g', '##lial']\n",
      "['S', '##mad', '##2', '/', '3']\n",
      "['p', '##hos', '##ph', '##o', '-', 'G', '##S', '##K', '##3']\n",
      "['Con', '##nex', '##in']\n",
      "['E', '-', 'C', '##ad']\n",
      "['G', '##D', '##NF', '##R', '##α']\n",
      "['H', '##SP', '##70']\n",
      "['t', '##yr', '##os', '##ine']\n",
      "['GT', '##P', '##ases']\n",
      "['AI', '##M', '-', '1']\n",
      "['Nr', '##BM', '##X', '##P', '##51', '##8', '##13', '##ME', '##K', '##2', '##P', '##36', '##50', '##6', '##C', '##a', '##M']\n",
      "['Ki', '##nas', '##e']\n",
      "['alpha', '/', 'SA', '##P', '##K', '##2', '##a', '##Q', '##16', '##53', '##9', '##C', '##ase', '##in']\n",
      "['CP', '##17', '##6', '##12', '##C', '##d', '##k', '##5', '##Q', '##00', '##53', '##5', '##P', '##K', '##C']\n",
      "['beta', '##P', '##0', '##5', '##7', '##7', '##1', '##C', '##d', '##k', '##7', '##P', '##50', '##6', '##13', '##P', '##K', '##C']\n",
      "['delta', '##Q', '##0', '##5', '##65', '##5', '##DA', '##P']\n",
      "['Ki', '##nas', '##e', '##P', '##53', '##35', '##5', '##PP', '##2', '##A']\n",
      "['beta', '##P', '##18', '##26', '##6', '##R', '##b', '##P', '##13', '##40', '##5', '##I']\n",
      "['ka', '##ppa']\n",
      "['beta', '##O', '##14', '##9', '##20', '##S', '##tat', '##1', '##A', '##46', '##15', '##9', '##J', '##A', '##K', '##1', '##P', '##23', '##45', '##8', '##S', '##tat', '##3', '##P', '##5', '##26', '##31', '##J', '##N', '##K', '##1', '##P', '##45', '##9', '##8', '##3', '##V', '##H', '##RP', '##51', '##45', '##2', '##ME', '##K', '##1', '##Q', '##0', '##27', '##50']\n",
      "['I', '##K', '##K', '##gam', '##ma']\n",
      "['E', '-', 'CA', '##D']\n",
      "['H', '##sp', '##70']\n",
      "['C', '##t', '##B', '##P', '##1']\n",
      "['C', '##t', '##B', '##P', '##2']\n",
      "['G', '##S', '-', '28']\n",
      "['HD', '##J', '-', '2']\n",
      "['H', '##sp', '##40']\n",
      "['heat']\n",
      "['L', '-', 'Cal', '##des', '##mon']\n",
      "['Ra', '##ba', '##pt', '##in']\n",
      "['p', '##hos', '##ph', '##ory', '##lated', '-', 'p', '##13', '##0']\n",
      "['C', '##as']\n",
      "['C', '##rk', '-', 'associated']\n",
      "['Ra', '##s', '-', 'GA', '##P']\n",
      "['p', '##hos', '##ph', '##ory', '##lated']\n",
      "['p', '##21', '##ras']\n",
      "['S', '##h', '##c', '##C']\n",
      "['h', '##ES', '##Cs']\n",
      "['Ra', '##s', '-', 'GA', '##P', '.']\n",
      "['p', '##hos', '##ph', '##o', '-', 'p', '##13', '##0']\n",
      "['T', '##NI', '##K']\n",
      "['p', '##13', '##0']\n",
      "['T', '##ra', '##f', '##2']\n",
      "['T', '##NI', '##K', ')']\n",
      "['F', '-', 'act', '##in']\n",
      "['p', '##13', '##0', '##C', '##as']\n",
      "['T', '##G', '##F', '##β']\n",
      "['G', '##S', '##K', '##3', '##β', '/', 'W', '##nt', '/', 'β', '-', 'cat', '##eni', '##n']\n",
      "['J', '##ak', '/', 'St', '##at']\n",
      "['MA', '##P', '##K', '/', 'ER', '##K']\n",
      "['Gap']\n",
      "['W', '##nt']\n",
      "['St', '##at', '##1']\n",
      "['SM', '##AD', '##s']\n",
      "['G', '##S', '##K', '##3', '##β']\n",
      "['β', '-', 'cat', '##eni', '##n']\n",
      "['β', '##S', '##tat', '##1', '+', '+']\n",
      "['Jun', '+', '+', '+', 'S', '##mad', '##4', '/', 'D', '##PC', '##4', '+', '+', '+', 'End', '##og', '##lin', '+', '-', 'W', '##nt', '##C', '##t', '##B', '##P', '##2', '+', '+']\n",
      "['Cat', '##alytic']\n",
      "['D', '##3', '/', 'CC', '##ND', '##3', '+', '+']\n",
      "['beta', '+', '+']\n",
      "['Jun', '+', '+', '+', 'Case', '##in']\n",
      "['Re', '##ceptor', '/', 'PA', '##R', '##1', '/', 'F', '##2', '##R', '+', '+', '+', 'SH', '##PS', '-', '1', '/', 'PT', '##P', '##NS', '##1', '+', '+']\n",
      "['Jun', '+', '+', '+', 'B', '##c', '##l', '-', 'x', '/', 'BC', '##L', '##2', '##L', '##1', '+', '+']\n",
      "['Brady', '##kini', '##n']\n",
      "['Re', '##ceptor']\n",
      "['3', '.', '4', '.', '24', '.', '16', '/', 'NL', '##N', '+', '+']\n",
      "['C', '+', '+', '-', 'P', '##KA']\n",
      "['R', '##I']\n",
      "['alpha', '+', '+', '-', 'C', '-', 'Ra', '##f', '/', 'RAF', '##1', '+', '+']\n",
      "['i', '##ota', '+', '+']\n",
      "['beta', '/', 'PR', '##K', '##C', '##B', '##1', '+', '+']\n",
      "['alpha', '/', 'A', '##kt', '+', '+', '+', 'G', '##S', '##K', '-', '3']\n",
      "['A', '##cid']\n",
      "['Jun', '+', '+', '+', 'RAF', '##T', '##1', '/', 'F', '##RA', '##P', '+', '+']\n",
      "['p', '##17', '##0', '/', 'P', '##I', '##K', '##3', '##C', '##2', '##A', '+', '+', 'PT', '##P', '##1', '##B', '/', 'PT', '##P', '##N', '##1', '+', '+', '+', 'Do', '##k', '##1', '/', 'p', '##6', '##2', '##do', '##k', '+', '+', '+', 'P', '##I', '##3', '-', 'Ki', '##nas', '##e']\n",
      "['p', '##11', '##0']\n",
      "['alpha', '+', '+', '-', 'Yes']\n",
      "['alpha', '/', 'SA', '##P', '##K', '##2', '##a', '+', '+', '-', 'G', '##3', '##B', '##P', '+', '+']\n",
      "['In', '##hibit', '##or', '##2', '/', 'PP', '##P', '##1', '##R', '##2', '+', '+']\n",
      "['e', '##ps', '##ilon', '/', 'Y', '##W', '##HA', '##E', '+', '+']\n",
      "['I', '##G', '##F']\n",
      "['ER', '##BB', '##2']\n",
      "['GP', '##CR']\n",
      "['G', '##D', '##NF']\n",
      "['Z', '##O', '##1']\n",
      "['o', '##cc', '##lu', '##din']\n",
      "['E', '##G', '##F', ')', '-', 'receptor']\n",
      "['E', '##G', '##F', '-', 'family']\n",
      "['Here', '##gu', '##lin']\n",
      "['ER', '##BB', '##3']\n",
      "['ER', '##BB']\n",
      "['ER', '##BB', '##2', '/', '3']\n",
      "['a', '##cc', '##uta', '##se']\n",
      "['O', '##cc', '##lu', '##din']\n",
      "['I', '##G', '##F', '##1', '##R']\n",
      "['tight']\n",
      "['transfer', '##rin']\n",
      "['her', '##gu', '##lin', '##1', '##β']\n",
      "['act', '##iv', '##in', '##A']\n",
      "['L', '##R', '##3', '-', 'I', '##G', '##F', '##1']\n",
      "['P', '##19', '##0']\n",
      "['Ada', '##pt', '##in']\n",
      "['ST', '##AT', '-', '3']\n",
      "['PT', '##P', '##1', '##D']\n",
      "['Me', '##k', '-', '2']\n",
      "['RA', '##C', '##K', '-', '1']\n",
      "['G', '##RB', '-', '2']\n",
      "['Rap', '##2']\n",
      "['Expo', '##rt', '##in', '-', '1', '/', 'CR', '##M', '##1']\n",
      "['MC', '##M']\n",
      "['N', '##uc', '##leo', '##por', '##in']\n",
      "['α', '-', 'tub', '##ulin']\n",
      "['Act', '##in']\n",
      "['K', '##NP', '-', '1', '/', 'H', '##ES', '##1']\n",
      "['N', '##TF', '##2']\n",
      "['p', '##19', '##0']\n",
      "['Hip', '##1', '##R']\n",
      "['Transport', '##in']\n",
      "['Cal', '##ret', '##ic', '##ulin']\n",
      "['A', '##rp', '##3']\n",
      "['e', '##IF', '-', '6']\n",
      "['horse']\n",
      "['per', '##ox', '##idas', '##e']\n",
      "['Ra', '##ba', '##pt', '##in', '-', '5']\n",
      "['p', '##hos', '##ph', '##o', '-', 'Ra', '##s', '-', 'GA', '##P']\n",
      "['S', '##h', '##c', '-', 'C']\n",
      "['e', '##pid', '##er', '##mal']\n",
      "['E', '##G', '##F', ')']\n",
      "['basic']\n",
      "['le', '##uke', '##mia']\n",
      "['L', '##IF']\n",
      "['E', '##G', '##F']\n",
      "['L', '##IF', '.']\n",
      "['try', '##ps', '##in', '-', 'E', '##D', '##TA', '.']\n",
      "['3', '##C', '##B', '##2']\n",
      "['bra', '##chy', '##ury']\n",
      "['fox', '##a', '##2']\n",
      "['V', '##iment', '##in']\n",
      "['Not', '##ch', '##1']\n",
      "['neural']\n",
      "['β', '-', 'tub', '##ulin']\n",
      "['medium', '-', 'size']\n",
      "['N', '##F', '-', 'M', ')']\n",
      "['micro', '##tub', '##ule', '-', 'associated']\n",
      "['MA', '##P', '-', '2']\n",
      "['G', '##FA', '##P']\n",
      "['my', '##elin']\n",
      "['MB', '##P']\n",
      "['g', '##lut', '##ami', '##c']\n",
      "['GA', '##D']\n",
      "['T', '##H', ')']\n",
      "['nest', '##in', '+']\n",
      "['Tu', '##J', '##1']\n",
      "['gal', '##act', '##oc', '##ere', '##bro', '##cid', '##e']\n",
      "['G', '##C']\n",
      "['G', '##FA', '##P', '+']\n",
      "['nest', '##in', '.', '10', '.', '137', '##1', '/', 'journal']\n",
      "['NC', '##AM']\n",
      "['MB', '##P', ')']\n",
      "['T', '##H', '.']\n",
      "['G', '##C', '-', 'expressing']\n",
      "['Tu', '##J', '##1', '+']\n",
      "['h', '##N', '##uc']\n",
      "['G', '##lu', '##T', '##1']\n",
      "['double', '##cor', '##tin']\n",
      "['C', '##NP', '##ase', '-', 'expressing']\n",
      "['g', '##lut', '##ama', '##te']\n",
      "['Ki', '##6', '##7', '+']\n",
      "['h', '##N', '##uc', '+']\n",
      "['h', '##N', '##uc', '+', '(']\n",
      "['C', '##NP', '##ase']\n",
      "['GA', '##D', '##65', '/', '67']\n",
      "['Sox', '##1']\n",
      "['he', '##par', '##in']\n",
      "['insulin']\n",
      "['try', '##ps', '##in']\n",
      "['try', '##ps', '##in', '-', 'E', '##D', '##TA']\n",
      "['Anti', '-', 'human']\n",
      "['Anti', '-', 'Tu', '##J', '##1']\n",
      "['anti', '-', 'GA', '##D', '##65', '/', '67']\n",
      "['Anti', '-', 'g', '##lial']\n",
      "['Anti', '-', 'gal', '##act', '##oc', '##ere', '##bro', '##cid', '##e']\n",
      "['Anti', '-', 'C', '##NP', '##ase']\n",
      "['Anti', '-', 'G', '##lu', '##cos', '##e']\n",
      "['Transport', '##er']\n",
      "['G', '##lut', '-', '1']\n",
      "['Anti', '-', 'N', '##est', '##in']\n",
      "['Anti', '-', 'v', '##iment', '##in']\n",
      "['Anti', '-', '3', '##C', '##B', '##2']\n",
      "['Anti', '-', 'double', '##cor', '##tin']\n",
      "['Anti', '-', 'Ki', '##6', '##7']\n",
      "['R', '##N', '##ase']\n",
      "['Trans', '##cript', '##ase']\n",
      "['MA', '##P', '##2']\n",
      "['N', '-', 'CA', '##M']\n",
      "['N', '##est', '##in']\n",
      "['N', '##F', '-', 'M']\n",
      "['Not', '##ch', '-', '1']\n",
      "['F', '##OX', '##a', '##2']\n",
      "['H', '##NF', '##3', '##B', ')']\n",
      "['B', '##rac', '##hy', '##ury']\n",
      "[100]\n",
      "[28996]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_ids(train_df[train_df['labels']=='B']['words'].unique()))\n",
    "print(tokenizer.convert_ids_to_tokens(tokenizer.convert_tokens_to_ids(train_df[train_df['labels']=='B']['words'].unique())))\n",
    "for sent in train_df[train_df['labels']=='B']['words'].unique():\n",
    "    print(tokenizer.tokenize(sent))\n",
    "train_df[train_df['labels']=='B']['words'].unique()\n",
    "print(tokenizer.convert_tokens_to_ids([\"cellsf\"]))\n",
    "tokenizer.add_tokens(\"cellsf\")\n",
    "\n",
    "print(tokenizer.convert_tokens_to_ids([\"cellsf\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_dir': 'sbksvol/nikhil/test/model_output_test/', 'num_train_epochs': 100, 'train_batch_size': 64, 'save_strategy': 'epoch', 'evaluation_strategy': 'steps', 'eval_steps': 22, 'logging_steps': 22, 'do_train': True, 'load_best_model_at_end': True, 'learning_rate': 5e-05, 'save_total_limit': 2}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer, EarlyStoppingCallback\n",
    "from utils import LogCallback, plot_loss_log\n",
    "import json\n",
    "from transformers.hf_argparser import HfArgumentParser\n",
    "training_args_dict = {\n",
    "        'output_dir': params[\"OUTPUT_DIR\"],\n",
    "        'num_train_epochs': params[\"EPOCH_TOP\"],\n",
    "        'train_batch_size': params[\"BATCH_SIZE\"],\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"eval_steps\": max(10,train_dataset.__len__()//params[\"BATCH_SIZE\"]),\n",
    "        \"logging_steps\":max(10,train_dataset.__len__()//params[\"BATCH_SIZE\"]),\n",
    "        \"do_train\": True,\n",
    "        \"load_best_model_at_end\": params[\"LOAD_BEST_MODEL\"],\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"save_total_limit\": 2,\n",
    "#         \"resume_from_checkpoint\"\n",
    "    }\n",
    "print(training_args_dict)\n",
    "with open(params[\"TRAIN_ARGS_FILE\"], 'w') as fp:\n",
    "    json.dump(training_args_dict, fp)\n",
    "parser = HfArgumentParser(TrainingArguments)\n",
    "training_args = parser.parse_json_file(\n",
    "    json_file=params[\"TRAIN_ARGS_FILE\"])[0]\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset\n",
    "    )\n",
    "print(trainer.optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, -100, 2, 0, -100]\n",
      "['H', '##1', '(', 'WA', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['WA', '-', '01', ',', 'X']\n",
      "\n",
      "\n",
      "[0, -100, 2, 0, -100]\n",
      "['H', '##9', '(', 'WA', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['WA', '-', '09', ',', 'X']\n",
      "\n",
      "\n",
      "[0, -100, -100, 0, 2]\n",
      "['O', '##P', '##9', 'cells', 'were']\n",
      "\n",
      "\n",
      "[0, 2, 2, 2, 2]\n",
      "['cells', 'were', 'maintained', 'in', 'alpha']\n",
      "\n",
      "\n",
      "[0, -100, -100, 1, 2]\n",
      "['O', '##P', '##9', 'cells', 'in']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['C', '##2', '##C', '##12', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['C', '##2', '##C', '##12', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, 2, 2, 2]\n",
      "['H', '##1', ';', 'passages', '42']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, 2, -100, -100]\n",
      "['H', '##1', '[', 'WA', '-']\n",
      "\n",
      "\n",
      "[0, -100, 2, -100, -100]\n",
      "['H', '##9', '[', 'WA', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['O', '##P', '##9', 's', '##trom']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['O', '##P', '##9', 'cells', 'have']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['O', '##P', '##9', 'and', 'differentiated']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 0]\n",
      "['H', '##1', '-', 'and', 'H']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['H', '##9', '-', 'derived', 'CD']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, 0, 2]\n",
      "['O', '##P', '##9', 'cells', '(']\n",
      "\n",
      "\n",
      "[0, 2, 2, 2, -100]\n",
      "['cells', '(', 'Figure', 'S', '##1']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['C', '##2', '##C', '##12', 'previously']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 1]\n",
      "['C', '##2', '##C', '##12', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 1]\n",
      "['C', '##2', '##C', '##12', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['h', '##ES', '##MP', '##C', '##9']\n",
      "\n",
      "\n",
      "[0, -100, 2, 2, 2]\n",
      "['S', '##17', 'mouse', 'bone', 'ma']\n",
      "\n",
      "\n",
      "[0, -100, 2, 2, 2]\n",
      "['S', '##17', 'mouse', 'bone', 'ma']\n",
      "\n",
      "\n",
      "[0, -100, 2, -100, 2]\n",
      "['H', '##1', 'h', '##ES', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, 2, 2, -100]\n",
      "['S', '##17', 'bone', 'ma', '##rrow']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Ma', '##gi', '-', 'C', '##X']\n",
      "\n",
      "\n",
      "[0, -100, 2, 2, 2]\n",
      "['H', '##1', 'cell', 'line', 'was']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['29', '##3', '##T', 'cells', 'and']\n",
      "\n",
      "\n",
      "[0, -100, 2, 2, 2]\n",
      "['S', '##17', 'mouse', 'bone', 'ma']\n",
      "\n",
      "\n",
      "[0, -100, 0, 2, 2]\n",
      "['S', '##17', 'cell', 'layers', 'and']\n",
      "\n",
      "\n",
      "[0, 2, 2, 2, -100]\n",
      "['cell', 'layers', 'and', 'culture', '##d']\n",
      "\n",
      "\n",
      "[0, -100, 0, 2, -100]\n",
      "['S', '##17', 'cells', '[', '20']\n",
      "\n",
      "\n",
      "[0, 2, -100, -100, -100]\n",
      "['cells', '[', '20', ']', '.']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Ma', '##gi', '-', 'C', '##X']\n",
      "\n",
      "\n",
      "[0, -100, -100, 0, 2]\n",
      "['He', '##L', '##a', 'cell', 'derivative']\n",
      "\n",
      "\n",
      "[0, 2, 2, 2, 2]\n",
      "['cell', 'derivative', 'with', 'no', 'p']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', '.']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['B', '##W', '##51', '##47', '.']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['B', '##W', '##51', '##47', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'material']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'biological']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['K', '##5', '##6', '##2', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 0]\n",
      "['Cy', '##ther', '##a', '(', 'Cy']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['H', '##UE', '##S', '##6', '(']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##SC']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##SC']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##6', '-', 'E']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'h', '##ES']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['Cy', '##20', '##3', '(', 'Cy']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['H', '##UE', '##S', '##6', 'was']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['F', '##BR', '##16', '##64', '(']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['Cy', '##t', '-', 'E', '##S']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, -100]\n",
      "['H', '##UE', '##S', '##6', '-']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['B', '##G', '##01', 'cells', 'for']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['B', '##G', '##01', 'h', '##ES']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['B', '##G', '##01', 'h', '##ES']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['B', '##G', '##01', 'h', '##ES']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['B', '##G', '##0', '##3', 'h']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['B', '##G', '##0', '##3', 'cells']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['B', '##G', '##0', '##3', 'l']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['B', '##G', '##01', 'cultures', '(']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['B', '##G', '##01', 'cells', 'grown']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['B', '##G', '##01', 'cultures', 'were']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['B', '##G', '##01', 'cultures', 'were']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['B', '##G', '##01', 'h', '##ES']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['B', '##G', '##01', 'cells', ',']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['B', '##G', '##01', 'h', '##ES']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['B', '##G', '##0', '##3', 'h']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['B', '##G', '##01', 'cells', 'were']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['B', '##G', '##0', '##3', 'cell']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 0]\n",
      "['B', '##G', '##01', ',', 'B']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['B', '##G', '##0', '##2', 'and']\n",
      "\n",
      "\n",
      "[0, -100, -100, -100, 2]\n",
      "['B', '##G', '##0', '##3', 'cell']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['SD', '##5', '##6', ')', 'from']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['SD', '##5', '##6', 'cell', 'line']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['SD', '##5', '##6', 'human', 'neural']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['SD', '##5', '##6', '(', 'intermittent']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['SD', '##5', '##6', ',', 'was']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, 2]\n",
      "['SD', '##5', '##6', 'cells', 'did']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, 2, 2, -100]\n",
      "['H', '##9', '(', 'W', '##i']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "[0, -100, -100, 2, -100]\n",
      "['SD', '##5', '##6', 'h', '##NS']\n",
      "\n",
      "\n",
      "['B', 'I', 'O']\n"
     ]
    }
   ],
   "source": [
    "for x in train_dataset:\n",
    "    if 0 in x.label_ids:\n",
    "        for i in range(len(x.label_ids)):\n",
    "            if x.label_ids[i] == 0:# and '[UNK]' == tokenizer.convert_ids_to_tokens(x.input_ids)[i]:\n",
    "                print(x.label_ids[i:i+5])\n",
    "                print(tokenizer.convert_ids_to_tokens(x.input_ids)[i:i+5])\n",
    "                print(\"\\n\")\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "files = [\"train\", \"dev\", \"test\"]\n",
    "\n",
    "\n",
    "def convert(lines, f):\n",
    "    tokens_ = []\n",
    "    tags_ = []\n",
    "\n",
    "    data = {\"words\": [], \"ner\": []}\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            data[\"words\"].append(tokens_)\n",
    "            data[\"ner\"].append(tags_)\n",
    "            tokens_ = []\n",
    "            tags_ = []\n",
    "        else:\n",
    "            token, tag = line.split(\"\\t\")\n",
    "            if len(tag) > 1:\n",
    "                tag = tag.split(\"-\")[0]\n",
    "            tokens_.append(token.strip())\n",
    "            tags_.append(tag.strip())\n",
    "            \n",
    "    if len(tokens_) > 0:\n",
    "        data[\"words\"].append(tokens_)\n",
    "        data[\"ner\"].append(tags_)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writer(data, fp, add_str=\"\"):\n",
    "\n",
    "    for (tokens, tags) in zip(data[\"words\"], data[\"ner\"]):\n",
    "        for (token, tag) in zip(tokens, tags):\n",
    "            if tag == \"B\" or tag == \"I\":\n",
    "                tag += add_str\n",
    "            fp.write(\"{}\\t{}\\n\".format(token, tag))\n",
    "        fp.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert all tsv files to txt\n",
    "\n",
    "all_data = {}\n",
    "for f in files:\n",
    "    with open(os.path.join(data_dir, f + \".tsv\"), \"r\") as fp:\n",
    "        lines = fp.readlines()\n",
    "        all_data[f] = convert(lines, fp)\n",
    "    fp = open(os.path.join(data_dir, f + \".txt\"), \"w\")\n",
    "    writer(all_data[f], fp)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121 22 61\n"
     ]
    }
   ],
   "source": [
    "num_train_sents = len(all_data[\"train\"][\"words\"])\n",
    "num_dev_sents = len(all_data[\"dev\"][\"words\"])\n",
    "num_test_sents = len(all_data[\"test\"][\"words\"])\n",
    "print(num_train_sents, num_dev_sents, num_test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# add the index to keep track of sentences\n",
    "train_tuples = []\n",
    "for i,(tokens,tags) in enumerate(zip(all_data[\"train\"][\"words\"],all_data[\"train\"][\"ner\"])):\n",
    "    for token,tag in zip(tokens,tags):\n",
    "        train_tuples.append([i,token,tag])\n",
    "\n",
    "test_tuples = []\n",
    "for i,(tokens,tags) in enumerate(zip(all_data[\"test\"][\"words\"],all_data[\"test\"][\"ner\"])):\n",
    "    for token,tag in zip(tokens,tags):\n",
    "        test_tuples.append([i,token,tag])\n",
    "    \n",
    "train_df = pd.DataFrame(train_tuples, columns=['sentence_id', 'words', 'labels'])\n",
    "test_df = pd.DataFrame(test_tuples, columns=['sentence_id', 'words', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentence_id       words labels\n",
      "0            0          By      O\n",
      "1            0    Northern      O\n",
      "2            0        blot      O\n",
      "3            0    analysis      O\n",
      "4            0           ,      O\n",
      "5            0         the      O\n",
      "6            0  expression      O\n",
      "7            0          of      O\n",
      "8            0          IL      O\n",
      "9            0           -      O\n"
     ]
    }
   ],
   "source": [
    "print(test_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B', 'I', 'O']\n"
     ]
    }
   ],
   "source": [
    "# a list that has all possible labels \n",
    "labels = np.sort(train_df['labels'].unique()).tolist()\n",
    "label_map =  {i: label for i, label in enumerate(labels)}\n",
    "num_labels = len(labels)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = dict()\n",
    "\n",
    "# Path to pretrained model or model identifier from huggingface.co/models\n",
    "model_args['model_name_or_path'] = 'dmis-lab/biobert-base-cased-v1.1'\n",
    "# saved_model_path\n",
    "# saved_model_path\n",
    "# pytorch_dump_path\n",
    "# 'dmis-lab/biobert-base-cased-v1.1'\n",
    "\n",
    "# Where do you want to store the pretrained models downloaded from s3\n",
    "model_args['cache_dir'] = \"/sbksvol/gaurav/NER_out/\"\n",
    "\n",
    "# we skip basic white-space tokenization by passing do_basic_tokenize = False to the tokenizer\n",
    "model_args['do_basic_tokenize'] = False\n",
    "\n",
    "\n",
    "data_args = dict()\n",
    "\n",
    "data_args['data_dir'] = data_dir\n",
    "\n",
    "# \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "# \"than this will be truncated, sequences shorter will be padded.\"\n",
    "data_args['max_seq_length'] = 256\n",
    "\n",
    "# Overwrite the cached training and evaluation sets\n",
    "# this means the model does not have to tokenize/preprocess and cache the data each time it's called\n",
    "# this can be made different for each NerDataset (training NerDataset, testing NerDataset)\n",
    "data_args['overwrite_cache'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.4.2'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizer\n",
    ")\n",
    "\n",
    "config = BertConfig.from_pretrained(\n",
    "    model_args['model_name_or_path'],\n",
    "    num_labels=num_labels,\n",
    "    id2label=label_map,\n",
    "    label2id={label: i for i, label in enumerate(labels)},\n",
    "    cache_dir=model_args['cache_dir']\n",
    ")\n",
    "\n",
    "# we skip basic white-space tokenization by passing do_basic_tokenize = False to the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    model_args['model_name_or_path'],\n",
    "    cache_dir=model_args['cache_dir']\n",
    "#     ,do_basic_tokenize = model_args['do_basic_tokenize']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_utils_path = \"/sbksvol/gaurav/transformers/examples/token-classification/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if data_utils_path not in sys.path:\n",
    "    sys.path.append(data_utils_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_ner import NerDataset, Split\n",
    "# %reset_selective -f \"utils_ner\"\n",
    "# NerDataset.__init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NerDataset(\n",
    "  data_dir=data_args['data_dir'],\n",
    "  tokenizer=tokenizer,\n",
    "  labels=labels,\n",
    "  model_type=config.model_type,\n",
    "  max_seq_length=data_args['max_seq_length'],\n",
    "  overwrite_cache=data_args['overwrite_cache'], # True\n",
    "  mode=Split.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = NerDataset(\n",
    "  data_dir=data_args['data_dir'],\n",
    "  tokenizer=tokenizer,\n",
    "  labels=labels,\n",
    "  model_type=config.model_type,\n",
    "  max_seq_length=data_args['max_seq_length'],\n",
    "  overwrite_cache=data_args['overwrite_cache'],\n",
    "  mode=Split.dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 21\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.__len__(), eval_dataset.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train top-model using the Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models import FullyConnectedLayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel, BertModel\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertNERTopModel(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "#         self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        hidden_units_list=[500, 250, 125]\n",
    "#         activations_list = [\"none\", \"none\", \"none\", \"none\"]\n",
    "\n",
    "        hid1, hid2, hid3 = hidden_units_list\n",
    "        self.fc1 = nn.Linear(config.hidden_size, hid1)\n",
    "        self.fc2 = nn.Linear(hid1, hid2)\n",
    "        self.fc3 = nn.Linear(hid2, hid3)\n",
    "        self.fc4 = nn.Linear(hid3, config.num_labels)\n",
    "        \n",
    "        self.crf = CRF(config.num_labels, batch_first=True)\n",
    "\n",
    "#         self.classifier = FullyConnectedLayers(hidden_units_list, activations_list,\n",
    "#                                                config.hidden_size, config.num_labels)\n",
    "\n",
    "\n",
    "        ## 0-hidden layers ##\n",
    "#         self.fc1 = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "\n",
    "        ## 1-hidden layer ##\n",
    "#         self.fc1 = nn.Linear(config.hidden_size, 250)\n",
    "#         self.fc2 = nn.Linear(250, config.num_labels)\n",
    "\n",
    "        print(\"Initializing weights\")\n",
    "        self.init_weights()\n",
    "        \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
    "            1]``.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        \n",
    "#         logits = self.classifier(sequence_output)\n",
    "\n",
    "        x = sequence_output\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        logits = x\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            ## the tokens whose labels == -100 denote padding tokens\n",
    "            ## since they are ignored for loss calculation and because crf cannot accept label values other \n",
    "            ## than 0, 1, ... num_tags-1, we just set all the pad token indices to 2 instead of -100\n",
    "            labels_copy = labels.detach().clone()\n",
    "            labels_copy[labels_copy == -100] = 2\n",
    "            loss = -self.crf.forward(logits, labels_copy, attention_mask.type(torch.uint8), reduction=\"mean\")\n",
    "#             loss_fct = nn.CrossEntropyLoss()\n",
    "#             # Only keep active parts of the loss\n",
    "#             if attention_mask is not None:\n",
    "#                 active_loss = attention_mask.view(-1) == 1\n",
    "#                 active_logits = logits.view(-1, self.num_labels)\n",
    "#                 active_labels = torch.where(\n",
    "#                     active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "#                 )\n",
    "                \n",
    "#                 loss = loss_fct(active_logits, active_labels)\n",
    "#             else:\n",
    "#                 print(\"Labels None\")\n",
    "#                 loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.top_model_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First freeze bert weights and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertNERTopModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertNERTopModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertNERTopModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertNERTopModel were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertNERTopModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc1): Linear(in_features=768, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=250, bias=True)\n",
       "  (fc3): Linear(in_features=250, out_features=125, bias=True)\n",
       "  (fc4): Linear(in_features=125, out_features=3, bias=True)\n",
       "  (crf): CRF(num_tags=3)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = BertNERTopModel.from_pretrained(\n",
    "    model_args['model_name_or_path'],\n",
    "    config=config\n",
    "    ,cache_dir=model_args['cache_dir']\n",
    ")\n",
    "\n",
    "## base_model -> bert (excluding the classification layer)\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.hf_argparser import HfArgumentParser\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "training_args_dict = {\n",
    "    'output_dir' : \"model_output/\",\n",
    "    'num_train_epochs' : 20,\n",
    "    'train_batch_size': 32,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"evaluation_strategy\": \"epoch\"\n",
    "#     ,\n",
    "#     \"load_best_model_at_end\": True\n",
    "}\n",
    "\n",
    "with open('training_args.json', 'w') as fp:\n",
    "    json.dump(training_args_dict, fp)\n",
    "    \n",
    "parser = HfArgumentParser(TrainingArguments)\n",
    "# this function returns a tuple so we get the first item in the tuple since we only passed one arguement type \"TrainingArguments\"\n",
    "training_args = parser.parse_json_file(json_file=\"training_args.json\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 02:06, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>59.282146</td>\n",
       "      <td>0.465500</td>\n",
       "      <td>45.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>48.672626</td>\n",
       "      <td>0.459300</td>\n",
       "      <td>45.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>33.185772</td>\n",
       "      <td>0.460400</td>\n",
       "      <td>45.613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>17.980986</td>\n",
       "      <td>0.457000</td>\n",
       "      <td>45.954000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>9.539531</td>\n",
       "      <td>0.460800</td>\n",
       "      <td>45.572000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.969814</td>\n",
       "      <td>0.458500</td>\n",
       "      <td>45.804000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.941916</td>\n",
       "      <td>0.460600</td>\n",
       "      <td>45.594000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.141191</td>\n",
       "      <td>0.461300</td>\n",
       "      <td>45.522000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.474339</td>\n",
       "      <td>0.462700</td>\n",
       "      <td>45.381000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.931677</td>\n",
       "      <td>0.471900</td>\n",
       "      <td>44.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.518576</td>\n",
       "      <td>0.457200</td>\n",
       "      <td>45.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.200491</td>\n",
       "      <td>0.464300</td>\n",
       "      <td>45.227000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.962125</td>\n",
       "      <td>0.461800</td>\n",
       "      <td>45.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.791451</td>\n",
       "      <td>0.490200</td>\n",
       "      <td>42.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.673448</td>\n",
       "      <td>0.460900</td>\n",
       "      <td>45.563000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.584123</td>\n",
       "      <td>0.459700</td>\n",
       "      <td>45.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.522175</td>\n",
       "      <td>0.458700</td>\n",
       "      <td>45.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.483365</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>45.405000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.459371</td>\n",
       "      <td>0.463000</td>\n",
       "      <td>45.361000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.450893</td>\n",
       "      <td>0.464000</td>\n",
       "      <td>45.261000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=train_dataset,\n",
    "  eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "trainOutput = trainer.train()\n",
    "# trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0533, 0.0881, 0.0246],\n",
       "        [0.0301, 0.0743, 0.0117],\n",
       "        [0.0033, 0.0089, 0.0249]], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[-1].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now reload the model from saved checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing weights\n"
     ]
    }
   ],
   "source": [
    "num_steps = trainOutput.global_step # 17880\n",
    "checkpoint = f\"checkpoint-{num_steps}\"\n",
    "top_model_path = f\"{training_args_dict['output_dir']}/{checkpoint}\" \n",
    "\n",
    "# model_output/checkpoint-17880\n",
    "\n",
    "#### Config ####\n",
    "config = BertConfig.from_pretrained(\n",
    "    top_model_path,\n",
    "    num_labels=num_labels,\n",
    "    id2label=label_map,\n",
    "    label2id={label: i for i, label in enumerate(labels)},\n",
    "    cache_dir=model_args['cache_dir']\n",
    ")\n",
    "\n",
    "#### Model ####\n",
    "\n",
    "reloaded_model = BertNERTopModel.from_pretrained(\n",
    "    top_model_path,\n",
    "    config=config,\n",
    "    cache_dir=model_args['cache_dir']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0533, 0.0881, 0.0246],\n",
       "        [0.0301, 0.0743, 0.0117],\n",
       "        [0.0033, 0.0089, 0.0249]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reloaded_model.parameters())[-1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Training args ####\n",
    "training_args_dict = {\n",
    "    'output_dir' : \"model_output\",\n",
    "    'num_train_epochs' : 5,\n",
    "    'train_batch_size': 32,\n",
    "    'seed':seed_value,\n",
    "    \"evaluation_strategy\": \"epoch\"\n",
    "#     ,\"load_best_model_at_end\": True\n",
    "}\n",
    "\n",
    "with open('training_args.json', 'w') as fp:\n",
    "    json.dump(training_args_dict, fp)\n",
    "    \n",
    "parser = HfArgumentParser(TrainingArguments)\n",
    "# this function returns a tuple so we get the first item in the tuple since we only passed one arguement type \"TrainingArguments\"\n",
    "training_args = parser.parse_json_file(json_file=\"training_args.json\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then unfreeze the bert weights and train end-to-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = reloaded_model\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(name, param.shape, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertNERTopModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc1): Linear(in_features=768, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=250, bias=True)\n",
       "  (fc3): Linear(in_features=250, out_features=125, bias=True)\n",
       "  (fc4): Linear(in_features=125, out_features=3, bias=True)\n",
       "  (crf): CRF(num_tags=3)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(trainer.model_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:39, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.891200</td>\n",
       "      <td>0.461100</td>\n",
       "      <td>45.538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.461137</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>45.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.151145</td>\n",
       "      <td>0.470300</td>\n",
       "      <td>44.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.953274</td>\n",
       "      <td>0.462600</td>\n",
       "      <td>45.397000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.862683</td>\n",
       "      <td>0.463200</td>\n",
       "      <td>45.341000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=3.7158329264322916, metrics={'train_runtime': 40.482, 'train_samples_per_second': 1.853, 'total_flos': 99773520076800.0, 'epoch': 5.0, 'init_mem_cpu_alloc_delta': 48563, 'init_mem_gpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 18306, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 208707, 'train_mem_gpu_alloc_delta': 1305872896, 'train_mem_cpu_peaked_delta': 138334, 'train_mem_gpu_peaked_delta': 2282095616})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=train_dataset,\n",
    "  eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "# Begin training from the latest checkpoint\n",
    "trainer.train(checkpoint)\n",
    "# trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0520, 0.0873, 0.0263],\n",
       "        [0.0290, 0.0743, 0.0098],\n",
       "        [0.0050, 0.0071, 0.0243]], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[-1].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Softmax models ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # we can pass overwrite_cache as True since we might like to make new predictions by just changing test.txt \n",
    "# test_dataset = NerDataset(\n",
    "#   data_dir=data_args['data_dir'],\n",
    "#   tokenizer=tokenizer,\n",
    "#   labels=labels,\n",
    "#   model_type=config.model_type,\n",
    "#   max_seq_length=data_args['max_seq_length'],\n",
    "#   overwrite_cache=True,\n",
    "#   mode=Split.test)\n",
    "\n",
    "# # last layer output/activation has the shape of (batch_size, seq_len,num_of_labels)\n",
    "# output, label_ids, metrics = trainer.predict(test_dataset)\n",
    "# preds = np.argmax(output, axis=2)\n",
    "# batch_size, seq_len = preds.shape\n",
    "\n",
    "# # list of token-level predictions shape = (batch_size, seq_len)\n",
    "# preds_list = [[] for _ in range(batch_size)]\n",
    "# for i in range(batch_size):\n",
    "#     for j in range(seq_len):\n",
    "#         # ignore pad_tokens\n",
    "#         if label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n",
    "#             preds_list[i].append(label_map[preds[i][j]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For CRF models ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# we can pass overwrite_cache as True since we might like to make new predictions by just changing test.txt \n",
    "test_dataset = NerDataset(\n",
    "  data_dir=data_args['data_dir'],\n",
    "  tokenizer=tokenizer,\n",
    "  labels=labels,\n",
    "  model_type=config.model_type,\n",
    "  max_seq_length=data_args['max_seq_length'],\n",
    "  overwrite_cache=True,\n",
    "  mode=Split.test)\n",
    "\n",
    "# last layer output/activation has the shape of (batch_size, seq_len, num_labels)\n",
    "output, label_ids, metrics = trainer.predict(test_dataset)\n",
    "batch_size, seq_len, num_labels = output.shape\n",
    "\n",
    "output = torch.tensor(output).to('cuda')\n",
    "\n",
    "all_attention_masks = []\n",
    "for sample in test_dataset:\n",
    "    all_attention_masks.append(sample.attention_mask)\n",
    "    \n",
    "all_attention_masks = torch.tensor(all_attention_masks).to('cuda')\n",
    "\n",
    "# get the best tag sequences using CRF's viterbi decode algo\n",
    "preds = model.crf.decode(output, all_attention_masks.type(torch.uint8))\n",
    "\n",
    "\n",
    "preds_list = [[] for _ in range(batch_size)]\n",
    "for i in range(batch_size):\n",
    "    for j in range(seq_len):\n",
    "        # ignore pad_tokens\n",
    "        if label_ids[i, j] != -100:\n",
    "            preds_list[i].append(label_map[preds[i][j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_combiner(df):\n",
    "    # 'words' and 'labels' are the column names in the CSV file\n",
    "    tupple_function = lambda x: [(w, t) for w, t in zip(x[\"words\"].values.tolist(),\n",
    "                                                      x[\"labels\"].values.tolist())]\n",
    "    grouped = df.groupby(\"sentence_id\").apply(tupple_function)\n",
    "    return [s for s in grouped]\n",
    "\n",
    "testing_sentences = sentences_combiner(test_df)\n",
    "test_labels = [[w[1] for w in s] for s in testing_sentences]\n",
    "test_tokens = [[w[0] for w in s] for s in testing_sentences]\n",
    "\n",
    "# reconstruct full sentences from lists of (token,label) tuples \n",
    "# test_reconstructed = [\" \".join([w[0] for w in s] ) for s in testing_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all test and pred sentences have the same length\n",
    "\n",
    "test_labels_new = []\n",
    "preds_list_new = []\n",
    "\n",
    "for i, x in enumerate(test_labels):\n",
    "    if len(x) == len(preds_list[i]):\n",
    "        test_labels_new.append(x)\n",
    "        preds_list_new.append(preds_list[i])\n",
    "    else:\n",
    "        print(\"ABORT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get entity level scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 95.5%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.95      0.96      0.95        77\n",
      "\n",
      "   micro avg       0.95      0.96      0.95        77\n",
      "   macro avg       0.95      0.96      0.95        77\n",
      "weighted avg       0.95      0.96      0.95        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import f1_score, classification_report\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_labels_new, preds_list_new)))\n",
    "print(classification_report(test_labels_new, preds_list_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gene, bioinfer (no relu)\n",
    "# seed = 42 -> 0.84      0.87      0.85\n",
    "# seed = 0 -> 0.86      0.89      0.88\n",
    "# seed = 13 -> 0.83      0.88      0.85\n",
    "\n",
    "# Gene, bioinfer (all relu)\n",
    "# seed = 42 -> 0.84      0.90      0.87\n",
    "# seed = 0 -> 0.83      0.87      0.85\n",
    "# seed = 13 -> 0.84      0.88      0.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.78      0.66      0.71 -> gellus, 3-layer +softmax with relu after each fc layer \n",
    "# 0.97 0.94 0.95 -> cll, 3-layer +softmax with relu after each fc layer except the 1st, seed=42\n",
    "# 0.96 0.86, 0.90, 0.94, 0.96, 0.95 -> '' seed=0 \n",
    "# 0.97, 0.91, 0.94 -> '' seed=13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following results are for the cll dataset with different # relu layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu after 1st, 2nd and 3rd layer -> 0, 0, 0\n",
    "# relu after 2nd and 3rd layer -> 0.86, 0.96, 0.91\n",
    "# relu only after 3rd layer -> 0.92, 0.99, 0.95\n",
    "# No relu -> 0.97, 0.94, 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following results are for 3 hidden layers without any relu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cll\n",
    "# seed = 42 -> 0.92      0.99      0.95\n",
    "# seed = 0  -> 0.95      0.97      0.96\n",
    "# seed = 13 -> 0.97      0.96      0.97\n",
    "# seed = 20 -> 0.90      0.95      0.92\n",
    "# seed = 50 -> 0.96      0.96      0.96\n",
    "# seed = 75 -> 0.92      0.95      0.94\n",
    "# seed = 100 -> 0.93      0.92      0.93\n",
    "# -----------------------\n",
    "# average f1 -> 95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following results are for 1 hidden layers without any relu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cll\n",
    "# seed = 42 -> Not obtained yet\n",
    "# seed = 0  -> 0.83      0.99      0.90\n",
    "# seed = 13 -> 0.90      0.99      0.94\n",
    "# seed = 20 -> 0.90      0.97      0.94\n",
    "# seed = 50 -> Not obtained yet\n",
    "# seed = 75 -> Not obtained yet\n",
    "# seed = 100 -> Not obtained yet\n",
    "# -----------------------\n",
    "# average f1 -> Not obtained yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cellfinder\n",
    "# seed = 42 -> 0.86      0.63      0.73\n",
    "# seed = 0  -> 0.84      0.77      0.80\n",
    "# seed = 13 -> 0.83      0.70      0.76\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '{\"a\":{\"b\":{\"c\":3}}}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': {'b': {'c': 3}}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1290,  0.6884,  0.3964,  1.4097],\n",
      "        [ 0.1188,  0.3447,  0.0828,  0.1599],\n",
      "        [ 1.5903,  0.3980, -0.0881,  0.4196]])\n",
      "tensor([-1.0938,  0.7452, -0.1625])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 4)\n",
    "a = torch.randn(3)\n",
    "print(x)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.0000, -0.8046,  0.5794, -1.1551]])\n",
      "tensor([[-0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1188, 0.3447, 0.0828, 0.1599],\n",
      "        [0.0000, 0.0000, -0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.ge(a,0.5)\n",
    "x[~y] *= 0\n",
    "print(b)\n",
    "b[0,0]=5\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(~y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.ones(4)\n",
    "a.requires_grad=True\n",
    "print(a.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.]) False\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "b = Variable(a)\n",
    "print(b,b.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1., 1., 1., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(a.unsqueeze(-1))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4],\n",
       "        [1, 2, 3, 4]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.expand(2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 1, 2, 3]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "a.extend(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
